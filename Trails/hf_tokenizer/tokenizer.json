{
  "version": "1.0",
  "truncation": null,
  "padding": null,
  "added_tokens": [
    {
      "id": 0,
      "content": "<unk>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 1,
      "content": "<pad>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 2,
      "content": "<s>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 3,
      "content": "</s>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 4,
      "content": "<mask>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    }
  ],
  "normalizer": {
    "type": "Sequence",
    "normalizers": [
      {
        "type": "NFC"
      }
    ]
  },
  "pre_tokenizer": {
    "type": "Sequence",
    "pretokenizers": [
      {
        "type": "Metaspace",
        "replacement": "▁",
        "prepend_scheme": "always",
        "split": true
      }
    ]
  },
  "post_processor": {
    "type": "TemplateProcessing",
    "single": [
      {
        "SpecialToken": {
          "id": "<s>",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "A",
          "type_id": 0
        }
      },
      {
        "SpecialToken": {
          "id": "</s>",
          "type_id": 0
        }
      }
    ],
    "pair": [
      {
        "Sequence": {
          "id": "A",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "B",
          "type_id": 1
        }
      }
    ],
    "special_tokens": {
      "</s>": {
        "id": "</s>",
        "ids": [
          3
        ],
        "tokens": [
          "</s>"
        ]
      },
      "<s>": {
        "id": "<s>",
        "ids": [
          2
        ],
        "tokens": [
          "<s>"
        ]
      }
    }
  },
  "decoder": {
    "type": "Metaspace",
    "replacement": "_",
    "prepend_scheme": "always",
    "split": true
  },
  "model": {
    "type": "BPE",
    "dropout": null,
    "unk_token": "<unk>",
    "continuing_subword_prefix": null,
    "end_of_word_suffix": null,
    "fuse_unk": false,
    "byte_fallback": false,
    "ignore_merges": false,
    "vocab": {
      "<unk>": 0,
      "<pad>": 1,
      "<s>": 2,
      "</s>": 3,
      "<mask>": 4,
      "\n": 5,
      "!": 6,
      "\"": 7,
      "#": 8,
      "'": 9,
      "(": 10,
      ")": 11,
      "+": 12,
      ",": 13,
      "-": 14,
      ".": 15,
      "/": 16,
      "0": 17,
      "1": 18,
      "2": 19,
      "3": 20,
      "4": 21,
      "5": 22,
      "6": 23,
      "7": 24,
      "8": 25,
      "9": 26,
      ":": 27,
      ";": 28,
      "=": 29,
      "?": 30,
      "@": 31,
      "A": 32,
      "B": 33,
      "C": 34,
      "D": 35,
      "E": 36,
      "F": 37,
      "G": 38,
      "H": 39,
      "I": 40,
      "K": 41,
      "L": 42,
      "M": 43,
      "N": 44,
      "O": 45,
      "P": 46,
      "Q": 47,
      "R": 48,
      "S": 49,
      "T": 50,
      "U": 51,
      "W": 52,
      "X": 53,
      "Y": 54,
      "[": 55,
      "]": 56,
      "_": 57,
      "`": 58,
      "a": 59,
      "b": 60,
      "c": 61,
      "d": 62,
      "e": 63,
      "f": 64,
      "g": 65,
      "h": 66,
      "i": 67,
      "j": 68,
      "k": 69,
      "l": 70,
      "m": 71,
      "n": 72,
      "o": 73,
      "p": 74,
      "q": 75,
      "r": 76,
      "s": 77,
      "t": 78,
      "u": 79,
      "v": 80,
      "w": 81,
      "x": 82,
      "y": 83,
      "z": 84,
      "{": 85,
      "}": 86,
      "Ċ": 87,
      "Ġ": 88,
      "ক": 89,
      "ট": 90,
      "ত": 91,
      "ন": 92,
      "ব": 93,
      "ম": 94,
      "য": 95,
      "র": 96,
      "্": 97,
      "—": 98,
      "’": 99,
      "“": 100,
      "”": 101,
      "←": 102,
      "→": 103,
      "▁": 104,
      "⚠": 105,
      "️": 106,
      "🤗": 107,
      "▁t": 108,
      "in": 109,
      "▁a": 110,
      "en": 111,
      "er": 112,
      "▁th": 113,
      "',": 114,
      "▁to": 115,
      "▁'": 116,
      "at": 117,
      "▁w": 118,
      "or": 119,
      "on": 120,
      "se": 121,
      "iz": 122,
      "ing": 123,
      "ken": 124,
      "ra": 125,
      "▁the": 126,
      "keniz": 127,
      "ou": 128,
      "▁c": 129,
      "▁f": 130,
      "▁l": 131,
      "st": 132,
      "re": 133,
      "kenizer": 134,
      "▁\n": 135,
      "▁s": 136,
      "▁in": 137,
      "it": 138,
      "de": 139,
      "an": 140,
      "▁o": 141,
      "▁b": 142,
      "▁tokenizer": 143,
      "ll": 144,
      "rain": 145,
      "is": 146,
      "▁n": 147,
      "▁an": 148,
      "▁'Ġ": 149,
      "▁y": 150,
      "▁T": 151,
      "▁you": 152,
      "me": 153,
      "le": 154,
      "ce": 155,
      "▁of": 156,
      "ch": 157,
      "ion": 158,
      "ts": 159,
      "ex": 160,
      "pu": 161,
      "ge": 162,
      "ro": 163,
      "th": 164,
      "ĠĠ": 165,
      "▁d": 166,
      "ata": 167,
      "▁we": 168,
      "▁on": 169,
      "ode": 170,
      "atase": 171,
      "▁p": 172,
      "▁u": 173,
      ":\n": 174,
      "ar": 175,
      "ct": 176,
      "ed": 177,
      "▁h": 178,
      "▁lo": 179,
      "to": 180,
      "▁m": 181,
      "▁ne": 182,
      "▁and": 183,
      "▁se": 184,
      "▁for": 185,
      "raining": 186,
      "mp": 187,
      "▁that": 188,
      ")\n": 189,
      "un": 190,
      "▁wh": 191,
      "▁re": 192,
      "▁Th": 193,
      "rom": 194,
      "00": 195,
      "▁li": 196,
      "pus": 197,
      "▁is": 198,
      "▁ex": 199,
      "\"\"": 200,
      "ame": 201,
      "ill": 202,
      "ent": 203,
      "ation": 204,
      "orpus": 205,
      "▁can": 206,
      "es": 207,
      "ua": 208,
      "▁1": 209,
      "▁g": 210,
      "▁i": 211,
      "▁sp": 212,
      "▁be": 213,
      "▁your": 214,
      "▁new": 215,
      "ad": 216,
      "ld": 217,
      "train": 218,
      "yth": 219,
      "▁will": 220,
      "▁use": 221,
      "mple": 222,
      "ly": 223,
      "ve": 224,
      "ran": 225,
      "ith": 226,
      "ataset": 227,
      ".\n": 228,
      "et": 229,
      "▁de": 230,
      "▁from": 231,
      "as": 232,
      "ace": 233,
      "ample": 234,
      "ow": 235,
      "t_": 236,
      "▁=": 237,
      "▁e": 238,
      "▁it": 239,
      "our": 240,
      "odel": 241,
      "\"]": 242,
      "[\"": 243,
      "_d": 244,
      "al": 245,
      "ke": 246,
      "lf": 247,
      "ot": 248,
      "▁A": 249,
      "▁training": 250,
      "ere": 251,
      "▁this": 252,
      "atasets": 253,
      "_c": 254,
      "ast": 255,
      "ec": 256,
      "fun": 257,
      "gua": 258,
      "ur": 259,
      "ut": 260,
      "’s": 261,
      "▁P": 262,
      "▁at": 263,
      "▁are": 264,
      "erat": 265,
      "▁corpus": 266,
      "angua": 267,
      "ction": 268,
      "▁model": 269,
      "func": 270,
      "erator": 271,
      "anguage": 272,
      "ic": 273,
      "▁C": 274,
      "▁I": 275,
      "ine": 276,
      "ers": 277,
      "▁token": 278,
      "▁with": 279,
      "▁language": 280,
      "▁dataset": 281,
      "ython": 282,
      "all": 283,
      "ave": 284,
      "ok": 285,
      "ver": 286,
      "▁\"": 287,
      "▁me": 288,
      "▁tex": 289,
      "▁al": 290,
      "▁as": 291,
      "▁co": 292,
      "▁whi": 293,
      "▁Python": 294,
      "()": 295,
      "mb": 296,
      "okenizer": 297,
      "pt": 298,
      "ring": 299,
      "▁(": 300,
      "ize": 301,
      "string": 302,
      "▁see": 303,
      "▁This": 304,
      "000": 305,
      "urn": 306,
      "▁texts": 307,
      "▁which": 308,
      "_n": 309,
      "ay": 310,
      "for": 311,
      "id": 312,
      "ir": 313,
      "lo": 314,
      "ss": 315,
      "size": 316,
      "turn": 317,
      "w_d": 318,
      "ĊĠĠ": 319,
      "▁F": 320,
      "▁🤗": 321,
      "▁train": 322,
      "▁tokeniz": 323,
      "raw_d": 324,
      "▁same": 325,
      "▁pro": 326,
      "▁load": 327,
      "▁The": 328,
      "▁example": 329,
      "\"\"\"": 330,
      "▁spec": 331,
      "raw_datasets": 332,
      "▁speci": 333,
      "\":": 334,
      ",\n": 335,
      "_',": 336,
      "_string": 337,
      "bra": 338,
      "ha": 339,
      "ig": 340,
      "ry": 341,
      "s\n": 342,
      "sp": 343,
      "ust": 344,
      "ক্": 345,
      "▁st": 346,
      "▁our": 347,
      "atch": 348,
      "▁fun": 349,
      "▁fast": 350,
      "▁by": 351,
      "▁but": 352,
      "▁tokenizer.": 353,
      "▁not": 354,
      "▁do": 355,
      "▁one": 356,
      "▁have": 357,
      "tokenizer": 358,
      "▁need": 359,
      "▁list": 360,
      "▁libra": 361,
      "train\"]": 362,
      "[\"train\"]": 363,
      "▁\"ক্": 364,
      "raw_datasets[\"train\"]": 365,
      "▁function": 366,
      "ain": 367,
      "ake": 368,
      "ble": 369,
      "ds": 370,
      "gg": 371,
      "pre": 372,
      "sfor": 373,
      "’ll": 374,
      "▁D": 375,
      "▁H": 376,
      "▁R": 377,
      "self": 378,
      "out": 379,
      "▁code": 380,
      "▁so": 381,
      "and": 382,
      "ant": 383,
      "▁Training": 384,
      "le_": 385,
      "▁us": 386,
      "arch": 387,
      "▁look": 388,
      "▁1000": 389,
      "ransfor": 390,
      "▁tokenization": 391,
      "▁special": 392,
      "▁library": 393,
      "gging": 394,
      "ransform": 395,
      "),": 396,
      "]\n": 397,
      "`',": 398,
      "act": 399,
      "dent": 400,
      "ew": 401,
      "from": 402,
      "ime": 403,
      "lit": 404,
      "ole_": 405,
      "qu": 406,
      "ub": 407,
      "ugging": 408,
      "wh": 409,
      "Ġ',": 410,
      "’t": 411,
      "▁E": 412,
      "▁G": 413,
      "▁S": 414,
      "▁W": 415,
      "▁[": 416,
      "▁ge": 417,
      "▁'.": 418,
      "▁'func": 419,
      "▁'ĊĠĠ": 420,
      "▁'_',": 421,
      "▁want": 422,
      "ory": 423,
      "ould": 424,
      "▁cre": 425,
      "▁tokenizers": 426,
      "▁name": 427,
      "▁self": 428,
      "▁when": 429,
      "▁return": 430,
      "▁gen": 431,
      "▁if": 432,
      "▁def": 433,
      "_corpus": 434,
      "func_string": 435,
      "irst": 436,
      "igh": 437,
      "from_": 438,
      "ole_func_string": 439,
      "(',": 440,
      "):\n": 441,
      "Tokenizer": 442,
      "a',": 443,
      "ach": 444,
      "el": 445,
      "ist": 446,
      "mm": 447,
      "od": 448,
      "os": 449,
      "ost": 450,
      "pl": 451,
      "pen": 452,
      "rin": 453,
      "t',": 454,
      "uto": 455,
      "umb": 456,
      "ven": 457,
      "ving": 458,
      "▁+": 459,
      "▁L": 460,
      "▁N": 461,
      "▁O": 462,
      "▁',": 463,
      "▁or": 464,
      "▁ran": 465,
      "▁take": 466,
      "▁av": 467,
      "en(": 468,
      "▁')": 469,
      "search": 470,
      "▁fo": 471,
      "▁first": 472,
      "▁into": 473,
      "▁old": 474,
      "llow": 475,
      "▁'Ġ',": 476,
      "▁To": 477,
      "▁Transform": 478,
      "cess": 479,
      "▁prin": 480,
      "▁go": 481,
      "▁split": 482,
      "trained": 483,
      "ything": 484,
      "▁Auto": 485,
      "pter": 486,
      "self',": 487,
      "outpu": 488,
      "▁some": 489,
      "whole_func_string": 490,
      "▁'.',": 491,
      "▁creat": 492,
      "▁generator": 493,
      "umbers": 494,
      "▁',',": 495,
      "▁print": 496,
      "▁AutoTokenizer": 497,
      "\")\n": 498,
      "(\"": 499,
      "))\n": 500,
      "-n": 501,
      "-tokenizer": 502,
      "-search": 503,
      "0,": 504,
      ":',": 505,
      "__": 506,
      "_from_": 507,
      "ck": 508,
      "cr": 509,
      "dd": 510,
      "gor": 511,
      "il": 512,
      "just": 513,
      "ms": 514,
      "s,": 515,
      "so": 516,
      "ten": 517,
      "training": 518,
      "ul": 519,
      "um": 520,
      "▁2": 521,
      "▁3": 522,
      "▁B": 523,
      "▁\"\"\"": 524,
      "▁just": 525,
      "▁time": 526,
      "▁'size": 527,
      "▁wor": 528,
      "▁cor": 529,
      "▁con": 530,
      "▁call": 531,
      "▁le": 532,
      "▁inst": 533,
      "iterator": 534,
      "def": 535,
      "▁batch": 536,
      "▁tokenizer\n": 537,
      "▁Tokenizer": 538,
      "ge(": 539,
      "ther": 540,
      "▁we’ll": 541,
      "▁only": 542,
      "▁log": 543,
      "tokeniz": 544,
      "▁like": 545,
      "▁exact": 546,
      "▁used": 547,
      "et’s": 548,
      "et-tokenizer": 549,
      "t_size": 550,
      "▁each": 551,
      "[\"whole_func_string": 552,
      "▁As": 553,
      "▁If": 554,
      "▁In": 555,
      "▁tokens": 556,
      "very": 557,
      "▁meth": 558,
      "▁algor": 559,
      "▁also": 560,
      "_new": 561,
      "hapter": 562,
      "spon": 563,
      "pretrained": 564,
      "▁En": 565,
      "▁GP": 566,
      "▁We": 567,
      "▁self.": 568,
      "▁range(": 569,
      "▁follow": 570,
      "▁print(": 571,
      "-net-tokenizer": 572,
      "-search-net-tokenizer": 573,
      "_from_iterator": 574,
      "▁'size',": 575,
      "[\"whole_func_string\"]": 576,
      "▁algorith": 577,
      "_new_from_iterator": 578,
      "▁following": 579,
      "(ex": 580,
      "-c": 581,
      "Lay": 582,
      "Us": 583,
      "][\"whole_func_string\"]": 584,
      "_tokenizer": 585,
      "able": 586,
      "aving": 587,
      "ail": 588,
      "bi": 589,
      "bo": 590,
      "b',": 591,
      "code": 592,
      "di": 593,
      "eigh": 594,
      "e(ex": 595,
      "ebo": 596,
      "face": 597,
      "ie": 598,
      "if": 599,
      "las": 600,
      "mory": 601,
      "oc": 602,
      "ri": 603,
      "rit": 604,
      "s:": 605,
      "tain": 606,
      "ure": 607,
      "vi": 608,
      "wo": 609,
      "weigh": 610,
      "▁#": 611,
      "▁5": 612,
      "▁7": 613,
      "▁9": 614,
      "▁M": 615,
      "▁_": 616,
      "▁raw_datasets[\"train\"]": 617,
      "▁qu": 618,
      "▁very": 619,
      "▁tas": 620,
      "▁all": 621,
      "ents": 622,
      "▁'(',": 623,
      "▁'a',": 624,
      "▁won": 625,
      "▁would": 626,
      "▁writ": 627,
      "ses": 628,
      "ized": 629,
      "▁them": 630,
      "out_": 631,
      "▁ch": 632,
      "▁len(": 633,
      "▁scr": 634,
      "▁indent": 635,
      "▁'Ġt": 636,
      "▁'Ġ`',": 637,
      "▁'Ġself',": 638,
      "the": 639,
      "ĠĠĠĠ": 640,
      "▁pa": 641,
      "▁un": 642,
      "arn": 643,
      "▁most": 644,
      "ual": 645,
      "▁space": 646,
      "rand": 647,
      "▁it’s": 648,
      "ourse": 649,
      "ote": 650,
      "inear": 651,
      "▁memory": 652,
      "():\n": 653,
      "▁process": 654,
      "▁star": 655,
      "▁notebo": 656,
      "▁Datasets": 657,
      "▁using": 658,
      "uggingface": 659,
      "▁['": 660,
      "▁get": 661,
      "▁'):',": 662,
      "▁create": 663,
      "umbers',": 664,
      "▁AutoTokenizer.": 665,
      "training_corpus": 666,
      "▁contain": 667,
      "▁learn": 668,
      "def',": 669,
      "▁Tokenizers": 670,
      "tokenize(ex": 671,
      "▁method": 672,
      "▁range(0,": 673,
      "▁print(l": 674,
      "▁algorithm": 675,
      "Layer": 676,
      "bias": 677,
      "▁task": 678,
      "▁won’t": 679,
      "▁written": 680,
      "out_ms": 681,
      "▁scratch": 682,
      "▁indentation": 683,
      "▁notebook": 684,
      "tokenize(example": 685,
      "!\n": 686,
      "''": 687,
      "']\n": 688,
      "+',": 689,
      "-2": 690,
      "Add": 691,
      "T-2": 692,
      "You": 693,
      "[i": 694,
      "_to": 695,
      "_pretrained": 696,
      "ab": 697,
      "ade": 698,
      "du": 699,
      "ect": 700,
      "ead": 701,
      "fo": 702,
      "gl": 703,
      "ire": 704,
      "ive": 705,
      "min": 706,
      "numbers',": 707,
      "ol": 708,
      "op": 709,
      "oid": 710,
      "pos": 711,
      "s',": 712,
      "ter": 713,
      "wor": 714,
      "yp": 715,
      "’re": 716,
      "▁4": 717,
      "▁6": 718,
      "▁8": 719,
      "▁:": 720,
      "▁r": 721,
      "▁Ġ": 722,
      "▁Us": 723,
      "▁You": 724,
      "▁typ": 725,
      "inpu": 726,
      "▁act": 727,
      "ence": 728,
      "▁'Ċ": 729,
      "▁'`',": 730,
      "ath": 731,
      "ature": 732,
      "ort": 733,
      "one": 734,
      "semb": 735,
      "▁line": 736,
      "▁let’s": 737,
      "return": 738,
      "respon": 739,
      "▁sh": 740,
      "▁sub": 741,
      "▁inpu": 742,
      "▁info": 743,
      "itory": 744,
      "▁bit": 745,
      "ish": 746,
      "▁anything": 747,
      "▁'Ġ+',": 748,
      "▁'Ġreturn": 749,
      "les": 750,
      "▁data": 751,
      "▁once": 752,
      "▁pre": 753,
      "▁here": 754,
      "▁hand": 755,
      "▁hel": 756,
      "▁loop": 757,
      "mport": 758,
      "▁repre": 759,
      "ame',": 760,
      "entation": 761,
      "▁1,": 762,
      "▁import": 763,
      "train_new_from_iterator": 764,
      "t_id": 765,
      "t_training_corpus": 766,
      "▁ever": 767,
      "▁iterator": 768,
      "_dataset": 769,
      "▁training_corpus": 770,
      "_cb": 771,
      "_code": 772,
      "▁models": 773,
      "ication": 774,
      "▁Chapter": 775,
      "▁It": 776,
      "▁comp": 777,
      "▁comm": 778,
      "_name',": 779,
      "load": 780,
      "▁Face": 781,
      "▁Fast": 782,
      "▁load_dataset": 783,
      "\"\"\"',": 784,
      "▁does": 785,
      "▁one\n": 786,
      "▁needs": 787,
      "▁lists": 788,
      "raw_datasets[\"train\"]),": 789,
      "▁Here": 790,
      "▁Hugging": 791,
      "▁Rust": 792,
      "▁1000][\"whole_func_string\"]": 793,
      "▁tokenization\n": 794,
      "▁get_training_corpus": 795,
      "▁'func_code": 796,
      "▁'ĊĠĠĠ',": 797,
      "▁'ĊĠĠĠĠĠĠ": 798,
      "istic": 799,
      "▁avail": 800,
      "▁avoid": 801,
      "▁old_tokenizer": 802,
      "▁Transformer": 803,
      "▁Transformers": 804,
      "output',": 805,
      "output_size": 806,
      "(\"code": 807,
      "▁2,": 808,
      "▁timeout_ms": 809,
      "▁work": 810,
      "▁instead": 811,
      "▁Engl": 812,
      "▁GPT-2": 813,
      "-search-net-tokenizer\")\n": 814,
      "▁len(raw_datasets[\"train\"]),": 815,
      "▁spaces": 816,
      "uggingface-c": 817,
      "tokenize(example)\n": 818,
      "aded": 819,
      "pository": 820,
      "▁Using": 821,
      "▁subwor": 822,
      "▁info_cb": 823,
      "▁'Ġreturn',": 824,
      "▁help": 825,
      "▁repres": 826,
      "t_idx": 827,
      "▁everything": 828,
      "▁'ĊĠĠĠĠĠĠĠ',": 829,
      "▁available": 830,
      "▁English": 831,
      "\",": 832,
      "'\n": 833,
      "'s": 834,
      "(g": 835,
      "()\n": 836,
      "(self": 837,
      ").": 838,
      "-t": 839,
      "12": 840,
      "3\n": 841,
      "=',": 842,
      "AY": 843,
      "DA": 844,
      "KAY": 845,
      "LP": 846,
      "Linear": 847,
      "Net": 848,
      "PI": 849,
      "Se": 850,
      "UDA": 851,
      "_h": 852,
      "_lo": 853,
      "__',": 854,
      "`.": 855,
      "aw": 856,
      "ait": 857,
      "age": 858,
      "ack": 859,
      "bj": 860,
      "b']\n": 861,
      "cc": 862,
      "ci": 863,
      "con": 864,
      "cce": 865,
      "cstring": 866,
      "clas": 867,
      "d',": 868,
      "ding": 869,
      "ea": 870,
      "ep": 871,
      "est": 872,
      "eature": 873,
      "ff": 874,
      "gin": 875,
      "gra": 876,
      "hen": 877,
      "huggingface-c": 878,
      "ice": 879,
      "ild": 880,
      "imple": 881,
      "iving": 882,
      "lin": 883,
      "ling": 884,
      "low": 885,
      "lum": 886,
      "mal": 887,
      "mall": 888,
      "ments": 889,
      "mized": 890,
      "nt": 891,
      "n',": 892,
      "n’t": 893,
      "other": 894,
      "rch": 895,
      "s.": 896,
      "s:\n": 897,
      "sul": 898,
      "ting": 899,
      "tere": 900,
      "uge": 901,
      "ues": 902,
      "ules": 903,
      "uild": 904,
      "vid": 905,
      "way": 906,
      "wice": 907,
      "x',": 908,
      "ymb": 909,
      "zer": 910,
      "{\n": 911,
      "র\":": 912,
      "্র\":": 913,
      "▁Q": 914,
      "▁`": 915,
      "▁x": 916,
      "▁}": 917,
      "▁Ċ": 918,
      "▁—": 919,
      "▁pu": 920,
      "▁)\n": 921,
      "▁ent": 922,
      "▁ĊĠĠ": 923,
      "▁output_size": 924,
      "▁transform": 925,
      "▁two": 926,
      "▁twice": 927,
      "init": 928,
      "inut": 929,
      "▁ad": 930,
      "▁able": 931,
      "▁add": 932,
      "en))\n": 933,
      "enote": 934,
      "er.\n": 935,
      "▁then": 936,
      "▁thing": 937,
      "▁thre": 938,
      "▁than": 939,
      "▁there": 940,
      "▁torch": 941,
      "▁'self',": 942,
      "▁'__": 943,
      "▁'b',": 944,
      "▁'weigh": 945,
      "▁'bias": 946,
      "▁'Add": 947,
      "ater": 948,
      "▁was": 949,
      "▁way": 950,
      "▁wait": 951,
      "ore": 952,
      "orch": 953,
      "ormal": 954,
      "kens',": 955,
      "rall": 956,
      "ract": 957,
      "▁cha": 958,
      "▁chapter": 959,
      "▁clas": 960,
      "▁fi": 961,
      "▁few": 962,
      "▁feature": 963,
      "▁later": 964,
      "rect": 965,
      "repository": 966,
      "▁su": 967,
      "▁sent": 968,
      "▁sample": 969,
      "▁sour": 970,
      "▁save": 971,
      "▁saw": 972,
      "▁small": 973,
      "▁symb": 974,
      "▁in,": 975,
      "▁indi": 976,
      "▁intere": 977,
      "aning": 978,
      "ance": 979,
      "▁other": 980,
      "▁obj": 981,
      "▁bo": 982,
      "▁blo": 983,
      "▁brand": 984,
      "▁tokenizer,": 985,
      "lly": 986,
      "▁nex": 987,
      "▁any": 988,
      "▁another": 989,
      "▁'Ġad": 990,
      "▁'Ġand": 991,
      "▁'Ġa',": 992,
      "▁'Ġb',": 993,
      "▁'Ġthe": 994,
      "▁'Ġdef',": 995,
      "▁'Ġnumbers',": 996,
      "▁'Ġ\"\"\"',": 997,
      "▁'Ġoutput',": 998,
      "▁'Ġ=',": 999,
      "▁'Ġ__',": 1000,
      "▁'Ġb']\n": 1001,
      "▁'Ġx',": 1002,
      "▁yie": 1003,
      "▁you’re": 1004,
      "lements": 1005,
      "ions": 1006,
      "rodu": 1007,
      "▁dis": 1008,
      "▁denote": 1009,
      "odeSe": 1010,
      "▁par": 1011,
      "▁pow": 1012,
      "▁pic": 1013,
      "ary": 1014,
      "arat": 1015,
      "aring": 1016,
      "▁has": 1017,
      "▁how": 1018,
      "▁having": 1019,
      "▁huge": 1020,
      "▁loaded": 1021,
      "▁mill": 1022,
      "▁make": 1023,
      "▁minut": 1024,
      "▁set": 1025,
      "▁section": 1026,
      "uning": 1027,
      "▁why": 1028,
      "▁what": 1029,
      "▁requ": 1030,
      "▁respon": 1031,
      "▁resul": 1032,
      "▁That": 1033,
      "push": 1034,
      "▁exec": 1035,
      "es)": 1036,
      "es,": 1037,
      "ually": 1038,
      "▁1.": 1039,
      "▁10": 1040,
      "▁12": 1041,
      "▁gra": 1042,
      "▁ident": 1043,
      "▁newlin": 1044,
      "ly,": 1045,
      "▁depen": 1046,
      "▁deter": 1047,
      "▁even": 1048,
      "▁elements": 1049,
      "_doc": 1050,
      "▁API": 1051,
      "▁this,": 1052,
      "eck": 1053,
      "▁corpus\n": 1054,
      "▁corpus.\n": 1055,
      "ction\n": 1056,
      "▁Course": 1057,
      "▁CUDA": 1058,
      "▁CodeSe": 1059,
      "ine-t": 1060,
      "▁tokens\n": 1061,
      "▁language.": 1062,
      "▁dataset.": 1063,
      "ally": 1064,
      "▁alway": 1065,
      "▁colum": 1066,
      "▁(\n": 1067,
      "▁(th": 1068,
      "fore": 1069,
      "▁For": 1070,
      "▁train_new_from_iterator": 1071,
      "▁tokenized": 1072,
      "▁provi": 1073,
      "▁progra": 1074,
      "▁example,": 1075,
      "▁example:\n": 1076,
      "▁specif": 1077,
      "_string',": 1078,
      "space": 1079,
      "▁stat": 1080,
      "▁tokenizer.tokenize(example)\n": 1081,
      "▁docstring": 1082,
      "▁\"ক্ট": 1083,
      "▁\"ক্ত": 1084,
      "▁functions": 1085,
      "▁code-search-net-tokenizer": 1086,
      "archNet": 1087,
      "▁1000)\n": 1088,
      "▁library\n": 1089,
      "▁Even": 1090,
      "▁Saving": 1091,
      "▁'func_doc": 1092,
      "▁tokenizers'": 1093,
      "▁namespace": 1094,
      "▁self,": 1095,
      "▁returns": 1096,
      "▁define": 1097,
      "ight": 1098,
      "from_pretrained": 1099,
      "ist(g": 1100,
      "mming": 1101,
      "place": 1102,
      "plication": 1103,
      "▁Let’s": 1104,
      "▁Linear": 1105,
      "▁Note": 1106,
      "▁NLP": 1107,
      "▁Open": 1108,
      "▁')',": 1109,
      "▁going": 1110,
      "▁generator,": 1111,
      "__(self": 1112,
      "umentation": 1113,
      "▁3,": 1114,
      "▁Build": 1115,
      "▁\"\"\"\n": 1116,
      "▁words": 1117,
      "▁correct": 1118,
      "▁instance": 1119,
      "▁batches": 1120,
      "▁login": 1121,
      "▁exactly": 1122,
      "t_size,": 1123,
      "▁Assemb": 1124,
      "▁We’ll": 1125,
      "▁self.weigh": 1126,
      "▁self.bias": 1127,
      "iece": 1128,
      "ify": 1129,
      "ries": 1130,
      "wo',": 1131,
      "▁7,": 1132,
      "▁Model": 1133,
      "▁raw_datasets[\"train\"]\n": 1134,
      "▁raw_datasets[\"train\"][i": 1135,
      "ses(": 1136,
      "▁check": 1137,
      "▁'Ġtorch": 1138,
      "▁'Ġtwo',": 1139,
      "▁parall": 1140,
      "▁processing": 1141,
      "▁start_idx": 1142,
      "▁['def',": 1143,
      "▁AutoTokenizer.train_new_from_iterator": 1144,
      "▁AutoTokenizer.from_pretrained": 1145,
      "▁contains": 1146,
      "▁method:\n": 1147,
      "▁print(len(": 1148,
      "▁print(list(g": 1149,
      "▁algorithm.": 1150,
      "▁indentation,": 1151,
      "▁notebook,": 1152,
      "▁notebook_lo": 1153,
      "_tokens',": 1154,
      "_pretrained()": 1155,
      "ab\n": 1156,
      "▁rules": 1157,
      "▁type": 1158,
      "input',": 1159,
      "▁actually": 1160,
      "▁'Ċ',": 1161,
      "ather": 1162,
      "responses(": 1163,
      "▁should": 1164,
      "▁inputs": 1165,
      "▁once.": 1166,
      "▁1,000": 1167,
      "▁It’s": 1168,
      "▁command": 1169,
      "▁1000][\"whole_func_string\"]\n": 1170,
      "▁get_training_corpus():\n": 1171,
      "▁old_tokenizer.": 1172,
      "(\"code-search-net-tokenizer\")\n": 1173,
      "▁timeout_ms=": 1174,
      "▁working": 1175,
      "▁subwords": 1176,
      "▁represents": 1177,
      "_hub": 1178,
      "conds": 1179,
      "ccep": 1180,
      "estion": 1181,
      "huggingface-course": 1182,
      "vidual": 1183,
      "zeros": 1184,
      "▁pure": 1185,
      "▁three": 1186,
      "▁torch.": 1187,
      "▁'__(',": 1188,
      "▁'bias',": 1189,
      "▁'Add',": 1190,
      "▁charact": 1191,
      "▁samples": 1192,
      "▁source": 1193,
      "▁symbol": 1194,
      "▁individual": 1195,
      "▁interest": 1196,
      "▁object": 1197,
      "▁block": 1198,
      "▁next": 1199,
      "▁'Ġadd',": 1200,
      "▁'Ġand',": 1201,
      "▁'Ġthe',": 1202,
      "▁yield": 1203,
      "roduction\n": 1204,
      "▁powers": 1205,
      "▁pick": 1206,
      "▁require": 1207,
      "▁results": 1208,
      "▁execut": 1209,
      "▁identify": 1210,
      "▁newlines,": 1211,
      "▁depend": 1212,
      "▁determin": 1213,
      "▁CodeSearchNet": 1214,
      "ine-tuning": 1215,
      "▁always": 1216,
      "▁column": 1217,
      "▁provide": 1218,
      "▁programming": 1219,
      "▁code-search-net-tokenizer,": 1220,
      "▁'func_documentation": 1221,
      "▁LinearLayer": 1222,
      "__(self,": 1223,
      "▁Building": 1224,
      "▁correctly": 1225,
      "▁instance,": 1226,
      "▁Assembling": 1227,
      "▁'Ġtorch',": 1228,
      "▁parallel": 1229,
      "▁AutoTokenizer.train_new_from_iterator()": 1230,
      "▁AutoTokenizer.from_pretrained(\"": 1231,
      "▁print(list(gen))\n": 1232,
      "▁notebook_login": 1233
    },
    "merges": [
      "▁ t",
      "i n",
      "▁ a",
      "e n",
      "e r",
      "▁t h",
      "' ,",
      "▁t o",
      "▁ '",
      "a t",
      "▁ w",
      "o r",
      "o n",
      "s e",
      "i z",
      "in g",
      "k en",
      "r a",
      "▁th e",
      "ken iz",
      "o u",
      "▁ c",
      "▁ f",
      "▁ l",
      "s t",
      "r e",
      "keniz er",
      "▁ \n",
      "▁ s",
      "▁ in",
      "i t",
      "d e",
      "a n",
      "▁ o",
      "▁ b",
      "▁to kenizer",
      "l l",
      "ra in",
      "i s",
      "▁ n",
      "▁a n",
      "▁' Ġ",
      "▁ y",
      "▁ T",
      "▁y ou",
      "m e",
      "l e",
      "c e",
      "▁o f",
      "c h",
      "i on",
      "t s",
      "e x",
      "p u",
      "g e",
      "r o",
      "t h",
      "Ġ Ġ",
      "▁ d",
      "at a",
      "▁w e",
      "▁ on",
      "o de",
      "ata se",
      "▁ p",
      "▁ u",
      ": \n",
      "a r",
      "c t",
      "e d",
      "▁ h",
      "▁l o",
      "t o",
      "▁ m",
      "▁n e",
      "▁an d",
      "▁ se",
      "▁f or",
      "rain ing",
      "m p",
      "▁th at",
      ") \n",
      "u n",
      "▁w h",
      "▁ re",
      "▁T h",
      "ro m",
      "0 0",
      "▁l i",
      "pu s",
      "▁ is",
      "▁ ex",
      "\" \"",
      "a me",
      "i ll",
      "en t",
      "at ion",
      "or pus",
      "▁c an",
      "e s",
      "u a",
      "▁ 1",
      "▁ g",
      "▁ i",
      "▁s p",
      "▁b e",
      "▁you r",
      "▁ne w",
      "a d",
      "l d",
      "t rain",
      "y th",
      "▁w ill",
      "▁u se",
      "mp le",
      "l y",
      "v e",
      "ra n",
      "it h",
      "atase t",
      ". \n",
      "e t",
      "▁ de",
      "▁f rom",
      "a s",
      "a ce",
      "a mple",
      "o w",
      "t _",
      "▁ =",
      "▁ e",
      "▁ it",
      "ou r",
      "ode l",
      "\" ]",
      "[ \"",
      "_ d",
      "a l",
      "k e",
      "l f",
      "o t",
      "▁ A",
      "▁t raining",
      "er e",
      "▁th is",
      "atase ts",
      "_ c",
      "a st",
      "e c",
      "f un",
      "g ua",
      "u r",
      "u t",
      "’ s",
      "▁ P",
      "▁a t",
      "▁a re",
      "er at",
      "▁c orpus",
      "an gua",
      "ct ion",
      "▁m odel",
      "fun c",
      "erat or",
      "angua ge",
      "i c",
      "▁ C",
      "▁ I",
      "in e",
      "er s",
      "▁to ken",
      "▁w ith",
      "▁l anguage",
      "▁d ataset",
      "yth on",
      "a ll",
      "a ve",
      "o k",
      "v er",
      "▁ \"",
      "▁ me",
      "▁t ex",
      "▁a l",
      "▁a s",
      "▁c o",
      "▁wh i",
      "▁P ython",
      "( )",
      "m b",
      "o kenizer",
      "p t",
      "r ing",
      "▁ (",
      "iz e",
      "st ring",
      "▁se e",
      "▁Th is",
      "00 0",
      "ur n",
      "▁tex ts",
      "▁whi ch",
      "_ n",
      "a y",
      "f or",
      "i d",
      "i r",
      "l o",
      "s s",
      "s ize",
      "t urn",
      "w _d",
      "Ċ ĠĠ",
      "▁ F",
      "▁ 🤗",
      "▁t rain",
      "▁to keniz",
      "ra w_d",
      "▁s ame",
      "▁p ro",
      "▁lo ad",
      "▁Th e",
      "▁ex ample",
      "\"\" \"",
      "▁sp ec",
      "raw_d atasets",
      "▁spec i",
      "\" :",
      ", \n",
      "_ ',",
      "_ string",
      "b ra",
      "h a",
      "i g",
      "r y",
      "s \n",
      "s p",
      "u st",
      "ক ্",
      "▁ st",
      "▁ our",
      "at ch",
      "▁f un",
      "▁f ast",
      "▁b y",
      "▁b ut",
      "▁tokenizer .",
      "▁n ot",
      "▁d o",
      "▁on e",
      "▁h ave",
      "to kenizer",
      "▁ne ed",
      "▁li st",
      "▁li bra",
      "train \"]",
      "[\" train\"]",
      "▁\" ক্",
      "raw_datasets [\"train\"]",
      "▁fun ction",
      "a in",
      "a ke",
      "b le",
      "d s",
      "g g",
      "p re",
      "s for",
      "’ ll",
      "▁ D",
      "▁ H",
      "▁ R",
      "se lf",
      "ou t",
      "▁c ode",
      "▁s o",
      "an d",
      "an t",
      "▁T raining",
      "le _",
      "▁u s",
      "ar ch",
      "▁lo ok",
      "▁1 000",
      "ran sfor",
      "▁tokeniz ation",
      "▁speci al",
      "▁libra ry",
      "gg ing",
      "ransfor m",
      ") ,",
      "] \n",
      "` ',",
      "a ct",
      "d ent",
      "e w",
      "f rom",
      "i me",
      "l it",
      "o le_",
      "q u",
      "u b",
      "u gging",
      "w h",
      "Ġ ',",
      "’ t",
      "▁ E",
      "▁ G",
      "▁ S",
      "▁ W",
      "▁ [",
      "▁ ge",
      "▁' .",
      "▁' func",
      "▁' ĊĠĠ",
      "▁' _',",
      "▁w ant",
      "or y",
      "ou ld",
      "▁c re",
      "▁tokenizer s",
      "▁n ame",
      "▁se lf",
      "▁wh en",
      "▁re turn",
      "▁g en",
      "▁i f",
      "▁de f",
      "_c orpus",
      "func _string",
      "ir st",
      "ig h",
      "from _",
      "ole_ func_string",
      "( ',",
      ") :\n",
      "T okenizer",
      "a ',",
      "a ch",
      "e l",
      "i st",
      "m m",
      "o d",
      "o s",
      "o st",
      "p l",
      "p en",
      "r in",
      "t ',",
      "u to",
      "u mb",
      "v en",
      "v ing",
      "▁ +",
      "▁ L",
      "▁ N",
      "▁ O",
      "▁ ',",
      "▁ or",
      "▁ ran",
      "▁t ake",
      "▁a v",
      "en (",
      "▁' )",
      "se arch",
      "▁f o",
      "▁f irst",
      "▁in to",
      "▁o ld",
      "ll ow",
      "▁'Ġ ',",
      "▁T o",
      "▁T ransform",
      "ce ss",
      "▁p rin",
      "▁g o",
      "▁sp lit",
      "train ed",
      "yth ing",
      "▁A uto",
      "pt er",
      "self ',",
      "out pu",
      "▁so me",
      "wh ole_func_string",
      "▁'. ',",
      "▁cre at",
      "▁gen erator",
      "umb ers",
      "▁', ',",
      "▁prin t",
      "▁Auto Tokenizer",
      "\" )\n",
      "( \"",
      ") )\n",
      "- n",
      "- tokenizer",
      "- search",
      "0 ,",
      ": ',",
      "_ _",
      "_ from_",
      "c k",
      "c r",
      "d d",
      "g or",
      "i l",
      "j ust",
      "m s",
      "s ,",
      "s o",
      "t en",
      "t raining",
      "u l",
      "u m",
      "▁ 2",
      "▁ 3",
      "▁ B",
      "▁ \"\"\"",
      "▁ just",
      "▁t ime",
      "▁' size",
      "▁w or",
      "▁c or",
      "▁c on",
      "▁c all",
      "▁l e",
      "▁in st",
      "it erator",
      "de f",
      "▁b atch",
      "▁tokenizer \n",
      "▁T okenizer",
      "ge (",
      "th er",
      "▁we ’ll",
      "▁on ly",
      "▁lo g",
      "to keniz",
      "▁li ke",
      "▁ex act",
      "▁use d",
      "et ’s",
      "et -tokenizer",
      "t_ size",
      "▁e ach",
      "[\" whole_func_string",
      "▁A s",
      "▁I f",
      "▁I n",
      "▁token s",
      "ver y",
      "▁me th",
      "▁al gor",
      "▁al so",
      "_n ew",
      "ha pter",
      "sp on",
      "pre trained",
      "▁E n",
      "▁G P",
      "▁W e",
      "▁self .",
      "▁ran ge(",
      "▁fo llow",
      "▁print (",
      "-n et-tokenizer",
      "-search -net-tokenizer",
      "_from_ iterator",
      "▁'size ',",
      "[\"whole_func_string \"]",
      "▁algor ith",
      "_new _from_iterator",
      "▁follow ing",
      "( ex",
      "- c",
      "L ay",
      "U s",
      "] [\"whole_func_string\"]",
      "_ tokenizer",
      "a ble",
      "a ving",
      "a il",
      "b i",
      "b o",
      "b ',",
      "c ode",
      "d i",
      "e igh",
      "e (ex",
      "e bo",
      "f ace",
      "i e",
      "i f",
      "l as",
      "m ory",
      "o c",
      "r i",
      "r it",
      "s :",
      "t ain",
      "u re",
      "v i",
      "w o",
      "w eigh",
      "▁ #",
      "▁ 5",
      "▁ 7",
      "▁ 9",
      "▁ M",
      "▁ _",
      "▁ raw_datasets[\"train\"]",
      "▁ qu",
      "▁ very",
      "▁t as",
      "▁a ll",
      "en ts",
      "▁' (',",
      "▁' a',",
      "▁w on",
      "▁w ould",
      "▁w rit",
      "se s",
      "iz ed",
      "▁the m",
      "ou t_",
      "▁c h",
      "▁l en(",
      "▁s cr",
      "▁in dent",
      "▁'Ġ t",
      "▁'Ġ `',",
      "▁'Ġ self',",
      "th e",
      "ĠĠ ĠĠ",
      "▁p a",
      "▁u n",
      "ar n",
      "▁m ost",
      "ua l",
      "▁sp ace",
      "ran d",
      "▁it ’s",
      "our se",
      "ot e",
      "ine ar",
      "▁me mory",
      "() :\n",
      "▁pro cess",
      "▁st ar",
      "▁not ebo",
      "▁D atasets",
      "▁us ing",
      "ugging face",
      "▁[ '",
      "▁ge t",
      "▁') :',",
      "▁creat e",
      "umbers ',",
      "▁AutoTokenizer .",
      "training _corpus",
      "▁con tain",
      "▁le arn",
      "def ',",
      "▁Tokenizer s",
      "tokeniz e(ex",
      "▁meth od",
      "▁range( 0,",
      "▁print( l",
      "▁algorith m",
      "Lay er",
      "bi as",
      "▁tas k",
      "▁won ’t",
      "▁writ ten",
      "out_ ms",
      "▁scr atch",
      "▁indent ation",
      "▁notebo ok",
      "tokenize(ex ample",
      "! \n",
      "' '",
      "' ]\n",
      "+ ',",
      "- 2",
      "A dd",
      "T -2",
      "Y ou",
      "[ i",
      "_ to",
      "_ pretrained",
      "a b",
      "a de",
      "d u",
      "e ct",
      "e ad",
      "f o",
      "g l",
      "i re",
      "i ve",
      "m in",
      "n umbers',",
      "o l",
      "o p",
      "o id",
      "p os",
      "s ',",
      "t er",
      "w or",
      "y p",
      "’ re",
      "▁ 4",
      "▁ 6",
      "▁ 8",
      "▁ :",
      "▁ r",
      "▁ Ġ",
      "▁ Us",
      "▁ You",
      "▁t yp",
      "in pu",
      "▁a ct",
      "en ce",
      "▁' Ċ",
      "▁' `',",
      "at h",
      "at ure",
      "or t",
      "on e",
      "se mb",
      "▁l ine",
      "▁l et’s",
      "re turn",
      "re spon",
      "▁s h",
      "▁s ub",
      "▁in pu",
      "▁in fo",
      "it ory",
      "▁b it",
      "is h",
      "▁an ything",
      "▁'Ġ +',",
      "▁'Ġ return",
      "le s",
      "▁d ata",
      "▁on ce",
      "▁p re",
      "▁h ere",
      "▁h and",
      "▁h el",
      "▁lo op",
      "mp ort",
      "▁re pre",
      "ame ',",
      "ent ation",
      "▁1 ,",
      "▁i mport",
      "train _new_from_iterator",
      "t_ id",
      "t_ training_corpus",
      "▁e ver",
      "▁it erator",
      "_d ataset",
      "▁training _corpus",
      "_c b",
      "_c ode",
      "▁model s",
      "ic ation",
      "▁C hapter",
      "▁I t",
      "▁co mp",
      "▁co mm",
      "_n ame',",
      "lo ad",
      "▁F ace",
      "▁F ast",
      "▁load _dataset",
      "\"\"\" ',",
      "▁do es",
      "▁one \n",
      "▁need s",
      "▁list s",
      "raw_datasets[\"train\"] ),",
      "▁H ere",
      "▁H ugging",
      "▁R ust",
      "▁1000 ][\"whole_func_string\"]",
      "▁tokenization \n",
      "▁ge t_training_corpus",
      "▁'func _code",
      "▁'ĊĠĠ Ġ',",
      "▁'ĊĠĠ ĠĠĠĠ",
      "ist ic",
      "▁av ail",
      "▁av oid",
      "▁old _tokenizer",
      "▁Transform er",
      "▁Transform ers",
      "outpu t',",
      "outpu t_size",
      "(\" code",
      "▁2 ,",
      "▁time out_ms",
      "▁wor k",
      "▁inst ead",
      "▁En gl",
      "▁GP T-2",
      "-search-net-tokenizer \")\n",
      "▁len( raw_datasets[\"train\"]),",
      "▁space s",
      "uggingface -c",
      "tokenize(example )\n",
      "ade d",
      "pos itory",
      "▁Us ing",
      "▁sub wor",
      "▁info _cb",
      "▁'Ġreturn ',",
      "▁hel p",
      "▁repre s",
      "t_id x",
      "▁ever ything",
      "▁'ĊĠĠĠĠĠĠ Ġ',",
      "▁avail able",
      "▁Engl ish",
      "\" ,",
      "' \n",
      "' s",
      "( g",
      "( )\n",
      "( self",
      ") .",
      "- t",
      "1 2",
      "3 \n",
      "= ',",
      "A Y",
      "D A",
      "K AY",
      "L P",
      "L inear",
      "N et",
      "P I",
      "S e",
      "U DA",
      "_ h",
      "_ lo",
      "_ _',",
      "` .",
      "a w",
      "a it",
      "a ge",
      "a ck",
      "b j",
      "b ']\n",
      "c c",
      "c i",
      "c on",
      "c ce",
      "c string",
      "c las",
      "d ',",
      "d ing",
      "e a",
      "e p",
      "e st",
      "e ature",
      "f f",
      "g in",
      "g ra",
      "h en",
      "h uggingface-c",
      "i ce",
      "i ld",
      "i mple",
      "i ving",
      "l in",
      "l ing",
      "l ow",
      "l um",
      "m al",
      "m all",
      "m ents",
      "m ized",
      "n t",
      "n ',",
      "n ’t",
      "o ther",
      "r ch",
      "s .",
      "s :\n",
      "s ul",
      "t ing",
      "t ere",
      "u ge",
      "u es",
      "u les",
      "u ild",
      "v id",
      "w ay",
      "w ice",
      "x ',",
      "y mb",
      "z er",
      "{ \n",
      "র \":",
      "্ র\":",
      "▁ Q",
      "▁ `",
      "▁ x",
      "▁ }",
      "▁ Ċ",
      "▁ —",
      "▁ pu",
      "▁ )\n",
      "▁ ent",
      "▁ ĊĠĠ",
      "▁ output_size",
      "▁t ransform",
      "▁t wo",
      "▁t wice",
      "in it",
      "in ut",
      "▁a d",
      "▁a ble",
      "▁a dd",
      "en ))\n",
      "en ote",
      "er .\n",
      "▁th en",
      "▁th ing",
      "▁th re",
      "▁th an",
      "▁th ere",
      "▁to rch",
      "▁' self',",
      "▁' __",
      "▁' b',",
      "▁' weigh",
      "▁' bias",
      "▁' Add",
      "at er",
      "▁w as",
      "▁w ay",
      "▁w ait",
      "or e",
      "or ch",
      "or mal",
      "ken s',",
      "ra ll",
      "ra ct",
      "▁c ha",
      "▁c hapter",
      "▁c las",
      "▁f i",
      "▁f ew",
      "▁f eature",
      "▁l ater",
      "re ct",
      "re pository",
      "▁s u",
      "▁s ent",
      "▁s ample",
      "▁s our",
      "▁s ave",
      "▁s aw",
      "▁s mall",
      "▁s ymb",
      "▁in ,",
      "▁in di",
      "▁in tere",
      "an ing",
      "an ce",
      "▁o ther",
      "▁o bj",
      "▁b o",
      "▁b lo",
      "▁b rand",
      "▁tokenizer ,",
      "ll y",
      "▁n ex",
      "▁an y",
      "▁an other",
      "▁'Ġ ad",
      "▁'Ġ and",
      "▁'Ġ a',",
      "▁'Ġ b',",
      "▁'Ġ the",
      "▁'Ġ def',",
      "▁'Ġ numbers',",
      "▁'Ġ \"\"\"',",
      "▁'Ġ output',",
      "▁'Ġ =',",
      "▁'Ġ __',",
      "▁'Ġ b']\n",
      "▁'Ġ x',",
      "▁y ie",
      "▁you ’re",
      "le ments",
      "ion s",
      "ro du",
      "▁d is",
      "▁d enote",
      "ode Se",
      "▁p ar",
      "▁p ow",
      "▁p ic",
      "ar y",
      "ar at",
      "ar ing",
      "▁h as",
      "▁h ow",
      "▁h aving",
      "▁h uge",
      "▁lo aded",
      "▁m ill",
      "▁m ake",
      "▁m inut",
      "▁se t",
      "▁se ction",
      "un ing",
      "▁wh y",
      "▁wh at",
      "▁re qu",
      "▁re spon",
      "▁re sul",
      "▁Th at",
      "pus h",
      "▁ex ec",
      "es )",
      "es ,",
      "ua lly",
      "▁1 .",
      "▁1 0",
      "▁1 2",
      "▁g ra",
      "▁i dent",
      "▁new lin",
      "ly ,",
      "▁de pen",
      "▁de ter",
      "▁e ven",
      "▁e lements",
      "_d oc",
      "▁A PI",
      "▁this ,",
      "ec k",
      "▁corpus \n",
      "▁corpus .\n",
      "ction \n",
      "▁C ourse",
      "▁C UDA",
      "▁C odeSe",
      "ine -t",
      "▁token s\n",
      "▁language .",
      "▁dataset .",
      "all y",
      "▁al way",
      "▁co lum",
      "▁( \n",
      "▁( th",
      "for e",
      "▁F or",
      "▁train _new_from_iterator",
      "▁tokeniz ed",
      "▁pro vi",
      "▁pro gra",
      "▁example ,",
      "▁example :\n",
      "▁speci f",
      "_string ',",
      "sp ace",
      "▁st at",
      "▁tokenizer. tokenize(example)\n",
      "▁do cstring",
      "▁\"ক্ ট",
      "▁\"ক্ ত",
      "▁function s",
      "▁code -search-net-tokenizer",
      "arch Net",
      "▁1000 )\n",
      "▁library \n",
      "▁E ven",
      "▁S aving",
      "▁'func _doc",
      "▁tokenizers '",
      "▁name space",
      "▁self ,",
      "▁return s",
      "▁def ine",
      "igh t",
      "from_ pretrained",
      "ist (g",
      "mm ing",
      "pl ace",
      "pl ication",
      "▁L et’s",
      "▁L inear",
      "▁N ote",
      "▁N LP",
      "▁O pen",
      "▁') ',",
      "▁go ing",
      "▁generator ,",
      "__ (self",
      "um entation",
      "▁3 ,",
      "▁B uild",
      "▁\"\"\" \n",
      "▁wor ds",
      "▁cor rect",
      "▁inst ance",
      "▁batch es",
      "▁log in",
      "▁exact ly",
      "t_size ,",
      "▁As semb",
      "▁We ’ll",
      "▁self. weigh",
      "▁self. bias",
      "ie ce",
      "if y",
      "ri es",
      "wo ',",
      "▁7 ,",
      "▁M odel",
      "▁raw_datasets[\"train\"] \n",
      "▁raw_datasets[\"train\"] [i",
      "ses (",
      "▁ch eck",
      "▁'Ġt orch",
      "▁'Ġt wo',",
      "▁pa rall",
      "▁process ing",
      "▁star t_idx",
      "▁[' def',",
      "▁AutoTokenizer. train_new_from_iterator",
      "▁AutoTokenizer. from_pretrained",
      "▁contain s",
      "▁method :\n",
      "▁print(l en(",
      "▁print(l ist(g",
      "▁algorithm .",
      "▁indentation ,",
      "▁notebook ,",
      "▁notebook _lo",
      "_to kens',",
      "_pretrained ()",
      "ab \n",
      "▁r ules",
      "▁typ e",
      "inpu t',",
      "▁act ually",
      "▁'Ċ ',",
      "ath er",
      "respon ses(",
      "▁sh ould",
      "▁inpu ts",
      "▁once .",
      "▁1, 000",
      "▁It ’s",
      "▁comm and",
      "▁1000][\"whole_func_string\"] \n",
      "▁get_training_corpus ():\n",
      "▁old_tokenizer .",
      "(\"code -search-net-tokenizer\")\n",
      "▁timeout_ms =",
      "▁work ing",
      "▁subwor ds",
      "▁repres ents",
      "_h ub",
      "con ds",
      "cce p",
      "est ion",
      "huggingface-c ourse",
      "vid ual",
      "zer os",
      "▁pu re",
      "▁thre e",
      "▁torch .",
      "▁'__ (',",
      "▁'bias ',",
      "▁'Add ',",
      "▁cha ract",
      "▁sample s",
      "▁sour ce",
      "▁symb ol",
      "▁indi vidual",
      "▁intere st",
      "▁obj ect",
      "▁blo ck",
      "▁nex t",
      "▁'Ġad d',",
      "▁'Ġand ',",
      "▁'Ġthe ',",
      "▁yie ld",
      "rodu ction\n",
      "▁pow ers",
      "▁pic k",
      "▁requ ire",
      "▁resul ts",
      "▁exec ut",
      "▁ident ify",
      "▁newlin es,",
      "▁depen d",
      "▁deter min",
      "▁CodeSe archNet",
      "ine-t uning",
      "▁alway s",
      "▁colum n",
      "▁provi de",
      "▁progra mming",
      "▁code-search-net-tokenizer ,",
      "▁'func_doc umentation",
      "▁Linear Layer",
      "__(self ,",
      "▁Build ing",
      "▁correct ly",
      "▁instance ,",
      "▁Assemb ling",
      "▁'Ġtorch ',",
      "▁parall el",
      "▁AutoTokenizer.train_new_from_iterator ()",
      "▁AutoTokenizer.from_pretrained (\"",
      "▁print(list(g en))\n",
      "▁notebook_lo gin"
    ]
  }
}