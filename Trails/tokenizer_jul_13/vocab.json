{"<pad>":0,"<s>":1,"</s>":2,"<mask>":3,"\n":4,"!":5,"\"":6,"#":7,"'":8,"(":9,")":10,"+":11,",":12,"-":13,".":14,"/":15,"0":16,"1":17,"2":18,"3":19,"4":20,"5":21,"6":22,"7":23,"8":24,"9":25,":":26,";":27,"=":28,"?":29,"@":30,"A":31,"B":32,"C":33,"D":34,"E":35,"F":36,"G":37,"H":38,"I":39,"K":40,"L":41,"M":42,"N":43,"O":44,"P":45,"Q":46,"R":47,"S":48,"T":49,"U":50,"W":51,"X":52,"Y":53,"[":54,"]":55,"_":56,"`":57,"a":58,"b":59,"c":60,"d":61,"e":62,"f":63,"g":64,"h":65,"i":66,"j":67,"k":68,"l":69,"m":70,"n":71,"o":72,"p":73,"q":74,"r":75,"s":76,"t":77,"u":78,"v":79,"w":80,"x":81,"y":82,"z":83,"{":84,"}":85,"ÄŠ":86,"Ä ":87,"à¦•":88,"à¦Ÿ":89,"à¦¤":90,"à¦¨":91,"à¦¬":92,"à¦®":93,"à¦¯":94,"à¦°":95,"à§":96,"â€”":97,"â€™":98,"â€œ":99,"â€":100,"â†":101,"â†’":102,"â–":103,"âš ":104,"ï¸":105,"ğŸ¤—":106,"â–t":107,"in":108,"â–a":109,"en":110,"er":111,"â–th":112,"',":113,"â–to":114,"â–'":115,"at":116,"â–w":117,"or":118,"on":119,"se":120,"iz":121,"ing":122,"ken":123,"ra":124,"â–the":125,"keniz":126,"ou":127,"â–c":128,"â–f":129,"â–l":130,"st":131,"re":132,"kenizer":133,"â–\n":134,"â–s":135,"â–in":136,"it":137,"de":138,"an":139,"â–o":140,"â–b":141,"â–tokenizer":142,"ll":143,"rain":144,"is":145,"â–n":146,"â–an":147,"â–'Ä ":148,"â–y":149,"â–T":150,"â–you":151,"me":152,"le":153,"ce":154,"â–of":155,"ch":156,"ion":157,"ts":158,"ex":159,"pu":160,"ge":161,"ro":162,"th":163,"Ä Ä ":164,"â–d":165,"ata":166,"â–we":167,"â–on":168,"ode":169,"atase":170,"â–p":171,"â–u":172,":\n":173,"ar":174,"ct":175,"ed":176,"â–h":177,"â–lo":178,"to":179,"â–m":180,"â–ne":181,"â–and":182,"â–se":183,"â–for":184,"raining":185,"mp":186,"â–that":187,")\n":188,"un":189,"â–wh":190,"â–re":191,"â–Th":192,"rom":193,"00":194,"â–li":195,"pus":196,"â–is":197,"â–ex":198,"\"\"":199,"ame":200,"ill":201,"ent":202,"ation":203,"orpus":204,"â–can":205,"es":206,"ua":207,"â–1":208,"â–g":209,"â–i":210,"â–sp":211,"â–be":212,"â–your":213,"â–new":214,"ad":215,"ld":216,"train":217,"yth":218,"â–will":219,"â–use":220,"mple":221,"ly":222,"ve":223,"ran":224,"ith":225,"ataset":226,".\n":227,"et":228,"â–de":229,"â–from":230,"as":231,"ace":232,"ample":233,"ow":234,"t_":235,"â–=":236,"â–e":237,"â–it":238,"our":239,"odel":240,"\"]":241,"[\"":242,"_d":243,"al":244,"ke":245,"lf":246,"ot":247,"â–A":248,"â–training":249,"ere":250,"â–this":251,"atasets":252,"_c":253,"ast":254,"ec":255,"fun":256,"gua":257,"ur":258,"ut":259,"â€™s":260,"â–P":261,"â–at":262,"â–are":263,"erat":264,"â–corpus":265,"angua":266,"ction":267,"â–model":268,"func":269,"erator":270,"anguage":271,"ic":272,"â–C":273,"â–I":274,"ine":275,"ers":276,"â–token":277,"â–with":278,"â–language":279,"â–dataset":280,"ython":281,"all":282,"ave":283,"ok":284,"ver":285,"â–\"":286,"â–me":287,"â–tex":288,"â–al":289,"â–as":290,"â–co":291,"â–whi":292,"â–Python":293,"()":294,"mb":295,"okenizer":296,"pt":297,"ring":298,"â–(":299,"ize":300,"string":301,"â–see":302,"â–This":303,"000":304,"urn":305,"â–texts":306,"â–which":307,"_n":308,"ay":309,"for":310,"id":311,"ir":312,"lo":313,"ss":314,"size":315,"turn":316,"w_d":317,"ÄŠÄ Ä ":318,"â–F":319,"â–ğŸ¤—":320,"â–train":321,"â–tokeniz":322,"raw_d":323,"â–same":324,"â–pro":325,"â–load":326,"â–The":327,"â–example":328,"\"\"\"":329,"â–spec":330,"raw_datasets":331,"â–speci":332,"\":":333,",\n":334,"_',":335,"_string":336,"bra":337,"ha":338,"ig":339,"ry":340,"s\n":341,"sp":342,"ust":343,"à¦•à§":344,"â–st":345,"â–our":346,"atch":347,"â–fun":348,"â–fast":349,"â–by":350,"â–but":351,"â–tokenizer.":352,"â–not":353,"â–do":354,"â–one":355,"â–have":356,"tokenizer":357,"â–need":358,"â–list":359,"â–libra":360,"train\"]":361,"[\"train\"]":362,"â–\"à¦•à§":363,"raw_datasets[\"train\"]":364,"â–function":365,"ain":366,"ake":367,"ble":368,"ds":369,"gg":370,"pre":371,"sfor":372,"â€™ll":373,"â–D":374,"â–H":375,"â–R":376,"self":377,"out":378,"â–code":379,"â–so":380,"and":381,"ant":382,"â–Training":383,"le_":384,"â–us":385,"arch":386,"â–look":387,"â–1000":388,"ransfor":389,"â–tokenization":390,"â–special":391,"â–library":392,"gging":393,"ransform":394,"),":395,"]\n":396,"`',":397,"act":398,"dent":399,"ew":400,"from":401,"ime":402,"lit":403,"ole_":404,"qu":405,"ub":406,"ugging":407,"wh":408,"Ä ',":409,"â€™t":410,"â–E":411,"â–G":412,"â–S":413,"â–W":414,"â–[":415,"â–ge":416,"â–'.":417,"â–'func":418,"â–'ÄŠÄ Ä ":419,"â–'_',":420,"â–want":421,"ory":422,"ould":423,"â–cre":424,"â–tokenizers":425,"â–name":426,"â–self":427,"â–when":428,"â–return":429,"â–gen":430,"â–if":431,"â–def":432,"_corpus":433,"func_string":434,"irst":435,"igh":436,"from_":437,"ole_func_string":438,"(',":439,"):\n":440,"Tokenizer":441,"a',":442,"ach":443,"el":444,"ist":445,"mm":446,"od":447,"os":448,"ost":449,"pl":450,"pen":451,"rin":452,"t',":453,"uto":454,"umb":455,"ven":456,"ving":457,"â–+":458,"â–L":459,"â–N":460,"â–O":461,"â–',":462,"â–or":463,"â–ran":464,"â–take":465,"â–av":466,"en(":467,"â–')":468,"search":469,"â–fo":470,"â–first":471,"â–into":472,"â–old":473,"llow":474,"â–'Ä ',":475,"â–To":476,"â–Transform":477,"cess":478,"â–prin":479,"â–go":480,"â–split":481,"trained":482,"ything":483,"â–Auto":484,"pter":485,"self',":486,"outpu":487,"â–some":488,"whole_func_string":489,"â–'.',":490,"â–creat":491,"â–generator":492,"umbers":493,"â–',',":494,"â–print":495,"â–AutoTokenizer":496,"\")\n":497,"(\"":498,"))\n":499,"-n":500,"-tokenizer":501,"-search":502,"0,":503,":',":504,"__":505,"_from_":506,"ck":507,"cr":508,"dd":509,"gor":510,"il":511,"just":512,"ms":513,"s,":514,"so":515,"ten":516,"training":517,"ul":518,"um":519,"â–2":520,"â–3":521,"â–B":522,"â–\"\"\"":523,"â–just":524,"â–time":525,"â–'size":526,"â–wor":527,"â–cor":528,"â–con":529,"â–call":530,"â–le":531,"â–inst":532,"iterator":533,"def":534,"â–batch":535,"â–tokenizer\n":536,"â–Tokenizer":537,"ge(":538,"ther":539,"â–weâ€™ll":540,"â–only":541,"â–log":542,"tokeniz":543,"â–like":544,"â–exact":545,"â–used":546,"etâ€™s":547,"et-tokenizer":548,"t_size":549,"â–each":550,"[\"whole_func_string":551,"â–As":552,"â–If":553,"â–In":554,"â–tokens":555,"very":556,"â–meth":557,"â–algor":558,"â–also":559,"_new":560,"hapter":561,"spon":562,"pretrained":563,"â–En":564,"â–GP":565,"â–We":566,"â–self.":567,"â–range(":568,"â–follow":569,"â–print(":570,"-net-tokenizer":571,"-search-net-tokenizer":572,"_from_iterator":573,"â–'size',":574,"[\"whole_func_string\"]":575,"â–algorith":576,"_new_from_iterator":577,"â–following":578,"(ex":579,"-c":580,"Lay":581,"Us":582,"][\"whole_func_string\"]":583,"_tokenizer":584,"able":585,"aving":586,"ail":587,"bi":588,"bo":589,"b',":590,"code":591,"di":592,"eigh":593,"e(ex":594,"ebo":595,"face":596,"ie":597,"if":598,"las":599,"mory":600,"oc":601,"ri":602,"rit":603,"s:":604,"tain":605,"ure":606,"vi":607,"wo":608,"weigh":609,"â–#":610,"â–5":611,"â–7":612,"â–9":613,"â–M":614,"â–_":615,"â–raw_datasets[\"train\"]":616,"â–qu":617,"â–very":618,"â–tas":619,"â–all":620,"ents":621,"â–'(',":622,"â–'a',":623,"â–won":624,"â–would":625,"â–writ":626,"ses":627,"ized":628,"â–them":629,"out_":630,"â–ch":631,"â–len(":632,"â–scr":633,"â–indent":634,"â–'Ä t":635,"â–'Ä `',":636,"â–'Ä self',":637,"the":638,"Ä Ä Ä Ä ":639,"â–pa":640,"â–un":641,"arn":642,"â–most":643,"ual":644,"â–space":645,"rand":646,"â–itâ€™s":647,"ourse":648,"ote":649,"inear":650,"â–memory":651,"():\n":652,"â–process":653,"â–star":654,"â–notebo":655,"â–Datasets":656,"â–using":657,"uggingface":658,"â–['":659,"â–get":660,"â–'):',":661,"â–create":662,"umbers',":663,"â–AutoTokenizer.":664,"training_corpus":665,"â–contain":666,"â–learn":667,"def',":668,"â–Tokenizers":669,"tokenize(ex":670,"â–method":671,"â–range(0,":672,"â–print(l":673,"â–algorithm":674,"Layer":675,"bias":676,"â–task":677,"â–wonâ€™t":678,"â–written":679,"out_ms":680,"â–scratch":681,"â–indentation":682,"â–notebook":683,"tokenize(example":684,"!\n":685,"''":686,"']\n":687,"+',":688,"-2":689,"Add":690,"T-2":691,"You":692,"[i":693,"_to":694,"_pretrained":695,"ab":696,"ade":697,"du":698,"ect":699,"ead":700,"fo":701,"gl":702,"ire":703,"ive":704,"min":705,"numbers',":706,"ol":707,"op":708,"oid":709,"pos":710,"s',":711,"ter":712,"wor":713,"yp":714,"â€™re":715,"â–4":716,"â–6":717,"â–8":718,"â–:":719,"â–r":720,"â–Ä ":721,"â–Us":722,"â–You":723,"â–typ":724,"inpu":725,"â–act":726,"ence":727,"â–'ÄŠ":728,"â–'`',":729,"ath":730,"ature":731,"ort":732,"one":733,"semb":734,"â–line":735,"â–letâ€™s":736,"return":737,"respon":738,"â–sh":739,"â–sub":740,"â–inpu":741,"â–info":742,"itory":743,"â–bit":744,"ish":745,"â–anything":746,"â–'Ä +',":747,"â–'Ä return":748,"les":749,"â–data":750,"â–once":751,"â–pre":752,"â–here":753,"â–hand":754,"â–hel":755,"â–loop":756,"mport":757,"â–repre":758,"ame',":759,"entation":760,"â–1,":761,"â–import":762,"train_new_from_iterator":763,"t_id":764,"t_training_corpus":765,"â–ever":766,"â–iterator":767,"_dataset":768,"â–training_corpus":769,"_cb":770,"_code":771,"â–models":772,"ication":773,"â–Chapter":774,"â–It":775,"â–comp":776,"â–comm":777,"_name',":778,"load":779,"â–Face":780,"â–Fast":781,"â–load_dataset":782,"\"\"\"',":783,"â–does":784,"â–one\n":785,"â–needs":786,"â–lists":787,"raw_datasets[\"train\"]),":788,"â–Here":789,"â–Hugging":790,"â–Rust":791,"â–1000][\"whole_func_string\"]":792,"â–tokenization\n":793,"â–get_training_corpus":794,"â–'func_code":795,"â–'ÄŠÄ Ä Ä ',":796,"â–'ÄŠÄ Ä Ä Ä Ä Ä ":797,"istic":798,"â–avail":799,"â–avoid":800,"â–old_tokenizer":801,"â–Transformer":802,"â–Transformers":803,"output',":804,"output_size":805,"(\"code":806,"â–2,":807,"â–timeout_ms":808,"â–work":809,"â–instead":810,"â–Engl":811,"â–GPT-2":812,"-search-net-tokenizer\")\n":813,"â–len(raw_datasets[\"train\"]),":814,"â–spaces":815,"uggingface-c":816,"tokenize(example)\n":817,"aded":818,"pository":819,"â–Using":820,"â–subwor":821,"â–info_cb":822,"â–'Ä return',":823,"â–help":824,"â–repres":825,"t_idx":826,"â–everything":827,"â–'ÄŠÄ Ä Ä Ä Ä Ä Ä ',":828,"â–available":829,"â–English":830,"\",":831,"'\n":832,"'s":833,"(g":834,"()\n":835,"(self":836,").":837,"-t":838,"12":839,"3\n":840,"=',":841,"AY":842,"DA":843,"KAY":844,"LP":845,"Linear":846,"Net":847,"PI":848,"Se":849,"UDA":850,"_h":851,"_lo":852,"__',":853,"`.":854,"aw":855,"ait":856,"age":857,"ack":858,"bj":859,"b']\n":860,"cc":861,"ci":862,"con":863,"cce":864,"cstring":865,"clas":866,"d',":867,"ding":868,"ea":869,"ep":870,"est":871,"eature":872,"ff":873,"gin":874,"gra":875,"hen":876,"huggingface-c":877,"ice":878,"ild":879,"imple":880,"iving":881,"lin":882,"ling":883,"low":884,"lum":885,"mal":886,"mall":887,"ments":888,"mized":889,"nt":890,"n',":891,"nâ€™t":892,"other":893,"rch":894,"s.":895,"s:\n":896,"sul":897,"ting":898,"tere":899,"uge":900,"ues":901,"ules":902,"uild":903,"vid":904,"way":905,"wice":906,"x',":907,"ymb":908,"zer":909,"{\n":910,"à¦°\":":911,"à§à¦°\":":912,"â–Q":913,"â–`":914,"â–x":915,"â–}":916,"â–ÄŠ":917,"â–â€”":918,"â–pu":919,"â–)\n":920,"â–ent":921,"â–ÄŠÄ Ä ":922,"â–output_size":923,"â–transform":924,"â–two":925,"â–twice":926,"init":927,"inut":928,"â–ad":929,"â–able":930,"â–add":931,"en))\n":932,"enote":933,"er.\n":934,"â–then":935,"â–thing":936,"â–thre":937,"â–than":938,"â–there":939,"â–torch":940,"â–'self',":941,"â–'__":942,"â–'b',":943,"â–'weigh":944,"â–'bias":945,"â–'Add":946,"ater":947,"â–was":948,"â–way":949,"â–wait":950,"ore":951,"orch":952,"ormal":953,"kens',":954,"rall":955,"ract":956,"â–cha":957,"â–chapter":958,"â–clas":959,"â–fi":960,"â–few":961,"â–feature":962,"â–later":963,"rect":964,"repository":965,"â–su":966,"â–sent":967,"â–sample":968,"â–sour":969,"â–save":970,"â–saw":971,"â–small":972,"â–symb":973,"â–in,":974,"â–indi":975,"â–intere":976,"aning":977,"ance":978,"â–other":979,"â–obj":980,"â–bo":981,"â–blo":982,"â–brand":983,"â–tokenizer,":984,"lly":985,"â–nex":986,"â–any":987,"â–another":988,"â–'Ä ad":989,"â–'Ä and":990,"â–'Ä a',":991,"â–'Ä b',":992,"â–'Ä the":993,"â–'Ä def',":994,"â–'Ä numbers',":995,"â–'Ä \"\"\"',":996,"â–'Ä output',":997,"â–'Ä =',":998,"â–'Ä __',":999,"â–'Ä b']\n":1000,"â–'Ä x',":1001,"â–yie":1002,"â–youâ€™re":1003,"lements":1004,"ions":1005,"rodu":1006,"â–dis":1007,"â–denote":1008,"odeSe":1009,"â–par":1010,"â–pow":1011,"â–pic":1012,"ary":1013,"arat":1014,"aring":1015,"â–has":1016,"â–how":1017,"â–having":1018,"â–huge":1019,"â–loaded":1020,"â–mill":1021,"â–make":1022,"â–minut":1023,"â–set":1024,"â–section":1025,"uning":1026,"â–why":1027,"â–what":1028,"â–requ":1029,"â–respon":1030,"â–resul":1031,"â–That":1032,"push":1033,"â–exec":1034,"es)":1035,"es,":1036,"ually":1037,"â–1.":1038,"â–10":1039,"â–12":1040,"â–gra":1041,"â–ident":1042,"â–newlin":1043,"ly,":1044,"â–depen":1045,"â–deter":1046,"â–even":1047,"â–elements":1048,"_doc":1049,"â–API":1050,"â–this,":1051,"eck":1052,"â–corpus\n":1053,"â–corpus.\n":1054,"ction\n":1055,"â–Course":1056,"â–CUDA":1057,"â–CodeSe":1058,"ine-t":1059,"â–tokens\n":1060,"â–language.":1061,"â–dataset.":1062,"ally":1063,"â–alway":1064,"â–colum":1065,"â–(\n":1066,"â–(th":1067,"fore":1068,"â–For":1069,"â–train_new_from_iterator":1070,"â–tokenized":1071,"â–provi":1072,"â–progra":1073,"â–example,":1074,"â–example:\n":1075,"â–specif":1076,"_string',":1077,"space":1078,"â–stat":1079,"â–tokenizer.tokenize(example)\n":1080,"â–docstring":1081,"â–\"à¦•à§à¦Ÿ":1082,"â–\"à¦•à§à¦¤":1083,"â–functions":1084,"â–code-search-net-tokenizer":1085,"archNet":1086,"â–1000)\n":1087,"â–library\n":1088,"â–Even":1089,"â–Saving":1090,"â–'func_doc":1091,"â–tokenizers'":1092,"â–namespace":1093,"â–self,":1094,"â–returns":1095,"â–define":1096,"ight":1097,"from_pretrained":1098,"ist(g":1099,"mming":1100,"place":1101,"plication":1102,"â–Letâ€™s":1103,"â–Linear":1104,"â–Note":1105,"â–NLP":1106,"â–Open":1107,"â–')',":1108,"â–going":1109,"â–generator,":1110,"__(self":1111,"umentation":1112,"â–3,":1113,"â–Build":1114,"â–\"\"\"\n":1115,"â–words":1116,"â–correct":1117,"â–instance":1118,"â–batches":1119,"â–login":1120,"â–exactly":1121,"t_size,":1122,"â–Assemb":1123,"â–Weâ€™ll":1124,"â–self.weigh":1125,"â–self.bias":1126,"iece":1127,"ify":1128,"ries":1129,"wo',":1130,"â–7,":1131,"â–Model":1132,"â–raw_datasets[\"train\"]\n":1133,"â–raw_datasets[\"train\"][i":1134,"ses(":1135,"â–check":1136,"â–'Ä torch":1137,"â–'Ä two',":1138,"â–parall":1139,"â–processing":1140,"â–start_idx":1141,"â–['def',":1142,"â–AutoTokenizer.train_new_from_iterator":1143,"â–AutoTokenizer.from_pretrained":1144,"â–contains":1145,"â–method:\n":1146,"â–print(len(":1147,"â–print(list(g":1148,"â–algorithm.":1149,"â–indentation,":1150,"â–notebook,":1151,"â–notebook_lo":1152,"_tokens',":1153,"_pretrained()":1154,"ab\n":1155,"â–rules":1156,"â–type":1157,"input',":1158,"â–actually":1159,"â–'ÄŠ',":1160,"ather":1161,"responses(":1162,"â–should":1163,"â–inputs":1164,"â–once.":1165,"â–1,000":1166,"â–Itâ€™s":1167,"â–command":1168,"â–1000][\"whole_func_string\"]\n":1169,"â–get_training_corpus():\n":1170,"â–old_tokenizer.":1171,"(\"code-search-net-tokenizer\")\n":1172,"â–timeout_ms=":1173,"â–working":1174,"â–subwords":1175,"â–represents":1176,"_hub":1177,"conds":1178,"ccep":1179,"estion":1180,"huggingface-course":1181,"vidual":1182,"zeros":1183,"â–pure":1184,"â–three":1185,"â–torch.":1186,"â–'__(',":1187,"â–'bias',":1188,"â–'Add',":1189,"â–charact":1190,"â–samples":1191,"â–source":1192,"â–symbol":1193,"â–individual":1194,"â–interest":1195,"â–object":1196,"â–block":1197,"â–next":1198,"â–'Ä add',":1199,"â–'Ä and',":1200,"â–'Ä the',":1201,"â–yield":1202,"roduction\n":1203,"â–powers":1204,"â–pick":1205,"â–require":1206,"â–results":1207,"â–execut":1208,"â–identify":1209,"â–newlines,":1210,"â–depend":1211,"â–determin":1212,"â–CodeSearchNet":1213,"ine-tuning":1214,"â–always":1215,"â–column":1216,"â–provide":1217,"â–programming":1218,"â–code-search-net-tokenizer,":1219,"â–'func_documentation":1220,"â–LinearLayer":1221,"__(self,":1222,"â–Building":1223,"â–correctly":1224,"â–instance,":1225,"â–Assembling":1226,"â–'Ä torch',":1227,"â–parallel":1228,"â–AutoTokenizer.train_new_from_iterator()":1229,"â–AutoTokenizer.from_pretrained(\"":1230,"â–print(list(gen))\n":1231,"â–notebook_login":1232}