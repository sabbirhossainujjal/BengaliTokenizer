{"<pad>":0,"<s>":1,"</s>":2,"<mask>":3,"\n":4,"!":5,"\"":6,"#":7,"'":8,"(":9,")":10,"+":11,",":12,"-":13,".":14,"/":15,"0":16,"1":17,"2":18,"3":19,"4":20,"5":21,"6":22,"7":23,"8":24,"9":25,":":26,";":27,"=":28,"?":29,"@":30,"A":31,"B":32,"C":33,"D":34,"E":35,"F":36,"G":37,"H":38,"I":39,"K":40,"L":41,"M":42,"N":43,"O":44,"P":45,"Q":46,"R":47,"S":48,"T":49,"U":50,"W":51,"X":52,"Y":53,"[":54,"]":55,"_":56,"`":57,"a":58,"b":59,"c":60,"d":61,"e":62,"f":63,"g":64,"h":65,"i":66,"j":67,"k":68,"l":69,"m":70,"n":71,"o":72,"p":73,"q":74,"r":75,"s":76,"t":77,"u":78,"v":79,"w":80,"x":81,"y":82,"z":83,"{":84,"}":85,"Ċ":86,"Ġ":87,"ক":88,"ট":89,"ত":90,"ন":91,"ব":92,"ম":93,"য":94,"র":95,"্":96,"—":97,"’":98,"“":99,"”":100,"←":101,"→":102,"▁":103,"⚠":104,"️":105,"🤗":106,"▁t":107,"in":108,"▁a":109,"en":110,"er":111,"▁th":112,"',":113,"▁to":114,"▁'":115,"at":116,"▁w":117,"or":118,"on":119,"se":120,"iz":121,"ing":122,"ken":123,"ra":124,"▁the":125,"keniz":126,"ou":127,"▁c":128,"▁f":129,"▁l":130,"st":131,"re":132,"kenizer":133,"▁\n":134,"▁s":135,"▁in":136,"it":137,"de":138,"an":139,"▁o":140,"▁b":141,"▁tokenizer":142,"ll":143,"rain":144,"is":145,"▁n":146,"▁an":147,"▁'Ġ":148,"▁y":149,"▁T":150,"▁you":151,"me":152,"le":153,"ce":154,"▁of":155,"ch":156,"ion":157,"ts":158,"ex":159,"pu":160,"ge":161,"ro":162,"th":163,"ĠĠ":164,"▁d":165,"ata":166,"▁we":167,"▁on":168,"ode":169,"atase":170,"▁p":171,"▁u":172,":\n":173,"ar":174,"ct":175,"ed":176,"▁h":177,"▁lo":178,"to":179,"▁m":180,"▁ne":181,"▁and":182,"▁se":183,"▁for":184,"raining":185,"mp":186,"▁that":187,")\n":188,"un":189,"▁wh":190,"▁re":191,"▁Th":192,"rom":193,"00":194,"▁li":195,"pus":196,"▁is":197,"▁ex":198,"\"\"":199,"ame":200,"ill":201,"ent":202,"ation":203,"orpus":204,"▁can":205,"es":206,"ua":207,"▁1":208,"▁g":209,"▁i":210,"▁sp":211,"▁be":212,"▁your":213,"▁new":214,"ad":215,"ld":216,"train":217,"yth":218,"▁will":219,"▁use":220,"mple":221,"ly":222,"ve":223,"ran":224,"ith":225,"ataset":226,".\n":227,"et":228,"▁de":229,"▁from":230,"as":231,"ace":232,"ample":233,"ow":234,"t_":235,"▁=":236,"▁e":237,"▁it":238,"our":239,"odel":240,"\"]":241,"[\"":242,"_d":243,"al":244,"ke":245,"lf":246,"ot":247,"▁A":248,"▁training":249,"ere":250,"▁this":251,"atasets":252,"_c":253,"ast":254,"ec":255,"fun":256,"gua":257,"ur":258,"ut":259,"’s":260,"▁P":261,"▁at":262,"▁are":263,"erat":264,"▁corpus":265,"angua":266,"ction":267,"▁model":268,"func":269,"erator":270,"anguage":271,"ic":272,"▁C":273,"▁I":274,"ine":275,"ers":276,"▁token":277,"▁with":278,"▁language":279,"▁dataset":280,"ython":281,"all":282,"ave":283,"ok":284,"ver":285,"▁\"":286,"▁me":287,"▁tex":288,"▁al":289,"▁as":290,"▁co":291,"▁whi":292,"▁Python":293,"()":294,"mb":295,"okenizer":296,"pt":297,"ring":298,"▁(":299,"ize":300,"string":301,"▁see":302,"▁This":303,"000":304,"urn":305,"▁texts":306,"▁which":307,"_n":308,"ay":309,"for":310,"id":311,"ir":312,"lo":313,"ss":314,"size":315,"turn":316,"w_d":317,"ĊĠĠ":318,"▁F":319,"▁🤗":320,"▁train":321,"▁tokeniz":322,"raw_d":323,"▁same":324,"▁pro":325,"▁load":326,"▁The":327,"▁example":328,"\"\"\"":329,"▁spec":330,"raw_datasets":331,"▁speci":332,"\":":333,",\n":334,"_',":335,"_string":336,"bra":337,"ha":338,"ig":339,"ry":340,"s\n":341,"sp":342,"ust":343,"ক্":344,"▁st":345,"▁our":346,"atch":347,"▁fun":348,"▁fast":349,"▁by":350,"▁but":351,"▁tokenizer.":352,"▁not":353,"▁do":354,"▁one":355,"▁have":356,"tokenizer":357,"▁need":358,"▁list":359,"▁libra":360,"train\"]":361,"[\"train\"]":362,"▁\"ক্":363,"raw_datasets[\"train\"]":364,"▁function":365,"ain":366,"ake":367,"ble":368,"ds":369,"gg":370,"pre":371,"sfor":372,"’ll":373,"▁D":374,"▁H":375,"▁R":376,"self":377,"out":378,"▁code":379,"▁so":380,"and":381,"ant":382,"▁Training":383,"le_":384,"▁us":385,"arch":386,"▁look":387,"▁1000":388,"ransfor":389,"▁tokenization":390,"▁special":391,"▁library":392,"gging":393,"ransform":394,"),":395,"]\n":396,"`',":397,"act":398,"dent":399,"ew":400,"from":401,"ime":402,"lit":403,"ole_":404,"qu":405,"ub":406,"ugging":407,"wh":408,"Ġ',":409,"’t":410,"▁E":411,"▁G":412,"▁S":413,"▁W":414,"▁[":415,"▁ge":416,"▁'.":417,"▁'func":418,"▁'ĊĠĠ":419,"▁'_',":420,"▁want":421,"ory":422,"ould":423,"▁cre":424,"▁tokenizers":425,"▁name":426,"▁self":427,"▁when":428,"▁return":429,"▁gen":430,"▁if":431,"▁def":432,"_corpus":433,"func_string":434,"irst":435,"igh":436,"from_":437,"ole_func_string":438,"(',":439,"):\n":440,"Tokenizer":441,"a',":442,"ach":443,"el":444,"ist":445,"mm":446,"od":447,"os":448,"ost":449,"pl":450,"pen":451,"rin":452,"t',":453,"uto":454,"umb":455,"ven":456,"ving":457,"▁+":458,"▁L":459,"▁N":460,"▁O":461,"▁',":462,"▁or":463,"▁ran":464,"▁take":465,"▁av":466,"en(":467,"▁')":468,"search":469,"▁fo":470,"▁first":471,"▁into":472,"▁old":473,"llow":474,"▁'Ġ',":475,"▁To":476,"▁Transform":477,"cess":478,"▁prin":479,"▁go":480,"▁split":481,"trained":482,"ything":483,"▁Auto":484,"pter":485,"self',":486,"outpu":487,"▁some":488,"whole_func_string":489,"▁'.',":490,"▁creat":491,"▁generator":492,"umbers":493,"▁',',":494,"▁print":495,"▁AutoTokenizer":496,"\")\n":497,"(\"":498,"))\n":499,"-n":500,"-tokenizer":501,"-search":502,"0,":503,":',":504,"__":505,"_from_":506,"ck":507,"cr":508,"dd":509,"gor":510,"il":511,"just":512,"ms":513,"s,":514,"so":515,"ten":516,"training":517,"ul":518,"um":519,"▁2":520,"▁3":521,"▁B":522,"▁\"\"\"":523,"▁just":524,"▁time":525,"▁'size":526,"▁wor":527,"▁cor":528,"▁con":529,"▁call":530,"▁le":531,"▁inst":532,"iterator":533,"def":534,"▁batch":535,"▁tokenizer\n":536,"▁Tokenizer":537,"ge(":538,"ther":539,"▁we’ll":540,"▁only":541,"▁log":542,"tokeniz":543,"▁like":544,"▁exact":545,"▁used":546,"et’s":547,"et-tokenizer":548,"t_size":549,"▁each":550,"[\"whole_func_string":551,"▁As":552,"▁If":553,"▁In":554,"▁tokens":555,"very":556,"▁meth":557,"▁algor":558,"▁also":559,"_new":560,"hapter":561,"spon":562,"pretrained":563,"▁En":564,"▁GP":565,"▁We":566,"▁self.":567,"▁range(":568,"▁follow":569,"▁print(":570,"-net-tokenizer":571,"-search-net-tokenizer":572,"_from_iterator":573,"▁'size',":574,"[\"whole_func_string\"]":575,"▁algorith":576,"_new_from_iterator":577,"▁following":578,"(ex":579,"-c":580,"Lay":581,"Us":582,"][\"whole_func_string\"]":583,"_tokenizer":584,"able":585,"aving":586,"ail":587,"bi":588,"bo":589,"b',":590,"code":591,"di":592,"eigh":593,"e(ex":594,"ebo":595,"face":596,"ie":597,"if":598,"las":599,"mory":600,"oc":601,"ri":602,"rit":603,"s:":604,"tain":605,"ure":606,"vi":607,"wo":608,"weigh":609,"▁#":610,"▁5":611,"▁7":612,"▁9":613,"▁M":614,"▁_":615,"▁raw_datasets[\"train\"]":616,"▁qu":617,"▁very":618,"▁tas":619,"▁all":620,"ents":621,"▁'(',":622,"▁'a',":623,"▁won":624,"▁would":625,"▁writ":626,"ses":627,"ized":628,"▁them":629,"out_":630,"▁ch":631,"▁len(":632,"▁scr":633,"▁indent":634,"▁'Ġt":635,"▁'Ġ`',":636,"▁'Ġself',":637,"the":638,"ĠĠĠĠ":639,"▁pa":640,"▁un":641,"arn":642,"▁most":643,"ual":644,"▁space":645,"rand":646,"▁it’s":647,"ourse":648,"ote":649,"inear":650,"▁memory":651,"():\n":652,"▁process":653,"▁star":654,"▁notebo":655,"▁Datasets":656,"▁using":657,"uggingface":658,"▁['":659,"▁get":660,"▁'):',":661,"▁create":662,"umbers',":663,"▁AutoTokenizer.":664,"training_corpus":665,"▁contain":666,"▁learn":667,"def',":668,"▁Tokenizers":669,"tokenize(ex":670,"▁method":671,"▁range(0,":672,"▁print(l":673,"▁algorithm":674,"Layer":675,"bias":676,"▁task":677,"▁won’t":678,"▁written":679,"out_ms":680,"▁scratch":681,"▁indentation":682,"▁notebook":683,"tokenize(example":684,"!\n":685,"''":686,"']\n":687,"+',":688,"-2":689,"Add":690,"T-2":691,"You":692,"[i":693,"_to":694,"_pretrained":695,"ab":696,"ade":697,"du":698,"ect":699,"ead":700,"fo":701,"gl":702,"ire":703,"ive":704,"min":705,"numbers',":706,"ol":707,"op":708,"oid":709,"pos":710,"s',":711,"ter":712,"wor":713,"yp":714,"’re":715,"▁4":716,"▁6":717,"▁8":718,"▁:":719,"▁r":720,"▁Ġ":721,"▁Us":722,"▁You":723,"▁typ":724,"inpu":725,"▁act":726,"ence":727,"▁'Ċ":728,"▁'`',":729,"ath":730,"ature":731,"ort":732,"one":733,"semb":734,"▁line":735,"▁let’s":736,"return":737,"respon":738,"▁sh":739,"▁sub":740,"▁inpu":741,"▁info":742,"itory":743,"▁bit":744,"ish":745,"▁anything":746,"▁'Ġ+',":747,"▁'Ġreturn":748,"les":749,"▁data":750,"▁once":751,"▁pre":752,"▁here":753,"▁hand":754,"▁hel":755,"▁loop":756,"mport":757,"▁repre":758,"ame',":759,"entation":760,"▁1,":761,"▁import":762,"train_new_from_iterator":763,"t_id":764,"t_training_corpus":765,"▁ever":766,"▁iterator":767,"_dataset":768,"▁training_corpus":769,"_cb":770,"_code":771,"▁models":772,"ication":773,"▁Chapter":774,"▁It":775,"▁comp":776,"▁comm":777,"_name',":778,"load":779,"▁Face":780,"▁Fast":781,"▁load_dataset":782,"\"\"\"',":783,"▁does":784,"▁one\n":785,"▁needs":786,"▁lists":787,"raw_datasets[\"train\"]),":788,"▁Here":789,"▁Hugging":790,"▁Rust":791,"▁1000][\"whole_func_string\"]":792,"▁tokenization\n":793,"▁get_training_corpus":794,"▁'func_code":795,"▁'ĊĠĠĠ',":796,"▁'ĊĠĠĠĠĠĠ":797,"istic":798,"▁avail":799,"▁avoid":800,"▁old_tokenizer":801,"▁Transformer":802,"▁Transformers":803,"output',":804,"output_size":805,"(\"code":806,"▁2,":807,"▁timeout_ms":808,"▁work":809,"▁instead":810,"▁Engl":811,"▁GPT-2":812,"-search-net-tokenizer\")\n":813,"▁len(raw_datasets[\"train\"]),":814,"▁spaces":815,"uggingface-c":816,"tokenize(example)\n":817,"aded":818,"pository":819,"▁Using":820,"▁subwor":821,"▁info_cb":822,"▁'Ġreturn',":823,"▁help":824,"▁repres":825,"t_idx":826,"▁everything":827,"▁'ĊĠĠĠĠĠĠĠ',":828,"▁available":829,"▁English":830,"\",":831,"'\n":832,"'s":833,"(g":834,"()\n":835,"(self":836,").":837,"-t":838,"12":839,"3\n":840,"=',":841,"AY":842,"DA":843,"KAY":844,"LP":845,"Linear":846,"Net":847,"PI":848,"Se":849,"UDA":850,"_h":851,"_lo":852,"__',":853,"`.":854,"aw":855,"ait":856,"age":857,"ack":858,"bj":859,"b']\n":860,"cc":861,"ci":862,"con":863,"cce":864,"cstring":865,"clas":866,"d',":867,"ding":868,"ea":869,"ep":870,"est":871,"eature":872,"ff":873,"gin":874,"gra":875,"hen":876,"huggingface-c":877,"ice":878,"ild":879,"imple":880,"iving":881,"lin":882,"ling":883,"low":884,"lum":885,"mal":886,"mall":887,"ments":888,"mized":889,"nt":890,"n',":891,"n’t":892,"other":893,"rch":894,"s.":895,"s:\n":896,"sul":897,"ting":898,"tere":899,"uge":900,"ues":901,"ules":902,"uild":903,"vid":904,"way":905,"wice":906,"x',":907,"ymb":908,"zer":909,"{\n":910,"র\":":911,"্র\":":912,"▁Q":913,"▁`":914,"▁x":915,"▁}":916,"▁Ċ":917,"▁—":918,"▁pu":919,"▁)\n":920,"▁ent":921,"▁ĊĠĠ":922,"▁output_size":923,"▁transform":924,"▁two":925,"▁twice":926,"init":927,"inut":928,"▁ad":929,"▁able":930,"▁add":931,"en))\n":932,"enote":933,"er.\n":934,"▁then":935,"▁thing":936,"▁thre":937,"▁than":938,"▁there":939,"▁torch":940,"▁'self',":941,"▁'__":942,"▁'b',":943,"▁'weigh":944,"▁'bias":945,"▁'Add":946,"ater":947,"▁was":948,"▁way":949,"▁wait":950,"ore":951,"orch":952,"ormal":953,"kens',":954,"rall":955,"ract":956,"▁cha":957,"▁chapter":958,"▁clas":959,"▁fi":960,"▁few":961,"▁feature":962,"▁later":963,"rect":964,"repository":965,"▁su":966,"▁sent":967,"▁sample":968,"▁sour":969,"▁save":970,"▁saw":971,"▁small":972,"▁symb":973,"▁in,":974,"▁indi":975,"▁intere":976,"aning":977,"ance":978,"▁other":979,"▁obj":980,"▁bo":981,"▁blo":982,"▁brand":983,"▁tokenizer,":984,"lly":985,"▁nex":986,"▁any":987,"▁another":988,"▁'Ġad":989,"▁'Ġand":990,"▁'Ġa',":991,"▁'Ġb',":992,"▁'Ġthe":993,"▁'Ġdef',":994,"▁'Ġnumbers',":995,"▁'Ġ\"\"\"',":996,"▁'Ġoutput',":997,"▁'Ġ=',":998,"▁'Ġ__',":999,"▁'Ġb']\n":1000,"▁'Ġx',":1001,"▁yie":1002,"▁you’re":1003,"lements":1004,"ions":1005,"rodu":1006,"▁dis":1007,"▁denote":1008,"odeSe":1009,"▁par":1010,"▁pow":1011,"▁pic":1012,"ary":1013,"arat":1014,"aring":1015,"▁has":1016,"▁how":1017,"▁having":1018,"▁huge":1019,"▁loaded":1020,"▁mill":1021,"▁make":1022,"▁minut":1023,"▁set":1024,"▁section":1025,"uning":1026,"▁why":1027,"▁what":1028,"▁requ":1029,"▁respon":1030,"▁resul":1031,"▁That":1032,"push":1033,"▁exec":1034,"es)":1035,"es,":1036,"ually":1037,"▁1.":1038,"▁10":1039,"▁12":1040,"▁gra":1041,"▁ident":1042,"▁newlin":1043,"ly,":1044,"▁depen":1045,"▁deter":1046,"▁even":1047,"▁elements":1048,"_doc":1049,"▁API":1050,"▁this,":1051,"eck":1052,"▁corpus\n":1053,"▁corpus.\n":1054,"ction\n":1055,"▁Course":1056,"▁CUDA":1057,"▁CodeSe":1058,"ine-t":1059,"▁tokens\n":1060,"▁language.":1061,"▁dataset.":1062,"ally":1063,"▁alway":1064,"▁colum":1065,"▁(\n":1066,"▁(th":1067,"fore":1068,"▁For":1069,"▁train_new_from_iterator":1070,"▁tokenized":1071,"▁provi":1072,"▁progra":1073,"▁example,":1074,"▁example:\n":1075,"▁specif":1076,"_string',":1077,"space":1078,"▁stat":1079,"▁tokenizer.tokenize(example)\n":1080,"▁docstring":1081,"▁\"ক্ট":1082,"▁\"ক্ত":1083,"▁functions":1084,"▁code-search-net-tokenizer":1085,"archNet":1086,"▁1000)\n":1087,"▁library\n":1088,"▁Even":1089,"▁Saving":1090,"▁'func_doc":1091,"▁tokenizers'":1092,"▁namespace":1093,"▁self,":1094,"▁returns":1095,"▁define":1096,"ight":1097,"from_pretrained":1098,"ist(g":1099,"mming":1100,"place":1101,"plication":1102,"▁Let’s":1103,"▁Linear":1104,"▁Note":1105,"▁NLP":1106,"▁Open":1107,"▁')',":1108,"▁going":1109,"▁generator,":1110,"__(self":1111,"umentation":1112,"▁3,":1113,"▁Build":1114,"▁\"\"\"\n":1115,"▁words":1116,"▁correct":1117,"▁instance":1118,"▁batches":1119,"▁login":1120,"▁exactly":1121,"t_size,":1122,"▁Assemb":1123,"▁We’ll":1124,"▁self.weigh":1125,"▁self.bias":1126,"iece":1127,"ify":1128,"ries":1129,"wo',":1130,"▁7,":1131,"▁Model":1132,"▁raw_datasets[\"train\"]\n":1133,"▁raw_datasets[\"train\"][i":1134,"ses(":1135,"▁check":1136,"▁'Ġtorch":1137,"▁'Ġtwo',":1138,"▁parall":1139,"▁processing":1140,"▁start_idx":1141,"▁['def',":1142,"▁AutoTokenizer.train_new_from_iterator":1143,"▁AutoTokenizer.from_pretrained":1144,"▁contains":1145,"▁method:\n":1146,"▁print(len(":1147,"▁print(list(g":1148,"▁algorithm.":1149,"▁indentation,":1150,"▁notebook,":1151,"▁notebook_lo":1152,"_tokens',":1153,"_pretrained()":1154,"ab\n":1155,"▁rules":1156,"▁type":1157,"input',":1158,"▁actually":1159,"▁'Ċ',":1160,"ather":1161,"responses(":1162,"▁should":1163,"▁inputs":1164,"▁once.":1165,"▁1,000":1166,"▁It’s":1167,"▁command":1168,"▁1000][\"whole_func_string\"]\n":1169,"▁get_training_corpus():\n":1170,"▁old_tokenizer.":1171,"(\"code-search-net-tokenizer\")\n":1172,"▁timeout_ms=":1173,"▁working":1174,"▁subwords":1175,"▁represents":1176,"_hub":1177,"conds":1178,"ccep":1179,"estion":1180,"huggingface-course":1181,"vidual":1182,"zeros":1183,"▁pure":1184,"▁three":1185,"▁torch.":1186,"▁'__(',":1187,"▁'bias',":1188,"▁'Add',":1189,"▁charact":1190,"▁samples":1191,"▁source":1192,"▁symbol":1193,"▁individual":1194,"▁interest":1195,"▁object":1196,"▁block":1197,"▁next":1198,"▁'Ġadd',":1199,"▁'Ġand',":1200,"▁'Ġthe',":1201,"▁yield":1202,"roduction\n":1203,"▁powers":1204,"▁pick":1205,"▁require":1206,"▁results":1207,"▁execut":1208,"▁identify":1209,"▁newlines,":1210,"▁depend":1211,"▁determin":1212,"▁CodeSearchNet":1213,"ine-tuning":1214,"▁always":1215,"▁column":1216,"▁provide":1217,"▁programming":1218,"▁code-search-net-tokenizer,":1219,"▁'func_documentation":1220,"▁LinearLayer":1221,"__(self,":1222,"▁Building":1223,"▁correctly":1224,"▁instance,":1225,"▁Assembling":1226,"▁'Ġtorch',":1227,"▁parallel":1228,"▁AutoTokenizer.train_new_from_iterator()":1229,"▁AutoTokenizer.from_pretrained(\"":1230,"▁print(list(gen))\n":1231,"▁notebook_login":1232}