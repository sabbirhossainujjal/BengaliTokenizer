#version: 0.2
â– t
i n
â– a
e n
e r
â–t h
' ,
â–t o
â– '
a t
â– w
o r
o n
s e
i z
in g
k en
r a
â–th e
ken iz
o u
â– c
â– f
â– l
s t
r e
keniz er
â– 

â– s
â– in
i t
d e
a n
â– o
â– b
â–to kenizer
l l
ra in
i s
â– n
â–a n
â–' Ä 
â– y
â– T
â–y ou
m e
l e
c e
â–o f
c h
i on
t s
e x
p u
g e
r o
t h
Ä  Ä 
â– d
at a
â–w e
â– on
o de
ata se
â– p
â– u
: 

a r
c t
e d
â– h
â–l o
t o
â– m
â–n e
â–an d
â– se
â–f or
rain ing
m p
â–th at
) 

u n
â–w h
â– re
â–T h
ro m
0 0
â–l i
pu s
â– is
â– ex
" "
a me
i ll
en t
at ion
or pus
â–c an
e s
u a
â– 1
â– g
â– i
â–s p
â–b e
â–you r
â–ne w
a d
l d
t rain
y th
â–w ill
â–u se
mp le
l y
v e
ra n
it h
atase t
. 

e t
â– de
â–f rom
a s
a ce
a mple
o w
t _
â– =
â– e
â– it
ou r
ode l
" ]
[ "
_ d
a l
k e
l f
o t
â– A
â–t raining
er e
â–th is
atase ts
_ c
a st
e c
f un
g ua
u r
u t
â€™ s
â– P
â–a t
â–a re
er at
â–c orpus
an gua
ct ion
â–m odel
fun c
erat or
angua ge
i c
â– C
â– I
in e
er s
â–to ken
â–w ith
â–l anguage
â–d ataset
yth on
a ll
a ve
o k
v er
â– "
â– me
â–t ex
â–a l
â–a s
â–c o
â–wh i
â–P ython
( )
m b
o kenizer
p t
r ing
â– (
iz e
st ring
â–se e
â–Th is
00 0
ur n
â–tex ts
â–whi ch
_ n
a y
f or
i d
i r
l o
s s
s ize
t urn
w _d
ÄŠ Ä Ä 
â– F
â– ğŸ¤—
â–t rain
â–to keniz
ra w_d
â–s ame
â–p ro
â–lo ad
â–Th e
â–ex ample
"" "
â–sp ec
raw_d atasets
â–spec i
" :
, 

_ ',
_ string
b ra
h a
i g
r y
s 

s p
u st
à¦• à§
â– st
â– our
at ch
â–f un
â–f ast
â–b y
â–b ut
â–tokenizer .
â–n ot
â–d o
â–on e
â–h ave
to kenizer
â–ne ed
â–li st
â–li bra
train "]
[" train"]
â–" à¦•à§
raw_datasets ["train"]
â–fun ction
a in
a ke
b le
d s
g g
p re
s for
â€™ ll
â– D
â– H
â– R
se lf
ou t
â–c ode
â–s o
an d
an t
â–T raining
le _
â–u s
ar ch
â–lo ok
â–1 000
ran sfor
â–tokeniz ation
â–speci al
â–libra ry
gg ing
ransfor m
) ,
] 

` ',
a ct
d ent
e w
f rom
i me
l it
o le_
q u
u b
u gging
w h
Ä  ',
â€™ t
â– E
â– G
â– S
â– W
â– [
â– ge
â–' .
â–' func
â–' ÄŠÄ Ä 
â–' _',
â–w ant
or y
ou ld
â–c re
â–tokenizer s
â–n ame
â–se lf
â–wh en
â–re turn
â–g en
â–i f
â–de f
_c orpus
func _string
ir st
ig h
from _
ole_ func_string
( ',
) :

T okenizer
a ',
a ch
e l
i st
m m
o d
o s
o st
p l
p en
r in
t ',
u to
u mb
v en
v ing
â– +
â– L
â– N
â– O
â– ',
â– or
â– ran
â–t ake
â–a v
en (
â–' )
se arch
â–f o
â–f irst
â–in to
â–o ld
ll ow
â–'Ä  ',
â–T o
â–T ransform
ce ss
â–p rin
â–g o
â–sp lit
train ed
yth ing
â–A uto
pt er
self ',
out pu
â–so me
wh ole_func_string
â–'. ',
â–cre at
â–gen erator
umb ers
â–', ',
â–prin t
â–Auto Tokenizer
" )

( "
) )

- n
- tokenizer
- search
0 ,
: ',
_ _
_ from_
c k
c r
d d
g or
i l
j ust
m s
s ,
s o
t en
t raining
u l
u m
â– 2
â– 3
â– B
â– """
â– just
â–t ime
â–' size
â–w or
â–c or
â–c on
â–c all
â–l e
â–in st
it erator
de f
â–b atch
â–tokenizer 

â–T okenizer
ge (
th er
â–we â€™ll
â–on ly
â–lo g
to keniz
â–li ke
â–ex act
â–use d
et â€™s
et -tokenizer
t_ size
â–e ach
[" whole_func_string
â–A s
â–I f
â–I n
â–token s
ver y
â–me th
â–al gor
â–al so
_n ew
ha pter
sp on
pre trained
â–E n
â–G P
â–W e
â–self .
â–ran ge(
â–fo llow
â–print (
-n et-tokenizer
-search -net-tokenizer
_from_ iterator
â–'size ',
["whole_func_string "]
â–algor ith
_new _from_iterator
â–follow ing
( ex
- c
L ay
U s
] ["whole_func_string"]
_ tokenizer
a ble
a ving
a il
b i
b o
b ',
c ode
d i
e igh
e (ex
e bo
f ace
i e
i f
l as
m ory
o c
r i
r it
s :
t ain
u re
v i
w o
w eigh
â– #
â– 5
â– 7
â– 9
â– M
â– _
â– raw_datasets["train"]
â– qu
â– very
â–t as
â–a ll
en ts
â–' (',
â–' a',
â–w on
â–w ould
â–w rit
se s
iz ed
â–the m
ou t_
â–c h
â–l en(
â–s cr
â–in dent
â–'Ä  t
â–'Ä  `',
â–'Ä  self',
th e
Ä Ä  Ä Ä 
â–p a
â–u n
ar n
â–m ost
ua l
â–sp ace
ran d
â–it â€™s
our se
ot e
ine ar
â–me mory
() :

â–pro cess
â–st ar
â–not ebo
â–D atasets
â–us ing
ugging face
â–[ '
â–ge t
â–') :',
â–creat e
umbers ',
â–AutoTokenizer .
training _corpus
â–con tain
â–le arn
def ',
â–Tokenizer s
tokeniz e(ex
â–meth od
â–range( 0,
â–print( l
â–algorith m
Lay er
bi as
â–tas k
â–won â€™t
â–writ ten
out_ ms
â–scr atch
â–indent ation
â–notebo ok
tokenize(ex ample
! 

' '
' ]

+ ',
- 2
A dd
T -2
Y ou
[ i
_ to
_ pretrained
a b
a de
d u
e ct
e ad
f o
g l
i re
i ve
m in
n umbers',
o l
o p
o id
p os
s ',
t er
w or
y p
â€™ re
â– 4
â– 6
â– 8
â– :
â– r
â– Ä 
â– Us
â– You
â–t yp
in pu
â–a ct
en ce
â–' ÄŠ
â–' `',
at h
at ure
or t
on e
se mb
â–l ine
â–l etâ€™s
re turn
re spon
â–s h
â–s ub
â–in pu
â–in fo
it ory
â–b it
is h
â–an ything
â–'Ä  +',
â–'Ä  return
le s
â–d ata
â–on ce
â–p re
â–h ere
â–h and
â–h el
â–lo op
mp ort
â–re pre
ame ',
ent ation
â–1 ,
â–i mport
train _new_from_iterator
t_ id
t_ training_corpus
â–e ver
â–it erator
_d ataset
â–training _corpus
_c b
_c ode
â–model s
ic ation
â–C hapter
â–I t
â–co mp
â–co mm
_n ame',
lo ad
â–F ace
â–F ast
â–load _dataset
""" ',
â–do es
â–one 

â–need s
â–list s
raw_datasets["train"] ),
â–H ere
â–H ugging
â–R ust
â–1000 ]["whole_func_string"]
â–tokenization 

â–ge t_training_corpus
â–'func _code
â–'ÄŠÄ Ä  Ä ',
â–'ÄŠÄ Ä  Ä Ä Ä Ä 
ist ic
â–av ail
â–av oid
â–old _tokenizer
â–Transform er
â–Transform ers
outpu t',
outpu t_size
(" code
â–2 ,
â–time out_ms
â–wor k
â–inst ead
â–En gl
â–GP T-2
-search-net-tokenizer ")

â–len( raw_datasets["train"]),
â–space s
uggingface -c
tokenize(example )

ade d
pos itory
â–Us ing
â–sub wor
â–info _cb
â–'Ä return ',
â–hel p
â–repre s
t_id x
â–ever ything
â–'ÄŠÄ Ä Ä Ä Ä Ä  Ä ',
â–avail able
â–Engl ish
" ,
' 

' s
( g
( )

( self
) .
- t
1 2
3 

= ',
A Y
D A
K AY
L P
L inear
N et
P I
S e
U DA
_ h
_ lo
_ _',
` .
a w
a it
a ge
a ck
b j
b ']

c c
c i
c on
c ce
c string
c las
d ',
d ing
e a
e p
e st
e ature
f f
g in
g ra
h en
h uggingface-c
i ce
i ld
i mple
i ving
l in
l ing
l ow
l um
m al
m all
m ents
m ized
n t
n ',
n â€™t
o ther
r ch
s .
s :

s ul
t ing
t ere
u ge
u es
u les
u ild
v id
w ay
w ice
x ',
y mb
z er
{ 

à¦° ":
à§ à¦°":
â– Q
â– `
â– x
â– }
â– ÄŠ
â– â€”
â– pu
â– )

â– ent
â– ÄŠÄ Ä 
â– output_size
â–t ransform
â–t wo
â–t wice
in it
in ut
â–a d
â–a ble
â–a dd
en ))

en ote
er .

â–th en
â–th ing
â–th re
â–th an
â–th ere
â–to rch
â–' self',
â–' __
â–' b',
â–' weigh
â–' bias
â–' Add
at er
â–w as
â–w ay
â–w ait
or e
or ch
or mal
ken s',
ra ll
ra ct
â–c ha
â–c hapter
â–c las
â–f i
â–f ew
â–f eature
â–l ater
re ct
re pository
â–s u
â–s ent
â–s ample
â–s our
â–s ave
â–s aw
â–s mall
â–s ymb
â–in ,
â–in di
â–in tere
an ing
an ce
â–o ther
â–o bj
â–b o
â–b lo
â–b rand
â–tokenizer ,
ll y
â–n ex
â–an y
â–an other
â–'Ä  ad
â–'Ä  and
â–'Ä  a',
â–'Ä  b',
â–'Ä  the
â–'Ä  def',
â–'Ä  numbers',
â–'Ä  """',
â–'Ä  output',
â–'Ä  =',
â–'Ä  __',
â–'Ä  b']

â–'Ä  x',
â–y ie
â–you â€™re
le ments
ion s
ro du
â–d is
â–d enote
ode Se
â–p ar
â–p ow
â–p ic
ar y
ar at
ar ing
â–h as
â–h ow
â–h aving
â–h uge
â–lo aded
â–m ill
â–m ake
â–m inut
â–se t
â–se ction
un ing
â–wh y
â–wh at
â–re qu
â–re spon
â–re sul
â–Th at
pus h
â–ex ec
es )
es ,
ua lly
â–1 .
â–1 0
â–1 2
â–g ra
â–i dent
â–new lin
ly ,
â–de pen
â–de ter
â–e ven
â–e lements
_d oc
â–A PI
â–this ,
ec k
â–corpus 

â–corpus .

ction 

â–C ourse
â–C UDA
â–C odeSe
ine -t
â–token s

â–language .
â–dataset .
all y
â–al way
â–co lum
â–( 

â–( th
for e
â–F or
â–train _new_from_iterator
â–tokeniz ed
â–pro vi
â–pro gra
â–example ,
â–example :

â–speci f
_string ',
sp ace
â–st at
â–tokenizer. tokenize(example)

â–do cstring
â–"à¦•à§ à¦Ÿ
â–"à¦•à§ à¦¤
â–function s
â–code -search-net-tokenizer
arch Net
â–1000 )

â–library 

â–E ven
â–S aving
â–'func _doc
â–tokenizers '
â–name space
â–self ,
â–return s
â–def ine
igh t
from_ pretrained
ist (g
mm ing
pl ace
pl ication
â–L etâ€™s
â–L inear
â–N ote
â–N LP
â–O pen
â–') ',
â–go ing
â–generator ,
__ (self
um entation
â–3 ,
â–B uild
â–""" 

â–wor ds
â–cor rect
â–inst ance
â–batch es
â–log in
â–exact ly
t_size ,
â–As semb
â–We â€™ll
â–self. weigh
â–self. bias
ie ce
if y
ri es
wo ',
â–7 ,
â–M odel
â–raw_datasets["train"] 

â–raw_datasets["train"] [i
ses (
â–ch eck
â–'Ä t orch
â–'Ä t wo',
â–pa rall
â–process ing
â–star t_idx
â–[' def',
â–AutoTokenizer. train_new_from_iterator
â–AutoTokenizer. from_pretrained
â–contain s
â–method :

â–print(l en(
â–print(l ist(g
â–algorithm .
â–indentation ,
â–notebook ,
â–notebook _lo
_to kens',
_pretrained ()
ab 

â–r ules
â–typ e
inpu t',
â–act ually
â–'ÄŠ ',
ath er
respon ses(
â–sh ould
â–inpu ts
â–once .
â–1, 000
â–It â€™s
â–comm and
â–1000]["whole_func_string"] 

â–get_training_corpus ():

â–old_tokenizer .
("code -search-net-tokenizer")

â–timeout_ms =
â–work ing
â–subwor ds
â–repres ents
_h ub
con ds
cce p
est ion
huggingface-c ourse
vid ual
zer os
â–pu re
â–thre e
â–torch .
â–'__ (',
â–'bias ',
â–'Add ',
â–cha ract
â–sample s
â–sour ce
â–symb ol
â–indi vidual
â–intere st
â–obj ect
â–blo ck
â–nex t
â–'Ä ad d',
â–'Ä and ',
â–'Ä the ',
â–yie ld
rodu ction

â–pow ers
â–pic k
â–requ ire
â–resul ts
â–exec ut
â–ident ify
â–newlin es,
â–depen d
â–deter min
â–CodeSe archNet
ine-t uning
â–alway s
â–colum n
â–provi de
â–progra mming
â–code-search-net-tokenizer ,
â–'func_doc umentation
â–Linear Layer
__(self ,
â–Build ing
â–correct ly
â–instance ,
â–Assemb ling
â–'Ä torch ',
â–parall el
â–AutoTokenizer.train_new_from_iterator ()
â–AutoTokenizer.from_pretrained ("
â–print(list(g en))

â–notebook_lo gin
