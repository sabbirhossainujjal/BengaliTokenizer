#version: 0.2
▁ t
i n
▁ a
e n
e r
▁t h
' ,
▁t o
▁ '
a t
▁ w
o r
o n
s e
i z
in g
k en
r a
▁th e
ken iz
o u
▁ c
▁ f
▁ l
s t
r e
keniz er
▁ 

▁ s
▁ in
i t
d e
a n
▁ o
▁ b
▁to kenizer
l l
ra in
i s
▁ n
▁a n
▁' Ġ
▁ y
▁ T
▁y ou
m e
l e
c e
▁o f
c h
i on
t s
e x
p u
g e
r o
t h
Ġ Ġ
▁ d
at a
▁w e
▁ on
o de
ata se
▁ p
▁ u
: 

a r
c t
e d
▁ h
▁l o
t o
▁ m
▁n e
▁an d
▁ se
▁f or
rain ing
m p
▁th at
) 

u n
▁w h
▁ re
▁T h
ro m
0 0
▁l i
pu s
▁ is
▁ ex
" "
a me
i ll
en t
at ion
or pus
▁c an
e s
u a
▁ 1
▁ g
▁ i
▁s p
▁b e
▁you r
▁ne w
a d
l d
t rain
y th
▁w ill
▁u se
mp le
l y
v e
ra n
it h
atase t
. 

e t
▁ de
▁f rom
a s
a ce
a mple
o w
t _
▁ =
▁ e
▁ it
ou r
ode l
" ]
[ "
_ d
a l
k e
l f
o t
▁ A
▁t raining
er e
▁th is
atase ts
_ c
a st
e c
f un
g ua
u r
u t
’ s
▁ P
▁a t
▁a re
er at
▁c orpus
an gua
ct ion
▁m odel
fun c
erat or
angua ge
i c
▁ C
▁ I
in e
er s
▁to ken
▁w ith
▁l anguage
▁d ataset
yth on
a ll
a ve
o k
v er
▁ "
▁ me
▁t ex
▁a l
▁a s
▁c o
▁wh i
▁P ython
( )
m b
o kenizer
p t
r ing
▁ (
iz e
st ring
▁se e
▁Th is
00 0
ur n
▁tex ts
▁whi ch
_ n
a y
f or
i d
i r
l o
s s
s ize
t urn
w _d
Ċ ĠĠ
▁ F
▁ 🤗
▁t rain
▁to keniz
ra w_d
▁s ame
▁p ro
▁lo ad
▁Th e
▁ex ample
"" "
▁sp ec
raw_d atasets
▁spec i
" :
, 

_ ',
_ string
b ra
h a
i g
r y
s 

s p
u st
ক ্
▁ st
▁ our
at ch
▁f un
▁f ast
▁b y
▁b ut
▁tokenizer .
▁n ot
▁d o
▁on e
▁h ave
to kenizer
▁ne ed
▁li st
▁li bra
train "]
[" train"]
▁" ক্
raw_datasets ["train"]
▁fun ction
a in
a ke
b le
d s
g g
p re
s for
’ ll
▁ D
▁ H
▁ R
se lf
ou t
▁c ode
▁s o
an d
an t
▁T raining
le _
▁u s
ar ch
▁lo ok
▁1 000
ran sfor
▁tokeniz ation
▁speci al
▁libra ry
gg ing
ransfor m
) ,
] 

` ',
a ct
d ent
e w
f rom
i me
l it
o le_
q u
u b
u gging
w h
Ġ ',
’ t
▁ E
▁ G
▁ S
▁ W
▁ [
▁ ge
▁' .
▁' func
▁' ĊĠĠ
▁' _',
▁w ant
or y
ou ld
▁c re
▁tokenizer s
▁n ame
▁se lf
▁wh en
▁re turn
▁g en
▁i f
▁de f
_c orpus
func _string
ir st
ig h
from _
ole_ func_string
( ',
) :

T okenizer
a ',
a ch
e l
i st
m m
o d
o s
o st
p l
p en
r in
t ',
u to
u mb
v en
v ing
▁ +
▁ L
▁ N
▁ O
▁ ',
▁ or
▁ ran
▁t ake
▁a v
en (
▁' )
se arch
▁f o
▁f irst
▁in to
▁o ld
ll ow
▁'Ġ ',
▁T o
▁T ransform
ce ss
▁p rin
▁g o
▁sp lit
train ed
yth ing
▁A uto
pt er
self ',
out pu
▁so me
wh ole_func_string
▁'. ',
▁cre at
▁gen erator
umb ers
▁', ',
▁prin t
▁Auto Tokenizer
" )

( "
) )

- n
- tokenizer
- search
0 ,
: ',
_ _
_ from_
c k
c r
d d
g or
i l
j ust
m s
s ,
s o
t en
t raining
u l
u m
▁ 2
▁ 3
▁ B
▁ """
▁ just
▁t ime
▁' size
▁w or
▁c or
▁c on
▁c all
▁l e
▁in st
it erator
de f
▁b atch
▁tokenizer 

▁T okenizer
ge (
th er
▁we ’ll
▁on ly
▁lo g
to keniz
▁li ke
▁ex act
▁use d
et ’s
et -tokenizer
t_ size
▁e ach
[" whole_func_string
▁A s
▁I f
▁I n
▁token s
ver y
▁me th
▁al gor
▁al so
_n ew
ha pter
sp on
pre trained
▁E n
▁G P
▁W e
▁self .
▁ran ge(
▁fo llow
▁print (
-n et-tokenizer
-search -net-tokenizer
_from_ iterator
▁'size ',
["whole_func_string "]
▁algor ith
_new _from_iterator
▁follow ing
( ex
- c
L ay
U s
] ["whole_func_string"]
_ tokenizer
a ble
a ving
a il
b i
b o
b ',
c ode
d i
e igh
e (ex
e bo
f ace
i e
i f
l as
m ory
o c
r i
r it
s :
t ain
u re
v i
w o
w eigh
▁ #
▁ 5
▁ 7
▁ 9
▁ M
▁ _
▁ raw_datasets["train"]
▁ qu
▁ very
▁t as
▁a ll
en ts
▁' (',
▁' a',
▁w on
▁w ould
▁w rit
se s
iz ed
▁the m
ou t_
▁c h
▁l en(
▁s cr
▁in dent
▁'Ġ t
▁'Ġ `',
▁'Ġ self',
th e
ĠĠ ĠĠ
▁p a
▁u n
ar n
▁m ost
ua l
▁sp ace
ran d
▁it ’s
our se
ot e
ine ar
▁me mory
() :

▁pro cess
▁st ar
▁not ebo
▁D atasets
▁us ing
ugging face
▁[ '
▁ge t
▁') :',
▁creat e
umbers ',
▁AutoTokenizer .
training _corpus
▁con tain
▁le arn
def ',
▁Tokenizer s
tokeniz e(ex
▁meth od
▁range( 0,
▁print( l
▁algorith m
Lay er
bi as
▁tas k
▁won ’t
▁writ ten
out_ ms
▁scr atch
▁indent ation
▁notebo ok
tokenize(ex ample
! 

' '
' ]

+ ',
- 2
A dd
T -2
Y ou
[ i
_ to
_ pretrained
a b
a de
d u
e ct
e ad
f o
g l
i re
i ve
m in
n umbers',
o l
o p
o id
p os
s ',
t er
w or
y p
’ re
▁ 4
▁ 6
▁ 8
▁ :
▁ r
▁ Ġ
▁ Us
▁ You
▁t yp
in pu
▁a ct
en ce
▁' Ċ
▁' `',
at h
at ure
or t
on e
se mb
▁l ine
▁l et’s
re turn
re spon
▁s h
▁s ub
▁in pu
▁in fo
it ory
▁b it
is h
▁an ything
▁'Ġ +',
▁'Ġ return
le s
▁d ata
▁on ce
▁p re
▁h ere
▁h and
▁h el
▁lo op
mp ort
▁re pre
ame ',
ent ation
▁1 ,
▁i mport
train _new_from_iterator
t_ id
t_ training_corpus
▁e ver
▁it erator
_d ataset
▁training _corpus
_c b
_c ode
▁model s
ic ation
▁C hapter
▁I t
▁co mp
▁co mm
_n ame',
lo ad
▁F ace
▁F ast
▁load _dataset
""" ',
▁do es
▁one 

▁need s
▁list s
raw_datasets["train"] ),
▁H ere
▁H ugging
▁R ust
▁1000 ]["whole_func_string"]
▁tokenization 

▁ge t_training_corpus
▁'func _code
▁'ĊĠĠ Ġ',
▁'ĊĠĠ ĠĠĠĠ
ist ic
▁av ail
▁av oid
▁old _tokenizer
▁Transform er
▁Transform ers
outpu t',
outpu t_size
(" code
▁2 ,
▁time out_ms
▁wor k
▁inst ead
▁En gl
▁GP T-2
-search-net-tokenizer ")

▁len( raw_datasets["train"]),
▁space s
uggingface -c
tokenize(example )

ade d
pos itory
▁Us ing
▁sub wor
▁info _cb
▁'Ġreturn ',
▁hel p
▁repre s
t_id x
▁ever ything
▁'ĊĠĠĠĠĠĠ Ġ',
▁avail able
▁Engl ish
" ,
' 

' s
( g
( )

( self
) .
- t
1 2
3 

= ',
A Y
D A
K AY
L P
L inear
N et
P I
S e
U DA
_ h
_ lo
_ _',
` .
a w
a it
a ge
a ck
b j
b ']

c c
c i
c on
c ce
c string
c las
d ',
d ing
e a
e p
e st
e ature
f f
g in
g ra
h en
h uggingface-c
i ce
i ld
i mple
i ving
l in
l ing
l ow
l um
m al
m all
m ents
m ized
n t
n ',
n ’t
o ther
r ch
s .
s :

s ul
t ing
t ere
u ge
u es
u les
u ild
v id
w ay
w ice
x ',
y mb
z er
{ 

র ":
্ র":
▁ Q
▁ `
▁ x
▁ }
▁ Ċ
▁ —
▁ pu
▁ )

▁ ent
▁ ĊĠĠ
▁ output_size
▁t ransform
▁t wo
▁t wice
in it
in ut
▁a d
▁a ble
▁a dd
en ))

en ote
er .

▁th en
▁th ing
▁th re
▁th an
▁th ere
▁to rch
▁' self',
▁' __
▁' b',
▁' weigh
▁' bias
▁' Add
at er
▁w as
▁w ay
▁w ait
or e
or ch
or mal
ken s',
ra ll
ra ct
▁c ha
▁c hapter
▁c las
▁f i
▁f ew
▁f eature
▁l ater
re ct
re pository
▁s u
▁s ent
▁s ample
▁s our
▁s ave
▁s aw
▁s mall
▁s ymb
▁in ,
▁in di
▁in tere
an ing
an ce
▁o ther
▁o bj
▁b o
▁b lo
▁b rand
▁tokenizer ,
ll y
▁n ex
▁an y
▁an other
▁'Ġ ad
▁'Ġ and
▁'Ġ a',
▁'Ġ b',
▁'Ġ the
▁'Ġ def',
▁'Ġ numbers',
▁'Ġ """',
▁'Ġ output',
▁'Ġ =',
▁'Ġ __',
▁'Ġ b']

▁'Ġ x',
▁y ie
▁you ’re
le ments
ion s
ro du
▁d is
▁d enote
ode Se
▁p ar
▁p ow
▁p ic
ar y
ar at
ar ing
▁h as
▁h ow
▁h aving
▁h uge
▁lo aded
▁m ill
▁m ake
▁m inut
▁se t
▁se ction
un ing
▁wh y
▁wh at
▁re qu
▁re spon
▁re sul
▁Th at
pus h
▁ex ec
es )
es ,
ua lly
▁1 .
▁1 0
▁1 2
▁g ra
▁i dent
▁new lin
ly ,
▁de pen
▁de ter
▁e ven
▁e lements
_d oc
▁A PI
▁this ,
ec k
▁corpus 

▁corpus .

ction 

▁C ourse
▁C UDA
▁C odeSe
ine -t
▁token s

▁language .
▁dataset .
all y
▁al way
▁co lum
▁( 

▁( th
for e
▁F or
▁train _new_from_iterator
▁tokeniz ed
▁pro vi
▁pro gra
▁example ,
▁example :

▁speci f
_string ',
sp ace
▁st at
▁tokenizer. tokenize(example)

▁do cstring
▁"ক্ ট
▁"ক্ ত
▁function s
▁code -search-net-tokenizer
arch Net
▁1000 )

▁library 

▁E ven
▁S aving
▁'func _doc
▁tokenizers '
▁name space
▁self ,
▁return s
▁def ine
igh t
from_ pretrained
ist (g
mm ing
pl ace
pl ication
▁L et’s
▁L inear
▁N ote
▁N LP
▁O pen
▁') ',
▁go ing
▁generator ,
__ (self
um entation
▁3 ,
▁B uild
▁""" 

▁wor ds
▁cor rect
▁inst ance
▁batch es
▁log in
▁exact ly
t_size ,
▁As semb
▁We ’ll
▁self. weigh
▁self. bias
ie ce
if y
ri es
wo ',
▁7 ,
▁M odel
▁raw_datasets["train"] 

▁raw_datasets["train"] [i
ses (
▁ch eck
▁'Ġt orch
▁'Ġt wo',
▁pa rall
▁process ing
▁star t_idx
▁[' def',
▁AutoTokenizer. train_new_from_iterator
▁AutoTokenizer. from_pretrained
▁contain s
▁method :

▁print(l en(
▁print(l ist(g
▁algorithm .
▁indentation ,
▁notebook ,
▁notebook _lo
_to kens',
_pretrained ()
ab 

▁r ules
▁typ e
inpu t',
▁act ually
▁'Ċ ',
ath er
respon ses(
▁sh ould
▁inpu ts
▁once .
▁1, 000
▁It ’s
▁comm and
▁1000]["whole_func_string"] 

▁get_training_corpus ():

▁old_tokenizer .
("code -search-net-tokenizer")

▁timeout_ms =
▁work ing
▁subwor ds
▁repres ents
_h ub
con ds
cce p
est ion
huggingface-c ourse
vid ual
zer os
▁pu re
▁thre e
▁torch .
▁'__ (',
▁'bias ',
▁'Add ',
▁cha ract
▁sample s
▁sour ce
▁symb ol
▁indi vidual
▁intere st
▁obj ect
▁blo ck
▁nex t
▁'Ġad d',
▁'Ġand ',
▁'Ġthe ',
▁yie ld
rodu ction

▁pow ers
▁pic k
▁requ ire
▁resul ts
▁exec ut
▁ident ify
▁newlin es,
▁depen d
▁deter min
▁CodeSe archNet
ine-t uning
▁alway s
▁colum n
▁provi de
▁progra mming
▁code-search-net-tokenizer ,
▁'func_doc umentation
▁Linear Layer
__(self ,
▁Build ing
▁correct ly
▁instance ,
▁Assemb ling
▁'Ġtorch ',
▁parall el
▁AutoTokenizer.train_new_from_iterator ()
▁AutoTokenizer.from_pretrained ("
▁print(list(g en))

▁notebook_lo gin
