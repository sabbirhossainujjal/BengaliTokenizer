{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:13.633836Z","iopub.status.busy":"2024-04-02T09:18:13.633439Z","iopub.status.idle":"2024-04-02T09:18:30.819396Z","shell.execute_reply":"2024-04-02T09:18:30.817830Z","shell.execute_reply.started":"2024-04-02T09:18:13.633808Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m  DEPRECATION: emoji is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  DEPRECATION: ftfy is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  DEPRECATION: normalizer is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q git+https://github.com/csebuetnlp/normalizer"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:30.822739Z","iopub.status.busy":"2024-04-02T09:18:30.822309Z","iopub.status.idle":"2024-04-02T09:18:31.437791Z","shell.execute_reply":"2024-04-02T09:18:31.436548Z","shell.execute_reply.started":"2024-04-02T09:18:30.822704Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'kaggle_secrets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      2\u001b[0m user_secrets \u001b[38;5;241m=\u001b[39m UserSecretsClient()\n\u001b[1;32m      3\u001b[0m hf_token \u001b[38;5;241m=\u001b[39m user_secrets\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"]}],"source":["# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# hf_token = user_secrets.get_secret(\"hf_token\")\n","\n","\n","from huggingface_hub import login\n","login(token= hf_token, add_to_git_credential=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:31.439376Z","iopub.status.busy":"2024-04-02T09:18:31.439072Z","iopub.status.idle":"2024-04-02T09:18:31.444353Z","shell.execute_reply":"2024-04-02T09:18:31.443460Z","shell.execute_reply.started":"2024-04-02T09:18:31.439350Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","from normalizer import normalize"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:31.447458Z","iopub.status.busy":"2024-04-02T09:18:31.446742Z","iopub.status.idle":"2024-04-02T09:18:34.779271Z","shell.execute_reply":"2024-04-02T09:18:34.778172Z","shell.execute_reply.started":"2024-04-02T09:18:31.447421Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'рж╕рж╛ржнрж╛рж░рзЗрж░ ржХржмрж┐рж░ржкрзБрж░ ржмрж╛ржгрж┐ржЬрзНржпрж┐ржХ ржПрж▓рж╛ржХрж╛рж░ ржирж┐ржЬрж╕рзНржм ржлрзНржпрж╛ржХрзНржЯрж░рж┐рждрзЗ рж╣рзЯрзЗ ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ (ржмрж┐ржбрж┐) рж▓рж┐ржГ ржПрж░ ржжрж┐ржиржмрзНржпрж╛ржкрзА рж╕рзЗрж▓рж╕ ржХржиржлрж╛рж░рзЗржирзНрж╕ред ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐ ржЕржирзБрж╖рзНржарж┐ржд рж╣рзЯ рж╢ржирж┐ржмрж╛рж░, рззрззржЗ ржорж╛рж░рзНржЪ, рж╕рж╛рж░рж╛ ржжрзЗрж╢ ржерзЗржХрзЗ ржкрзНрж░рж╛рзЯ рзйрззрзж ржЬржи ржбрж┐рж▓рж╛рж░ ржПржЗ ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐рждрзЗ ржЕржВрж╢ржЧрзНрж░рж╣ржг ржХрж░рзЗржи ржПржмржВ ржЗржЙржирж┐ржЯрзЗржХрж░ ржлрзНржпрж╛ржХрзНржЯрж░рж┐ ржШрзБрж░рзЗ ржжрзЗржЦрж╛рж░ рж╕рзБржпрзЛржЧ ржкрж╛ржиредржПрж╕ржорзЯ ржЗржЙржирж┐ржЯрзЗржХрж░ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛ ржкрж░рж┐ржЪрж╛рж▓ржХ, ржЗржЮрзНржЬрж┐: ржЖржирж┐рж╕ ржЖрж╣ржорзЗржж ржбрж┐рж▓рж╛рж░ржжрзЗрж░ ржЙржжрзНржжрзЗрж╢рзЗ ржмржХрзНрждржмрзНржп рж░рж╛ржЦрзЗржи ржПржмржВ ржЗржЙржирж┐ржЯрзЗржХ ржХрзЗ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж╕рзЗрж░рж╛ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХ ржмрзНрж░рзНржпрж╛ржирзНржб рж╣рж┐рж╕рзЗржмрзЗ ржЧрзЬрзЗ рждрзЛрж▓рж╛рж░ ржЖрж╢рзНржмрж╛рж╕ ржмрзНржпржХрзНржд ржХрж░рзЗржиред ржЗржЮрзНржЬрж┐: ржЖржирж┐рж╕ ржЖрж╣ржорзЗржж, ржЬрж╛рждрзАрзЯ ржПржмржВ ржмрж┐ржнрж╛ржЧрзАрзЯ ржкрж░рзНржпрж╛рзЯрзЗ рж╕рзЗрж░рж╛ ржбрж┐рж▓рж╛рж░ржжрзЗрж░ рж╣рж╛рждрзЗ рж╕рж╛рж░рзНржЯрж┐ржлрж┐ржХрзЗржЯ, ржХрзНрж░рзЗрж╕рзНржЯ ржУ ржкрзНрж░рж╛ржЗржЬ ржмржирзНржб рждрзБрж▓рзЗ ржжрзЗржиред ржмрж┐ржЬрзЯрзАрж░рж╛ рж╣рж▓рзЗржи рж░рзБржкрж╛рж▓рж┐ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), рж▓рж┐рзЯрж╛ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ (ржжрзНржмрж┐рждрзАрзЯ ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), ржоржжрж┐ржирж╛ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (рждрзГрждрзАрзЯ ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), рж╕рзБржирзНржжрж░ржмржи ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржврж╛ржХрж╛ ржмрж┐ржнрж╛ржЧ), ржЖрж▓рж╛ржЙржжрзНржжрж┐ржи ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржЪрж┐ржЯрж╛ржЧржВ ржмрж┐ржнрж╛ржЧ), рж╢рж╛ржирзНржд ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (рж░рж╛ржЬрж╢рж╛рж╣рзА рж░ржВржкрзБрж░ ржмрж┐ржнрж╛ржЧ), ржирзБрж░рзБрж▓ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ (ржЦрзБрж▓ржирж╛ ржмрж┐ржнрж╛ржЧ), ржлрзЗрзЯрж╛рж░ ржнрж┐ржЙ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржмрж░рж┐рж╢рж╛рж▓ ржмрж┐ржнрж╛ржЧ) ржУ ржлрж┐ржбржмрзНржпрж╛ржХ ржХржорж┐ржЙржирж┐ржХрзЗрж╕ржи (рж╕рж┐рж▓рзЗржЯ ржмрж┐ржнрж╛ржЧ)ред ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐рждрзЗ ржЖрж░ржУ ржмржХрзНрждржмрзНржп рж░рж╛ржЦрзЗржи ржЗржЙржирж┐ржЯрзЗржХрзЗрж░ ржмрж┐ржЬржирзЗрж╕ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржЕрзНржпрж╛ржирзНржб ржкрзНрж▓рзНржпрж╛ржирж┐ржВ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржП ржХрзЗ ржПржо рж╣рж╛ржорж┐ржжрзБрж░ рж░рж╣ржорж╛ржи, рж╕рзЗрж▓рж╕ ржЕрзНржпрж╛ржирзНржб ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржП ржЯрж┐ ржПржо ржЖржЦрждрж╛рж░ рж╣рзЛрж╕рзЗржи ржУ ржЗржЙржирж┐ржЯрзЗржХрзЗрж░ рж╕рзЗрж▓рж╕ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржбрж┐рж▓рж╛рж░ ржЪрзНржпрж╛ржирзЗрж▓ ржЬрж╛ржХрж┐рж░ рж╣рзЛрж╕рзЗржиред -ржкрзНрж░рзЗрж╕ ржмрж┐ржЬрзНржЮржкрзНрждрж┐'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"/kaggle/input/potrika-bangla-newspaper-datasets/Potrika-Newspaper Datasets in the Bangla Language/BalancedDataset/Economy_40k.csv\")\n","data['article'][0]"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.781154Z","iopub.status.busy":"2024-04-02T09:18:34.780844Z","iopub.status.idle":"2024-04-02T09:18:34.788699Z","shell.execute_reply":"2024-04-02T09:18:34.787513Z","shell.execute_reply.started":"2024-04-02T09:18:34.781130Z"},"trusted":true},"outputs":[],"source":["text = ['рж╕рж╛ржнрж╛рж░рзЗрж░ ржХржмрж┐рж░ржкрзБрж░ ржмрж╛ржгрж┐ржЬрзНржпрж┐ржХ ржПрж▓рж╛ржХрж╛рж░ ржирж┐ржЬрж╕рзНржм ржлрзНржпрж╛ржХрзНржЯрж░рж┐рждрзЗ рж╣рзЯрзЗ ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ (ржмрж┐ржбрж┐) рж▓рж┐ржГ ржПрж░ ржжрж┐ржиржмрзНржпрж╛ржкрзА рж╕рзЗрж▓рж╕ ржХржиржлрж╛рж░рзЗржирзНрж╕ред ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐ ржЕржирзБрж╖рзНржарж┐ржд рж╣рзЯ рж╢ржирж┐ржмрж╛рж░, рззрззржЗ ржорж╛рж░рзНржЪ, рж╕рж╛рж░рж╛ ржжрзЗрж╢ ржерзЗржХрзЗ ржкрзНрж░рж╛рзЯ рзйрззрзж ржЬржи ржбрж┐рж▓рж╛рж░ ржПржЗ ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐рждрзЗ ржЕржВрж╢ржЧрзНрж░рж╣ржг ржХрж░рзЗржи ржПржмржВ ржЗржЙржирж┐ржЯрзЗржХрж░ ржлрзНржпрж╛ржХрзНржЯрж░рж┐ ржШрзБрж░рзЗ ржжрзЗржЦрж╛рж░ рж╕рзБржпрзЛржЧ ржкрж╛ржиредржПрж╕ржорзЯ ржЗржЙржирж┐ржЯрзЗржХрж░ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛ ржкрж░рж┐ржЪрж╛рж▓ржХ, ржЗржЮрзНржЬрж┐: ржЖржирж┐рж╕ ржЖрж╣ржорзЗржж ржбрж┐рж▓рж╛рж░ржжрзЗрж░ ржЙржжрзНржжрзЗрж╢рзЗ ржмржХрзНрждржмрзНржп рж░рж╛ржЦрзЗржи ржПржмржВ ржЗржЙржирж┐ржЯрзЗржХ ржХрзЗ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж╕рзЗрж░рж╛ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХ ржмрзНрж░рзНржпрж╛ржирзНржб рж╣рж┐рж╕рзЗржмрзЗ ржЧрзЬрзЗ рждрзЛрж▓рж╛рж░ ржЖрж╢рзНржмрж╛рж╕ ржмрзНржпржХрзНржд ржХрж░рзЗржиред ржЗржЮрзНржЬрж┐: ржЖржирж┐рж╕ ржЖрж╣ржорзЗржж, ржЬрж╛рждрзАрзЯ ржПржмржВ ржмрж┐ржнрж╛ржЧрзАрзЯ ржкрж░рзНржпрж╛рзЯрзЗ рж╕рзЗрж░рж╛ ржбрж┐рж▓рж╛рж░ржжрзЗрж░ рж╣рж╛рждрзЗ рж╕рж╛рж░рзНржЯрж┐ржлрж┐ржХрзЗржЯ, ржХрзНрж░рзЗрж╕рзНржЯ ржУ ржкрзНрж░рж╛ржЗржЬ ржмржирзНржб рждрзБрж▓рзЗ ржжрзЗржиред ржмрж┐ржЬрзЯрзАрж░рж╛ рж╣рж▓рзЗржи рж░рзБржкрж╛рж▓рж┐ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), рж▓рж┐рзЯрж╛ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ (ржжрзНржмрж┐рждрзАрзЯ ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), ржоржжрж┐ржирж╛ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (рждрзГрждрзАрзЯ ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), рж╕рзБржирзНржжрж░ржмржи ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржврж╛ржХрж╛ ржмрж┐ржнрж╛ржЧ), ржЖрж▓рж╛ржЙржжрзНржжрж┐ржи ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржЪрж┐ржЯрж╛ржЧржВ ржмрж┐ржнрж╛ржЧ), рж╢рж╛ржирзНржд ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (рж░рж╛ржЬрж╢рж╛рж╣рзА рж░ржВржкрзБрж░ ржмрж┐ржнрж╛ржЧ), ржирзБрж░рзБрж▓ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ (ржЦрзБрж▓ржирж╛ ржмрж┐ржнрж╛ржЧ), ржлрзЗрзЯрж╛рж░ ржнрж┐ржЙ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржмрж░рж┐рж╢рж╛рж▓ ржмрж┐ржнрж╛ржЧ) ржУ ржлрж┐ржбржмрзНржпрж╛ржХ ржХржорж┐ржЙржирж┐ржХрзЗрж╕ржи (рж╕рж┐рж▓рзЗржЯ ржмрж┐ржнрж╛ржЧ)ред ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐рждрзЗ ржЖрж░ржУ ржмржХрзНрждржмрзНржп рж░рж╛ржЦрзЗржи ржЗржЙржирж┐ржЯрзЗржХрзЗрж░ ржмрж┐ржЬржирзЗрж╕ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржЕрзНржпрж╛ржирзНржб ржкрзНрж▓рзНржпрж╛ржирж┐ржВ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржП ржХрзЗ ржПржо рж╣рж╛ржорж┐ржжрзБрж░ рж░рж╣ржорж╛ржи, рж╕рзЗрж▓рж╕ ржЕрзНржпрж╛ржирзНржб ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржП ржЯрж┐ ржПржо ржЖржЦрждрж╛рж░ рж╣рзЛрж╕рзЗржи ржУ ржЗржЙржирж┐ржЯрзЗржХрзЗрж░ рж╕рзЗрж▓рж╕ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржбрж┐рж▓рж╛рж░ ржЪрзНржпрж╛ржирзЗрж▓ ржЬрж╛ржХрж┐рж░ рж╣рзЛрж╕рзЗржиред -ржкрзНрж░рзЗрж╕ ржмрж┐ржЬрзНржЮржкрзНрждрж┐']"]},{"cell_type":"code","execution_count":41,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T09:26:25.452757Z","iopub.status.busy":"2024-04-02T09:26:25.452298Z","iopub.status.idle":"2024-04-02T09:26:25.465230Z","shell.execute_reply":"2024-04-02T09:26:25.463939Z","shell.execute_reply.started":"2024-04-02T09:26:25.452723Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on class SentencePieceBPETokenizer in module tokenizers.implementations.sentencepiece_bpe:\n","\n","class SentencePieceBPETokenizer(tokenizers.implementations.base_tokenizer.BaseTokenizer)\n"," |  SentencePieceBPETokenizer(vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = 'тЦБ', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |  \n"," |  SentencePiece BPE Tokenizer\n"," |  \n"," |  Represents the BPE algorithm, with the pretokenization used by SentencePiece\n"," |  \n"," |  Method resolution order:\n"," |      SentencePieceBPETokenizer\n"," |      tokenizers.implementations.base_tokenizer.BaseTokenizer\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = 'тЦБ', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  train(self, files: Union[str, List[str]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True)\n"," |      Train the model using the given files\n"," |  \n"," |  train_from_iterator(self, iterator: Union[Iterator[str], Iterator[Iterator[str]]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True, length: Optional[int] = None)\n"," |      Train the model using the given iterator\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods defined here:\n"," |  \n"," |  from_file(vocab_filename: str, merges_filename: str, **kwargs)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __slotnames__ = []\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  add_special_tokens(self, special_tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given special tokens to the vocabulary, and treat them as special tokens.\n"," |      \n"," |      The special tokens will never be processed by the model, and will be\n"," |      removed while decoding.\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of special tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  add_tokens(self, tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given tokens to the vocabulary\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the given list of ids to a string sequence\n"," |      \n"," |      Args:\n"," |          ids: List[unsigned int]:\n"," |              A list of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output string\n"," |      \n"," |      Returns:\n"," |          The decoded string\n"," |  \n"," |  decode_batch(self, sequences: List[List[int]], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the list of sequences to a list of string sequences\n"," |      \n"," |      Args:\n"," |          sequences: List[List[unsigned int]]:\n"," |              A list of sequence of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output strings\n"," |      \n"," |      Returns:\n"," |          A list of decoded strings\n"," |  \n"," |  enable_padding(self, direction: Optional[str] = 'right', pad_to_multiple_of: Optional[int] = None, pad_id: Optional[int] = 0, pad_type_id: Optional[int] = 0, pad_token: Optional[str] = '[PAD]', length: Optional[int] = None)\n"," |      Change the padding strategy\n"," |      \n"," |      Args:\n"," |          direction: (`optional`) str:\n"," |              Can be one of: `right` or `left`\n"," |      \n"," |          pad_to_multiple_of: (`optional`) unsigned int:\n"," |              If specified, the padding length should always snap to the next multiple of\n"," |              the given value. For example if we were going to pad with a length of 250 but\n"," |              `pad_to_multiple_of=8` then we will pad to 256.\n"," |      \n"," |          pad_id: (`optional`) unsigned int:\n"," |              The indice to be used when padding\n"," |      \n"," |          pad_type_id: (`optional`) unsigned int:\n"," |              The type indice to be used when padding\n"," |      \n"," |          pad_token: (`optional`) str:\n"," |              The pad token to be used when padding\n"," |      \n"," |          length: (`optional`) unsigned int:\n"," |              If specified, the length at which to pad. If not specified\n"," |              we pad using the size of the longest sequence in a batch\n"," |  \n"," |  enable_truncation(self, max_length: int, stride: Optional[int] = 0, strategy: Optional[str] = 'longest_first')\n"," |      Change the truncation options\n"," |      \n"," |      Args:\n"," |          max_length: unsigned int:\n"," |              The maximum length at which to truncate\n"," |      \n"," |          stride: (`optional`) unsigned int:\n"," |              The length of the previous first sequence to be included\n"," |              in the overflowing sequence\n"," |      \n"," |          strategy: (`optional`) str:\n"," |              Can be one of `longest_first`, `only_first` or `only_second`\n"," |  \n"," |  encode(self, sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Encode the given sequence and pair. This method can process raw text sequences as well\n"," |      as already pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          sequence: InputSequence:\n"," |              The sequence we want to encode. This sequence can be either raw text or\n"," |              pre-tokenized, according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          An Encoding\n"," |  \n"," |  encode_batch(self, inputs: List[Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]], is_pretokenized: bool = False, add_special_tokens: bool = True) -> List[tokenizers.Encoding]\n"," |      Encode the given inputs. This method accept both raw text sequences as well as already\n"," |      pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          inputs: List[EncodeInput]:\n"," |              A list of single sequences or pair sequences to encode. Each `EncodeInput` is\n"," |              expected to be of the following form:\n"," |                  `Union[InputSequence, Tuple[InputSequence, InputSequence]]`\n"," |      \n"," |              Each `InputSequence` can either be raw text or pre-tokenized,\n"," |              according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          A list of Encoding\n"," |  \n"," |  get_added_tokens_decoder(self) -> Dict[int, tokenizers.AddedToken]\n"," |      Returns the added reverse vocabulary\n"," |      \n"," |      Returns:\n"," |          The added vocabulary mapping ints to AddedTokens\n"," |  \n"," |  get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]\n"," |      Returns the vocabulary\n"," |      \n"," |      Args:\n"," |          with_added_tokens: boolean:\n"," |              Whether to include the added tokens in the vocabulary\n"," |      \n"," |      Returns:\n"," |          The vocabulary\n"," |  \n"," |  get_vocab_size(self, with_added_tokens: bool = True) -> int\n"," |      Return the size of vocabulary, with or without added tokens.\n"," |      \n"," |      Args:\n"," |          with_added_tokens: (`optional`) bool:\n"," |              Whether to count in added special tokens or not\n"," |      \n"," |      Returns:\n"," |          Size of vocabulary\n"," |  \n"," |  id_to_token(self, id: int) -> Optional[str]\n"," |      Convert the given token id to its corresponding string\n"," |      \n"," |      Args:\n"," |          token: id:\n"," |              The token id to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding string if it exists, None otherwise\n"," |  \n"," |  no_padding(self)\n"," |      Disable padding\n"," |  \n"," |  no_truncation(self)\n"," |      Disable truncation\n"," |  \n"," |  normalize(self, sequence: str) -> str\n"," |      Normalize the given sequence\n"," |      \n"," |      Args:\n"," |          sequence: str:\n"," |              The sequence to normalize\n"," |      \n"," |      Returns:\n"," |          The normalized string\n"," |  \n"," |  num_special_tokens_to_add(self, is_pair: bool) -> int\n"," |      Return the number of special tokens that would be added for single/pair sentences.\n"," |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n"," |      :return:\n"," |  \n"," |  post_process(self, encoding: tokenizers.Encoding, pair: Optional[tokenizers.Encoding] = None, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Apply all the post-processing steps to the given encodings.\n"," |      \n"," |      The various steps are:\n"," |          1. Truncate according to global params (provided to `enable_truncation`)\n"," |          2. Apply the PostProcessor\n"," |          3. Pad according to global params. (provided to `enable_padding`)\n"," |      \n"," |      Args:\n"," |          encoding: Encoding:\n"," |              The main Encoding to post process\n"," |      \n"," |          pair: Optional[Encoding]:\n"," |              An optional pair Encoding\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add special tokens\n"," |      \n"," |      Returns:\n"," |          The resulting Encoding\n"," |  \n"," |  save(self, path: str, pretty: bool = True)\n"," |      Save the current Tokenizer at the given path\n"," |      \n"," |      Args:\n"," |          path: str:\n"," |              A path to the destination Tokenizer file\n"," |  \n"," |  save_model(self, directory: str, prefix: Optional[str] = None)\n"," |      Save the current model to the given directory\n"," |      \n"," |      Args:\n"," |          directory: str:\n"," |              A path to the destination directory\n"," |      \n"," |          prefix: (Optional) str:\n"," |              An optional prefix, used to prefix each file name\n"," |  \n"," |  to_str(self, pretty: bool = False)\n"," |      Get a serialized JSON version of the Tokenizer as a str\n"," |      \n"," |      Args:\n"," |          pretty: bool:\n"," |              Whether the JSON string should be prettified\n"," |      \n"," |      Returns:\n"," |          str\n"," |  \n"," |  token_to_id(self, token: str) -> Optional[int]\n"," |      Convert the given token to its corresponding id\n"," |      \n"," |      Args:\n"," |          token: str:\n"," |              The token to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding id if it exists, None otherwise\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  padding\n"," |      Get the current padding parameters\n"," |      \n"," |      Returns:\n"," |          None if padding is disabled, a dict with the currently set parameters\n"," |          if the padding is enabled.\n"," |  \n"," |  truncation\n"," |      Get the current truncation parameters\n"," |      \n"," |      Returns:\n"," |          None if truncation is disabled, a dict with the current truncation parameters if\n"," |          truncation is enabled\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  decoder\n"," |  \n"," |  model\n"," |  \n"," |  normalizer\n"," |  \n"," |  post_processor\n"," |  \n"," |  pre_tokenizer\n","\n"]}],"source":["# help(SentencePieceBPETokenizer)\n","# self, files: Union[str, List[str]], \n","# vocab_size: int = 30000, min_frequency: int = 2, \n","# special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], \n","# limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from tokenizers import SentencePieceBPETokenizer\n","\n","tokenizer = SentencePieceBPETokenizer()\n","\n","# tokenizer_arguments = dict(\n","    \n","# )"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import json\n","with open('/home/virus_proton/Projects/P_Projects/LLM_Mastery/Tokenizer_train/vocab.json', 'r') as f:\n","    vocab = json.load(f)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["{'<pad>': 0,\n"," '<s>': 1,\n"," '</s>': 2,\n"," '<unk>': 3,\n"," '<mask>': 4,\n"," '\\n': 5,\n"," ',': 6,\n"," '-': 7,\n"," '.': 8,\n"," '4': 9,\n"," '5': 10,\n"," '6': 11,\n"," '7': 12,\n"," ':': 13,\n"," 'B': 14,\n"," 'E': 15,\n"," 'F': 16,\n"," 'N': 17,\n"," 'P': 18,\n"," 'V': 19,\n"," 'a': 20,\n"," 'b': 21,\n"," 'c': 22,\n"," 'e': 23,\n"," 'g': 24,\n"," 'h': 25,\n"," 'i': 26,\n"," 'k': 27,\n"," 'l': 28,\n"," 'm': 29,\n"," 'n': 30,\n"," 'o': 31,\n"," 's': 32,\n"," 't': 33,\n"," 'w': 34,\n"," 'y': 35,\n"," '|': 36,\n"," 'ред': 37,\n"," 'ржБ': 38,\n"," 'ржВ': 39,\n"," 'ржГ': 40,\n"," 'ржЕ': 41,\n"," 'ржЖ': 42,\n"," 'ржЗ': 43,\n"," 'ржЙ': 44,\n"," 'ржП': 45,\n"," 'ржУ': 46,\n"," 'ржХ': 47,\n"," 'ржЦ': 48,\n"," 'ржЧ': 49,\n"," 'ржШ': 50,\n"," 'ржЩ': 51,\n"," 'ржЪ': 52,\n"," 'ржЫ': 53,\n"," 'ржЬ': 54,\n"," 'ржЭ': 55,\n"," 'ржЮ': 56,\n"," 'ржЯ': 57,\n"," 'ржа': 58,\n"," 'ржб': 59,\n"," 'ржг': 60,\n"," 'ржд': 61,\n"," 'рже': 62,\n"," 'ржж': 63,\n"," 'ржз': 64,\n"," 'ржи': 65,\n"," 'ржк': 66,\n"," 'ржл': 67,\n"," 'ржм': 68,\n"," 'ржн': 69,\n"," 'ржо': 70,\n"," 'ржп': 71,\n"," 'рж░': 72,\n"," 'рж▓': 73,\n"," 'рж╢': 74,\n"," 'рж╖': 75,\n"," 'рж╕': 76,\n"," 'рж╣': 77,\n"," 'рж╝': 78,\n"," 'рж╛': 79,\n"," 'рж┐': 80,\n"," 'рзА': 81,\n"," 'рзБ': 82,\n"," 'рзВ': 83,\n"," 'рзГ': 84,\n"," 'рзЗ': 85,\n"," 'рзИ': 86,\n"," 'рзЛ': 87,\n"," 'рзН': 88,\n"," 'рзО': 89,\n"," 'рзж': 90,\n"," 'рзз': 91,\n"," 'рзи': 92,\n"," 'рзк': 93,\n"," 'рзл': 94,\n"," '\\u200d': 95,\n"," 'тАУ': 96,\n"," 'тАФ': 97,\n"," 'тЦБ': 98,\n"," 'ЁЯМ╖': 99,\n"," 'ЁЯМ╣': 100,\n"," 'ЁЯТЧ': 101,\n"," 'ЁЯТЩ': 102,\n"," 'ЁЯТЪ': 103,\n"," 'ЁЯеА': 104,\n"," 'ржпрж╝': 105,\n"," 'тЦБржм': 106,\n"," 'тЦБрж╕': 107,\n"," 'ржирзН': 108,\n"," 'рж╛рж░': 109,\n"," 'тЦБржХ': 110,\n"," 'рзЗрж░': 111,\n"," 'тЦБржЖ': 112,\n"," 'ржзрзБ': 113,\n"," 'ржирзНржзрзБ': 114,\n"," 'тЦБржмржирзНржзрзБ': 115,\n"," 'тЦБржП': 116,\n"," 'рзНржп': 117,\n"," 'тЦБржи': 118,\n"," 'тЦБржк': 119,\n"," 'тЦБржд': 120,\n"," 'ржХрзЗ': 121,\n"," 'тЦБрж╣': 122,\n"," 'тЦБржп': 123,\n"," 'рзНрж░': 124,\n"," 'тЦБржо': 125,\n"," 'рзНржм': 126,\n"," 'рж╛ржи': 127,\n"," 'ржпрж╝рзЗ': 128,\n"," 'тЦБржХрж░': 129,\n"," 'ржжрзЗрж░': 130,\n"," 'ржмрзЗ': 131,\n"," 'ржкржи': 132,\n"," 'тЦБржЬ': 133,\n"," 'тЦБржж': 134,\n"," 'тЦБржЖржкржи': 135,\n"," 'рждрзЗ': 136,\n"," 'тЦБржЕ': 137,\n"," 'тЦБрже': 138,\n"," 'ред\\n': 139,\n"," 'тЦБржЖржо': 140,\n"," 'рзНржпрж╛': 141,\n"," 'рждрзНржм': 142,\n"," 'тЦБржПржХ': 143,\n"," 'тЦБржн': 144,\n"," 'тЦБржирж╛': 145,\n"," 'тЦБржерж╛': 146,\n"," '\\n\\n': 147,\n"," 'тЦБржирж┐': 148,\n"," 'ржЯрж┐': 149,\n"," 'ржЦржи': 150,\n"," 'ржЫрзЗ': 151,\n"," 'рж╛ржпрж╝': 152,\n"," 'рждрзБ': 153,\n"," 'рж╛рж╕': 154,\n"," 'тЦБрж╕ржо': 155,\n"," 'тЦБрж╢': 156,\n"," 'тЦБржкрзНрж░': 157,\n"," 'ржмржВ': 158,\n"," 'рж░рж╛': 159,\n"," 'рж▓рзЗ': 160,\n"," 'тЦБржЪ': 161,\n"," 'тЦБрж╕рж╛': 162,\n"," 'тЦБржмржирзНржзрзБрждрзНржм': 163,\n"," 'тЦБтАУ': 164,\n"," 'тЦБржПржмржВ': 165,\n"," 'тЦБржХрж░рзЗ': 166,\n"," 'ржерзЗ': 167,\n"," 'рзНржЯ': 168,\n"," 'тЦБрж╕рзЗ': 169,\n"," 'тЦБржХрж┐': 170,\n"," 'рж╛ржжрзЗрж░': 171,\n"," 'рж┐ржпрж╝': 172,\n"," 'ржирзЗ': 173,\n"," 'рж┐ржХ': 174,\n"," 'рж╛рж░рж╛': 175,\n"," 'ржбрж╝': 176,\n"," 'рж░рзН': 177,\n"," 'тЦБржпрзЗ': 178,\n"," 'рждрзНржп': 179,\n"," 'рж╛рж▓': 180,\n"," 'тЦБржЙ': 181,\n"," 'тЦБржмрж┐': 182,\n"," 'тЦБржЖржкржирж╛рж░': 183,\n"," 'тЦБрж╕ржм': 184,\n"," 'тЦБрждрзЛ': 185,\n"," 'тЦБржерж╛ржХ': 186,\n"," 'рж╛ржЗ': 187,\n"," 'рж╛ржХрзЗ': 188,\n"," 'ржЬржи': 189,\n"," 'рждрж┐': 190,\n"," 'рж▓рж┐': 191,\n"," 'тЦБрж╕рзНржЯ': 192,\n"," 'тЦБржЖржкржирж┐': 193,\n"," 'тЦБрж╕рж╛ржерзЗ': 194,\n"," 'рждрзЛ': 195,\n"," 'ЁЯеАЁЯТЧ': 196,\n"," 'ржЯрж╛рж╕': 197,\n"," 'рж┐ржм': 198,\n"," 'ржирзНржп': 199,\n"," 'тЦБржкрж╛': 200,\n"," 'рзНржпрж╛ржЯрж╛рж╕': 201,\n"," 'тЦБржирж┐ржпрж╝рзЗ': 202,\n"," 'тЦБрж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕': 203,\n"," 'ржХрзН': 204,\n"," 'рж┐ржи': 205,\n"," 'тЦБтАФ': 206,\n"," 'ржпрж╝ред': 207,\n"," 'тЦБрж╕рзБ': 208,\n"," 'тЦБржХржЦржи': 209,\n"," 'тЦБржмржирзНржзрзБржжрзЗрж░': 210,\n"," 'тЦБржкрж╛рж░': 211,\n"," 'рждрзБржи': 212,\n"," 'ржУржпрж╝': 213,\n"," 'ржорж┐': 214,\n"," 'рж▓рж╛': 215,\n"," 'рзЗржпрж╝рзЗ': 216,\n"," 'тЦБржЦ': 217,\n"," 'тЦБржХрж╛': 218,\n"," 'тЦБржжрзЗ': 219,\n"," 'тЦБржерж╛ржХрзЗ': 220,\n"," 'тЦБрж╕ржоржпрж╝': 221,\n"," 'рж┐ржХрж╛рж░': 222,\n"," 'тЦБрждрзЛржо': 223,\n"," 'ржоржи': 224,\n"," 'рж░рзЗ': 225,\n"," 'рзАржм': 226,\n"," 'рзБржи': 227,\n"," 'тЦБржЧ': 228,\n"," 'тЦБржпрж╛': 229,\n"," 'рж╛ржирж╛': 230,\n"," 'тЦБржкрзНрж░рж┐ржпрж╝': 231,\n"," 'рждрзНржпрж┐ржХрж╛рж░': 232,\n"," 'ржмржирзНржзрзБ': 233,\n"," 'тЦБржирждрзБржи': 234,\n"," 'рзНржпрж╛ржк': 235,\n"," 'рждрзНржпрж┐ржХрж╛рж░рзЗрж░': 236,\n"," 'ржЬрж╛ржирж╛': 237,\n"," 'ржкрзНрж░': 238,\n"," 'рж╢ржи': 239,\n"," 'рж╛ржд': 240,\n"," 'рж┐ржпрж╝рзЗ': 241,\n"," 'тЦБржл': 242,\n"," 'ржирзНрждрзБ': 243,\n"," 'тЦБржХрж╛рж░': 244,\n"," 'тЦБржХрзНржпрж╛ржк': 245,\n"," 'тЦБржЕржЬрж╛ржирж╛': 246,\n"," 'тЦБржПржХржЬржи': 247,\n"," 'ржУржпрж╝рж╛': 248,\n"," 'тЦБржХрзНржпрж╛ржкрж╢ржи': 249,\n"," 'рж▓рзЛ': 250,\n"," 'рзЗржи': 251,\n"," 'рзЗржХрзЗ': 252,\n"," 'рзЛржи': 253,\n"," 'рзНржк': 254,\n"," 'тЦБржЖрж╕': 255,\n"," 'тЦБржЬржирзНржп': 256,\n"," 'тЦБржЬрзАржм': 257,\n"," 'тЦБржЖржорж┐': 258,\n"," 'тЦБрж╕рзЗржЗ': 259,\n"," 'тЦБржХрж┐ржирзНрждрзБ': 260,\n"," 'ржЫрзБ': 261,\n"," 'рж╢рзЗ': 262,\n"," 'тЦБржмрж╛': 263,\n"," 'тЦБржПржоржи': 264,\n"," 'тЦБрждрж╛ржжрзЗрж░': 265,\n"," 'тЦБрждрж╛рж░рж╛': 266,\n"," 'тЦБржПржХржЯрж┐': 267,\n"," 'тЦБржнрж╛рж▓': 268,\n"," 'тЦБржХрж┐ржЫрзБ': 269,\n"," 'тЦБржХржЦржирзЛ': 270,\n"," 'ржХрж╛': 271,\n"," 'ржХрзЗрж░': 272,\n"," 'ржЫрж┐': 273,\n"," 'ржирзЗрж░': 274,\n"," 'рзБрж╖': 275,\n"," 'рзНржо': 276,\n"," 'тЦБрждрзБ': 277,\n"," 'тЦБрж╣ржпрж╝': 278,\n"," 'тЦБржорждрзЛ': 279,\n"," 'рж╛ржирзБрж╖': 280,\n"," 'тЦБржЖржорж░рж╛': 281,\n"," 'ржХрзНрждрж┐': 282,\n"," 'тЦБрждрзЛржорж╛рж░': 283,\n"," 'рзНржкрж░рзН': 284,\n"," 'ржерж┐ржм': 285,\n"," 'ржжрж┐': 286,\n"," 'ржнрж╛': 287,\n"," 'рж░рж┐': 288,\n"," 'рж┐ржд': 289,\n"," 'рзГржерж┐ржм': 290,\n"," 'рзЗрж╕': 291,\n"," 'ЁЯМ╣ЁЯМ╣': 292,\n"," 'тЦБрж╣рж▓': 293,\n"," 'тЦБржорж╛ржирзБрж╖': 294,\n"," 'тЦБржЖржорж╛ржжрзЗрж░': 295,\n"," 'тЦБржирж╛,': 296,\n"," 'тЦБржирж╛ред': 297,\n"," '\\n\\nЁЯеАЁЯТЧ': 298,\n"," '\\n\\nЁЯМ╣ЁЯМ╣': 299,\n"," 'тЦБржнрж╛рж▓рзЛ': 300,\n"," 'рзГржерж┐ржмрзА': 301,\n"," '\\nржП': 302,\n"," 'ржХрж╛рж▓': 303,\n"," 'ржЦрзЗ': 304,\n"," 'ржЧрзБ': 305,\n"," 'ржирж╛': 306,\n"," 'рж╢рж┐': 307,\n"," 'рзГржж': 308,\n"," 'тЦБрж░рж╛': 309,\n"," 'тЦБржмрзЗ': 310,\n"," 'ржирзНржж': 311,\n"," 'рзЗрж░рж╛': 312,\n"," 'тЦБржЖрж░': 313,\n"," 'тЦБржЖржЫрзЗ': 314,\n"," 'тЦБржпрж╛ржпрж╝': 315,\n"," 'тЦБржпрж╛рж░рж╛': 316,\n"," 'рж╛ржирзЗ': 317,\n"," 'тЦБржжрзБ': 318,\n"," 'тЦБржЕржирзЗ': 319,\n"," 'тЦБржерзЗржХрзЗ': 320,\n"," 'тЦБржЖржорж╛рж░': 321,\n"," 'тЦБржмржирзНржзрзБрждрзНржмрзЗрж░': 322,\n"," 'тЦБржХрж╛рж░ржг': 323,\n"," '\\nржХ': 324,\n"," 'ржЪрзЗржпрж╝рзЗ': 325,\n"," 'ржмрж┐': 326,\n"," 'ржмрзБ': 327,\n"," 'рж╢рзНржм': 328,\n"," 'рж╖рзН': 329,\n"," 'рж╕рж╛': 330,\n"," 'рж╣рж╛': 331,\n"," 'рж┐рж╕': 332,\n"," 'рзБрж▓рзЗ': 333,\n"," 'рзНржд': 334,\n"," 'тЦБржЯ': 335,\n"," 'тЦБржб': 336,\n"," 'тЦБрж░': 337,\n"," 'тЦБЁЯеАЁЯТЧ': 338,\n"," 'ЁЯМ╖ЁЯМ╖': 339,\n"," 'тЦБржмрзНржп': 340,\n"," 'тЦБрж╕рждрзНржпрж┐ржХрж╛рж░рзЗрж░': 341,\n"," 'ржирзНржд': 342,\n"," 'тЦБржХрзЗ': 343,\n"," 'тЦБржирзЗ': 344,\n"," 'тЦБрж╣рзГржж': 345,\n"," 'тЦБржПржХрж╕рж╛': 346,\n"," 'тЦБрж╕ржорзНржкрж░рзН': 347,\n"," 'тЦБрж╢рзБ': 348,\n"," 'тЦБрж╕ржмржЪрзЗржпрж╝рзЗ': 349,\n"," 'тЦБржкрж╛рж╢рзЗ': 350,\n"," 'ржнрж╛ржмрзЗ': 351,\n"," 'тЦБржПржХрж╕рж╛ржерзЗ': 352,\n"," 'ржжрзЗ': 353,\n"," 'ржзрзНржп': 354,\n"," 'ржкржжрзЗ': 355,\n"," 'рж╕рждрзНржпрж┐ржХрж╛рж░рзЗрж░': 356,\n"," 'рзВрж▓': 357,\n"," 'тЦБржЫ': 358,\n"," 'тЦБржЖржХрж╛': 359,\n"," 'тЦБржмржирзНржзрзБрж░рж╛': 360,\n"," 'тЦБржкрж░': 361,\n"," 'тЦБржкрзГржерж┐ржмрзА': 362,\n"," 'тЦБрждрж╛': 363,\n"," 'тЦБрждрж╛рж░': 364,\n"," 'тЦБрж╣ржпрж╝ред': 365,\n"," 'тЦБржХрж░рж┐': 366,\n"," 'тЦБржХрж░рждрзЗ': 367,\n"," 'ржмрзЗред': 368,\n"," 'ржмрзЗржи': 369,\n"," 'тЦБржЖржкржирж╛ржХрзЗ': 370,\n"," 'ред\\nржЖ': 371,\n"," 'тЦБржнрж╛': 372,\n"," '\\n\\nЁЯМ╖ЁЯМ╖': 373,\n"," 'тЦБржмрж┐рж╢рзНржм': 374,\n"," 'тЦБрж╕ржмрж╛рж░': 375,\n"," 'тЦБржХрж╛ржЫрзЗ': 376,\n"," 'ржмржирзНржзрзБрждрзНржм': 377,\n"," '\\n\\nЁЯМ╣ЁЯМ╣ржп': 378,\n"," 'тЦБржЕржирзЗржХ': 379,\n"," 'ржЦрж╛': 380,\n"," 'ржЦрзЗрж░': 381,\n"," 'ржЦрж╛ржирзЗ': 382,\n"," 'ржарж┐ржи': 383,\n"," 'ржжрж╛': 384,\n"," 'ржирж┐': 385,\n"," 'ржорж╛ржд': 386,\n"," 'рж▓рж╛рж░': 387,\n"," 'рж╕ржо': 388,\n"," 'рж┐рж░': 389,\n"," 'рзЗ,': 390,\n"," 'тЦБржмржбрж╝': 391,\n"," 'ржирзНржз': 392,\n"," 'тЦБржХрзЛржи': 393,\n"," 'тЦБржХржарж┐ржи': 394,\n"," 'тЦБржмржирзНржзрзБрж░': 395,\n"," 'тЦБржПржЗ': 396,\n"," 'тЦБржпржЦржи': 397,\n"," 'тЦБржпржжрж┐': 398,\n"," 'тЦБржорж╛': 399,\n"," 'тЦБржорж┐': 400,\n"," 'тЦБржорзВрж▓': 401,\n"," 'тЦБржХрж░рзБржи': 402,\n"," 'тЦБржЬрж┐ржи': 403,\n"," 'тЦБржжрзВ': 404,\n"," 'ред\\nржкрзНрж░': 405,\n"," 'тЦБржирж┐ржЬ': 406,\n"," 'тЦБржкрзНрж░рждрж┐': 407,\n"," 'тЦБржЪрж╛ржЗ': 408,\n"," 'тЦБржЪрзЗржпрж╝рзЗ': 409,\n"," 'тЦБржХрж░рзЗред': 410,\n"," 'ржбрж╝рзЗ': 411,\n"," 'тЦБржкрж╛ржУржпрж╝рж╛': 412,\n"," 'тЦБрж╕рзБржирзНржж': 413,\n"," 'тЦБржкрж╛рж░рзЗржи': 414,\n"," 'тЦБржЦрзБ': 415,\n"," 'тЦБржЬрзАржмржирзЗ': 416,\n"," 'рзЗрж╕ржмрзБ': 417,\n"," 'тЦБрж░рж╛ржЦ': 418,\n"," 'тЦБржмрзЗрж╢рж┐': 419,\n"," 'рж╖рзНржЯрж┐': 420,\n"," 'тЦБрж╕ржорзНржкрж░рзНржХрзЗ': 421,\n"," 'тЦБрж╢рзБржзрзБ': 422,\n"," 'тЦБржнрж╛ржЧ': 423,\n"," 'ржорж╛рждрзНрж░': 424,\n"," 'тЦБржорзВрж▓рзНржп': 425,\n"," 'тЦБржЬрж┐ржирж┐рж╕': 426,\n"," 'тЦБрж╕рзБржирзНржжрж░': 427,\n"," '\\nржк': 428,\n"," 'gl': 429,\n"," 'ngl': 430,\n"," 'ржПржХ': 431,\n"," 'ржЪрзН': 432,\n"," 'ржЬрзЗ': 433,\n"," 'ржЬрзН': 434,\n"," 'ржЬрж╛рж░': 435,\n"," 'рждрзНржо': 436,\n"," 'ржерж╛': 437,\n"," 'ржмрж╛рж░': 438,\n"," 'ржмрж╛ржи': 439,\n"," 'ржпрзНржп': 440,\n"," 'рж▓,': 441,\n"," 'рзБржХ': 442,\n"," 'рзГржд': 443,\n"," 'рзЛржЧ': 444,\n"," 'рзЛрж▓рж╛': 445,\n"," 'ЁЯТЩЁЯТЪ': 446,\n"," 'ржпрж╝рж╛рж░': 447,\n"," 'тЦБрж╕рзЗрж░рж╛': 448,\n"," 'тЦБржЖрждрзНржо': 449,\n"," 'тЦБржиржпрж╝ред': 450,\n"," 'тЦБрждржЦржи': 451,\n"," 'тЦБрж╣ржпрж╝рзЗ': 452,\n"," 'тЦБрж╣рждрзЗ': 453,\n"," 'тЦБржпржд': 454,\n"," 'тЦБржорзБ': 455,\n"," 'тЦБржоржзрзНржп': 456,\n"," 'тЦБржЕрж░рзН': 457,\n"," 'ред\\nржп': 458,\n"," 'тЦБржЖржорж╛ржХрзЗ': 459,\n"," 'тЦБржнрзБрж▓рзЗ': 460,\n"," '\\n\\nЁЯТЩЁЯТЪ': 461,\n"," 'ржЦржиржУ': 462,\n"," 'тЦБржЪрж┐рж░': 463,\n"," 'тЦБрж╕рж╛рж╣рж╛': 464,\n"," 'тЦБржЙржХрзНрждрж┐': 465,\n"," 'тЦБржмрж┐ржкржжрзЗ': 466,\n"," 'тЦБржерж╛ржХржмрзЗ': 467,\n"," 'ржХрзНрж╖': 468,\n"," 'тЦБржкрж╛рж░рзЗ': 469,\n"," 'тЦБржерж╛ржХрзЗред': 470,\n"," 'тЦБрж╣ржпрж╝рждрзЛ': 471,\n"," 'ржЧрзБрж▓рж┐': 472,\n"," 'тЦБЁЯеАЁЯТЧтАФ': 473,\n"," 'тЦБрж╣рзГржжржпрж╝': 474,\n"," 'тЦБржЖржХрж╛рж╢': 475,\n"," '\\n\\nЁЯМ╖ЁЯМ╖рж╕рждрзНржпрж┐ржХрж╛рж░рзЗрж░': 476,\n"," 'тЦБржоржзрзНржпрзЗ': 477,\n"," 'тЦБржЪрж┐рж░ржХрж╛рж▓': 478,\n"," 'тЦБрж╕рж╛рж╣рж╛ржпрзНржп': 479,\n"," 'ржБржЬрзЗ': 480,\n"," 'ржЗ,': 481,\n"," 'ржХрж╛рж░': 482,\n"," 'ржХрзГржд': 483,\n"," 'ржЯрж╛': 484,\n"," 'ржЯрж╛ржЗ': 485,\n"," 'ржгрзЗрж░': 486,\n"," 'ржкрж░': 487,\n"," 'ржмрзН': 488,\n"," 'ржмрж╛рж╕': 489,\n"," 'ржнржм': 490,\n"," 'ржорзЗ': 491,\n"," 'ржпрзЛржЧ': 492,\n"," 'рж░рзБ': 493,\n"," 'рж░рзНржм': 494,\n"," 'рж░рж╛ржи': 495,\n"," 'рж▓рзЗрж░': 496,\n"," 'рж╢рж╛': 497,\n"," 'рж╕рзНржд': 498,\n"," 'рж╣рж╛рж░': 499,\n"," 'рж┐ржЗ': 500,\n"," 'рж┐рждрзЗ': 501,\n"," 'рзГрж╖рзНржЯрж┐': 502,\n"," 'рзЛрж░': 503,\n"," 'рзНрже': 504,\n"," 'рзНрж╖': 505,\n"," 'тЦБB': 506,\n"," 'тЦБ|': 507,\n"," 'тЦБржЫрзЗ': 508,\n"," 'тЦБрж▓рзЛ': 509,\n"," 'ржпрж╝,': 510,\n"," 'тЦБржмрзЛ': 511,\n"," 'тЦБржмрж▓рзЗ': 512,\n"," 'тЦБржмржирзНржз': 513,\n"," 'тЦБржмрзГрж╖рзНржЯрж┐': 514,\n"," 'тЦБрж╕рж╣': 515,\n"," 'тЦБрж╕рзНржм': 516,\n"," 'тЦБрж╕рж░рзНржм': 517,\n"," 'ржирзНржб': 518,\n"," 'тЦБржЖржЬ': 519,\n"," 'тЦБржЖрж╢рж╛': 520,\n"," 'тЦБржПржЯрж┐': 521,\n"," 'рзНржпржирзНржд': 522,\n"," 'тЦБрж╣рж╛рж╕': 523,\n"," 'тЦБрж╣рж╛ржд': 524,\n"," 'рзНрж░рж╣': 525,\n"," 'рзНрж░рзБ': 526,\n"," 'тЦБржорзЗ': 527,\n"," 'тЦБржХрж░рж╛': 528,\n"," 'тЦБржЬрж┐': 529,\n"," 'тЦБржжрж┐ржпрж╝рзЗ': 530,\n"," 'ред\\nржХ': 531,\n"," 'ред\\nржПржХ': 532,\n"," 'тЦБржПржХржЗ': 533,\n"," 'тЦБржнрж┐ржд': 534,\n"," 'тЦБрж╢рж┐': 535,\n"," 'тЦБрж╢рзЗ': 536,\n"," 'тЦБржЪрзЛ': 537,\n"," 'тЦБржХрж░рзЗ,': 538,\n"," 'рж┐ржпрж╝рж╛': 539,\n"," 'тЦБржпрзЗрждрзЗ': 540,\n"," 'рждрзНржпрзЗ': 541,\n"," 'рждрзНржпрж┐ржЗ': 542,\n"," 'тЦБржЙржк': 543,\n"," 'тЦБрж╕ржмрж╕ржо': 544,\n"," 'тЦБржерж╛ржХржм': 545,\n"," 'тЦБрж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕\\n': 546,\n"," 'тЦБржжрзЗржЦрж╛': 547,\n"," 'тЦБрждрзЛржорж╛ржХрзЗ': 548,\n"," 'тЦБржЧрзБ': 549,\n"," 'тЦБржЧрзЗ': 550,\n"," 'тЦБржЧрзЛрж▓рж╛': 551,\n"," 'тЦБржлрзНрж░': 552,\n"," 'тЦБржлрзЗрж╕ржмрзБ': 553,\n"," 'тЦБржЖрж╕рзЗ': 554,\n"," 'тЦБржЬрзАржмржирзЗрж░': 555,\n"," 'ржЫрж┐ред': 556,\n"," 'тЦБрждрзБржорж┐': 557,\n"," 'тЦБрж╣ржпрж╝,': 558,\n"," '\\n\\nЁЯеАЁЯТЧржмржирзНржзрзБрждрзНржм': 559,\n"," 'тЦБржмрзНржпржХрзНрждрж┐': 560,\n"," 'тЦБржХрзЗржЙ': 561,\n"," 'ред\\nржЖржорж┐': 562,\n"," 'тЦБржорж┐рж▓': 563,\n"," 'ред\\nржкрзНрж░рж┐ржпрж╝': 564,\n"," 'тЦБржЦрзБржБржЬрзЗ': 565,\n"," 'тЦБржорзВрж▓рзНржпржмрж╛ржи': 566,\n"," 'ржЪрзНржЫрзЗ': 567,\n"," 'тЦБржЕрж░рзНрже': 568,\n"," 'тЦБржмржирзНржзржи': 569,\n"," 'тЦБрж╕ржмрж╕ржоржпрж╝': 570,\n"," 'тЦБржЧрзЛрж▓рж╛ржк': 571,\n"," '\\nржи': 572,\n"," 'angl': 573,\n"," 'ржЧрзНрж░рж╣': 574,\n"," 'ржЫржирзНржж': 575,\n"," 'ржЬрж┐': 576,\n"," 'ржЬрзАржм': 577,\n"," 'ржЬрж┐ржпрж╝рзЗ': 578,\n"," 'рждржЗ': 579,\n"," 'рждрж╛рж░': 580,\n"," 'ржерж╛ржпрж╝': 581,\n"," 'ржжрж┐ржи': 582,\n"," 'ржирзБ': 583,\n"," 'ржнрж┐': 584,\n"," 'ржорж╛ржжрзЗрж░': 585,\n"," 'рж░рж▓': 586,\n"," 'рж░рзЗрж░': 587,\n"," 'рж░рждрзНржм': 588,\n"," 'рж╕ржи': 589,\n"," 'рж╛ржбрж╝': 590,\n"," 'рзАрж░': 591,\n"," 'рзАржпрж╝': 592,\n"," 'рзВрж░рзН': 593,\n"," 'рзГрждрж┐': 594,\n"," 'рзЗржо': 595,\n"," 'рзИрж░рж┐': 596,\n"," 'рзЛржЯ': 597,\n"," 'рзНржХ': 598,\n"," 'тЦБржУ': 599,\n"," 'тЦБржз': 600,\n"," 'тЦБрж▓': 601,\n"," 'тЦБрзи': 602,\n"," 'тЦБрж▓рж┐': 603,\n"," 'тЦБрж▓рж╛': 604,\n"," 'тЦБржмрж▓': 605,\n"," 'тЦБржмрзБ': 606,\n"," 'тЦБрж╕ржВ': 607,\n"," 'тЦБрж╕рзО': 608,\n"," 'тЦБрж╕ржХрж╛рж▓': 609,\n"," 'тЦБрж╕рждрзНржпрж┐ржЗ': 610,\n"," 'ржирзНржЯ': 611,\n"," 'ржирзНржирж╛': 612,\n"," 'тЦБржХржерж╛': 613,\n"," 'ржзрзБрж░': 614,\n"," 'тЦБржкрзЛ': 615,\n"," 'тЦБржкрзЗржпрж╝рзЗ': 616,\n"," 'тЦБржкрж░рж┐': 617,\n"," 'тЦБрждржмрзЗ': 618,\n"," 'тЦБрждрж╛ржЗ': 619,\n"," 'тЦБрждрзИрж░рж┐': 620,\n"," 'тЦБрж╣рж╛': 621,\n"," 'тЦБрж╣рж╛рж░': 622,\n"," 'тЦБрж╣рзЗржи': 623,\n"," 'тЦБржоржирзЗ': 624,\n"," 'рж╛ржирж┐': 625,\n"," 'тЦБржХрж░рж╛рж░': 626,\n"," 'тЦБржЬрж┐ржм': 627,\n"," 'тЦБржжрж┐ржм': 628,\n"," 'тЦБржжрж┐рждрзЗ': 629,\n"," 'тЦБржЕржирзНржп': 630,\n"," 'тЦБржЕржирзБ': 631,\n"," 'ред\\n\\n': 632,\n"," 'ред\\nрж╕': 633,\n"," 'ред\\nржмржирзНржзрзБрждрзНржм': 634,\n"," 'тЦБржнрзБ': 635,\n"," 'тЦБржнржпрж╝': 636,\n"," 'тЦБржирж╛ржо': 637,\n"," 'ржЫрзЗржи': 638,\n"," 'тЦБрж╢ржд': 639,\n"," 'тЦБрж╢рж┐ржХ': 640,\n"," 'тЦБрж╢ржмрзН': 641,\n"," 'тЦБржкрзНрж░ржХрзГржд': 642,\n"," 'рж▓рзЗржи': 643,\n"," 'рж▓рзЗржжрзЗрж░': 644,\n"," 'тЦБрж╕рж╛ржЬрж┐ржпрж╝рзЗ': 645,\n"," 'тЦБрж╕рзЗржЦрж╛ржирзЗ': 646,\n"," 'рж╛рж░рж╛ржк': 647,\n"," 'ржбрж╝рзБржи': 648,\n"," 'тЦБржЙржж': 649,\n"," 'тЦБржмрж┐рж░рж▓': 650,\n"," 'тЦБрждрзЛрж░': 651,\n"," 'тЦБржерж╛ржХрж╛рж░': 652,\n"," 'тЦБржерж╛ржХрждрзЗ': 653,\n"," 'рж▓рж┐рж▓': 654,\n"," 'рж▓рж┐ржЬрж╛рж░': 655,\n"," 'тЦБрж╕рзБржЦ': 656,\n"," 'тЦБрж╕рзБржпрзЛржЧ': 657,\n"," 'тЦБржХржЦржиржУ': 658,\n"," 'тЦБржкрж╛рж░ржмрзЗ': 659,\n"," 'тЦБржЦрж╛рж░рж╛ржк': 660,\n"," 'тЦБржжрзЗржпрж╝': 661,\n"," 'тЦБржжрзЗржпрж╝ред': 662,\n"," 'тЦБржпрж╛ржУ': 663,\n"," 'ржкрзНрж░рждрзНржпрзЗ': 664,\n"," 'тЦБржлрзЗ': 665,\n"," 'тЦБржЕржЬрж╛ржирж╛\\n\\nЁЯТЩЁЯТЪ': 666,\n"," 'тЦБржЕржЬрж╛ржирж╛\\n\\nЁЯМ╖ЁЯМ╖рж╕рждрзНржпрж┐ржХрж╛рж░рзЗрж░': 667,\n"," 'тЦБржХрзНржпрж╛ржкрж╢ржи\\n': 668,\n"," 'тЦБржЖрж╕рж▓': 669,\n"," 'тЦБржЬрзАржмржи': 670,\n"," 'тЦБржмрж╛ржбрж╝': 671,\n"," 'ржХрж╛рждрзЗ': 672,\n"," 'тЦБрждрзБрж▓': 673,\n"," 'тЦБржорж╛ржирзБрж╖рзЗрж░': 674,\n"," 'ржЧрзБрж▓рзЛ': 675,\n"," 'тЦБржЖрж░ржУ': 676,\n"," 'тЦБржЖрж░рзЛ': 677,\n"," 'тЦБржЖржЫрзЗред': 678,\n"," 'тЦБржжрзБрж░рзН': 679,\n"," '\\nржХржЦржиржУ': 680,\n"," 'тЦБрж░ржЩ': 681,\n"," 'тЦБрж╣рзГржжржпрж╝рзЗ': 682,\n"," 'тЦБржЫрзЛржЯ': 683,\n"," 'тЦБржкрж░рзНржпржирзНржд': 684,\n"," 'тЦБрждрж╛ржХрж╛рждрзЗ': 685,\n"," '\\n\\nЁЯМ╣ЁЯМ╣ржпржЦржи': 686,\n"," 'тЦБржмржбрж╝ржЗ': 687,\n"," 'тЦБржжрзВрж░рзЗ': 688,\n"," 'тЦБржжрзВрж░рждрзНржм': 689,\n"," 'тЦБржирж┐ржЬрзЗрж░': 690,\n"," 'тЦБржирж┐ржЬрзЗржХрзЗ': 691,\n"," 'тЦБржЪрж╛ржЗржирж╛': 692,\n"," '\\nржкрзГржерж┐ржмрзА': 693,\n"," 'ржЬрзНржЮ': 694,\n"," 'ред\\nржпрзЗ': 695,\n"," 'тЦБBangl': 696,\n"," 'тЦБржмрзЛржЭ': 697,\n"," 'тЦБрж╕рж░рзНржмржжрж╛': 698,\n"," 'тЦБрж╣рж╛рж╕рж┐': 699,\n"," 'тЦБржорзЗржШ': 700,\n"," 'тЦБржЪрзЛржЦрзЗрж░': 701,\n"," '\\nржирждрзБржи': 702,\n"," 'рзВрж░рзНржг': 703,\n"," 'тЦБрж▓рж╛ржЧ': 704,\n"," 'тЦБржЬрж┐ржмрж░рж╛ржи': 705,\n"," 'тЦБрж╢рж┐ржХрзНрж╖': 706,\n"," 'тЦБрж╢ржмрзНржж': 707,\n"," 'тЦБBangla': 708,\n"," ',\\n': 709,\n"," 'Fa': 710,\n"," 'Na': 711,\n"," 'bo': 712,\n"," 'ce': 713,\n"," 'is': 714,\n"," 'me': 715,\n"," 'ok': 716,\n"," 'oy': 717,\n"," 'ржБржЪ': 718,\n"," 'ржЗрж▓': 719,\n"," 'ржЙржХрзЗ': 720,\n"," 'ржХрзЛ': 721,\n"," 'ржХрж▓рзЗрж░': 722,\n"," 'ржЦрзНржпрж╛': 723,\n"," 'ржЦрзЗржи': 724,\n"," 'ржЧрзЗ': 725,\n"," 'ржЩрзН': 726,\n"," 'ржЪрзЗрж░': 727,\n"," 'ржЪрж┐ржд': 728,\n"," 'ржЫрж░': 729,\n"," 'ржЬрзЗрж░': 730,\n"," 'ржЭрзЗ': 731,\n"," 'ржЯрзЛ': 732,\n"," 'ржарж┐ржХ': 733,\n"," 'рждрзНржд': 734,\n"," 'ржжрзБ': 735,\n"," 'ржжрзГ': 736,\n"," 'ржзрж╛ржи': 737,\n"," 'ржзрзГрждрж┐': 738,\n"," 'ржкржжрзЗрж░': 739,\n"," 'ржкрзВрж░рзНржг': 740,\n"," 'ржлрзЗрж╕ржмрзБ': 741,\n"," 'ржмрзЗрж░': 742,\n"," 'ржмрзНрж░рзБ': 743,\n"," 'ржнрзЗ': 744,\n"," 'ржорзО': 745,\n"," 'ржорж░рзН': 746,\n"," 'рж░рзЛ': 747,\n"," 'рж▓рзА': 748,\n"," 'рж╢,': 749,\n"," 'рж╢ржЗ': 750,\n"," 'рж╢рзИ': 751,\n"," 'рж╢рзНржп': 752,\n"," 'рж╢ржмрзЗрж░': 753,\n"," 'рж╖рзНржЯ': 754,\n"," 'рж╕рзНржЯ': 755,\n"," 'рж╕\\nржкрзГржерж┐ржмрзА': 756,\n"," 'рж╣рзЗ': 757,\n"," 'рж╣рж▓рзЗ': 758,\n"," 'рж╛ржБ': 759,\n"," 'рж╛ржо': 760,\n"," 'рж╛ржмрж╛рж░': 761,\n"," 'рж╛ржгрзЗрж░': 762,\n"," 'рж┐ржУ': 763,\n"," 'рж┐ржЪ': 764,\n"," 'рж┐рждрзНржм': 765,\n"," 'рж┐\\n\\nЁЯТЩЁЯТЪ': 766,\n"," 'рзЗржЫрж┐': 767,\n"," 'рзЗржирзНржб': 768,\n"," 'рзЗрж▓рзЗржи': 769,\n"," 'рзЛржЬржи': 770,\n"," 'рзНржи': 771,\n"," 'рзНрж▓': 772,\n"," 'рзНрж▓рж╛рж░': 773,\n"," 'рзНржнржм': 774,\n"," 'рзНржзрзГрждрж┐': 775,\n"," 'тЦБржЯрж┐': 776,\n"," 'тЦБрж░рзЗ': 777,\n"," 'тЦБFa': 778,\n"," 'тЦБNa': 779,\n"," 'тЦБржарж┐ржХ': 780,\n"," 'ржпрж╝рзЛржЬржи': 781,\n"," 'тЦБржмрж╕': 782,\n"," 'тЦБржмржирзНржп': 783,\n"," 'тЦБржмржЫрж░': 784,\n"," 'тЦБрж╕рж┐': 785,\n"," 'тЦБрж╕рждрзЗ': 786,\n"," 'тЦБрж╕рзНржо': 787,\n"," 'тЦБрж╕рзНржкрж░рзН': 788,\n"," 'тЦБрж╕ржирзНржз': 789,\n"," 'тЦБрж╕рзНржХ': 790,\n"," 'тЦБрж╕ржХрж▓рзЗрж░': 791,\n"," 'ржирзНржи': 792,\n"," 'ржирзНржо': 793,\n"," 'ржирзНржжрзЗрж░': 794,\n"," 'рж╛рж░ржг': 795,\n"," 'тЦБржЖржЧ': 796,\n"," 'тЦБржЖржи': 797,\n"," 'тЦБржЖрж▓рж╛': 798,\n"," 'тЦБржЖржЫрж┐': 799,\n"," 'тЦБржПржд': 800,\n"," 'тЦБржПржХрзЗ': 801,\n"," 'тЦБржиржпрж╝,': 802,\n"," 'тЦБржкрзБ': 803,\n"," 'тЦБржкржЫржирзНржж': 804,\n"," 'тЦБрждрж╛ржХрзЗ': 805,\n"," 'тЦБрждрждржЗ': 806,\n"," 'ржХрзЗржЗ': 807,\n"," 'ржХрзЗржжрзЗрж░': 808,\n"," 'тЦБрж╣ржУ': 809,\n"," 'тЦБрж╣рзЛ': 810,\n"," 'тЦБрж╣ржмрзЗ': 811,\n"," 'тЦБрж╣рж▓рзЗ': 812,\n"," 'тЦБрж╣рж╛рж░рж╛': 813,\n"," 'тЦБрж╣ржУржпрж╝рж╛': 814,\n"," 'тЦБрж╣рж▓рзЛ': 815,\n"," 'тЦБрж╣ржмрзЗред': 816,\n"," 'тЦБрж╣рзЗрж▓рзЗржи': 817,\n"," 'тЦБржпрж╛ржжрзЗрж░': 818,\n"," 'тЦБржоржи': 819,\n"," 'тЦБржорж╛рж░': 820,\n"," 'тЦБржоржирзЗрж░': 821,\n"," 'тЦБржоржзрзБрж░': 822,\n"," 'рж╛ржирзЗрж░': 823,\n"," 'рж╛ржирзЛрж░': 824,\n"," 'тЦБржХрж░ржмрзЗржи': 825,\n"," 'ржмрзЗ,': 826,\n"," 'ржмрзЗред\\nржкрзНрж░рж┐ржпрж╝': 827,\n"," 'тЦБржЬрзЗ': 828,\n"," 'тЦБржЬрзЛ': 829,\n"," 'тЦБржЬржирзНржо': 830,\n"," 'тЦБржжрж░': 831,\n"," 'тЦБржжрж┐ржи': 832,\n"," 'тЦБржжрзБржи': 833,\n"," 'рждрзЗред\\n': 834,\n"," 'тЦБржЕрзНржпрж╛': 835,\n"," 'тЦБржЕржкрж░': 836,\n"," 'тЦБржЕржжрзГ': 837,\n"," 'ред\\nржмржирзНржзрзБ': 838,\n"," 'тЦБржЖржорж╛ржпрж╝': 839,\n"," 'рзНржпрж╛ржпрж╝рзЗ': 840,\n"," 'тЦБржПржХржорж╛рждрзНрж░': 841,\n"," 'тЦБржПржХржжрж┐ржи': 842,\n"," 'тЦБржнрж┐': 843,\n"," 'тЦБржнрзЗ': 844,\n"," 'тЦБржирж╛ред\\nржПржХ': 845,\n"," 'тЦБржирж┐ржи': 846,\n"," 'тЦБржирж┐рждрзЗ': 847,\n"," 'ржЫрзЗред': 848,\n"," 'рж╛ржпрж╝рж╛рж░': 849,\n"," 'рж╛ржпрж╝рж╢ржЗ': 850,\n"," 'рждрзБржорж┐': 851,\n"," 'тЦБрж╕ржорж╕': 852,\n"," 'тЦБрж╕ржорзНржо': 853,\n"," 'тЦБрж╕ржорж╕рзНржд': 854,\n"," 'тЦБрж╢ржХрзНрждрж┐': 855,\n"," 'тЦБржкрзНрж░рзЗржо': 856,\n"," 'тЦБржкрзНрж░рж╛ржпрж╝рж╢ржЗ': 857,\n"," 'тЦБржЪрж▓рзЗ': 858,\n"," 'тЦБржЪржорзО': 859,\n"," 'тЦБрж╕рж╛ржо': 860,\n"," 'тЦБржмржирзНржзрзБрждрзНржмржЗ': 861,\n"," 'тЦБржмржирзНржзрзБрждрзНржмржХрзЗ': 862,\n"," 'тЦБржХрж░рзЗржи': 863,\n"," 'тЦБржХрж┐рж▓рж╛рж░': 864,\n"," 'ржбрж╝рж┐ржпрж╝рзЗ': 865,\n"," 'тЦБржЙржа': 866,\n"," 'тЦБржЙржЗрж▓': 867,\n"," 'тЦБржмрж┐ржнрж┐': 868,\n"," 'тЦБржмрж┐ржкржжрзЗрж░': 869,\n"," 'тЦБрждрзЛ,': 870,\n"," 'тЦБржерж╛ржХржмрзЗред': 871,\n"," 'тЦБржерж╛ржХрзБржХ': 872,\n"," 'рж┐ржмрж╛рж░': 873,\n"," 'тЦБржирж┐ржпрж╝рзЗ\\n': 874,\n"," 'тЦБрж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕,': 875,\n"," 'тЦБрж╕рзНржЯрзНржпрж╛ржЯрж╛рж╕ржХрзЗ': 876,\n"," 'тЦБрж╕рзБржЦрзЗрж░': 877,\n"," 'тЦБржХржЦржиржЗ': 878,\n"," 'ржУржпрж╝рж╛рж░': 879,\n"," 'рж▓рж╛ржо': 880,\n"," 'тЦБржЦрзБрж▓рзЗ': 881,\n"," 'тЦБржЦрж▓рж┐рж▓': 882,\n"," 'тЦБржЦрж╛ржмрж╛рж░': 883,\n"," 'тЦБржХрж╛ржЯрж╛ржЗ': 884,\n"," 'тЦБржХрж╛ржирзНржирж╛': 885,\n"," 'тЦБржХрж╛ржЙржХрзЗ': 886,\n"," 'тЦБржжрзЗржЦ': 887,\n"," 'тЦБржерж╛ржХрзЗржи': 888,\n"," 'тЦБрж╕ржоржпрж╝рзЗрж░': 889,\n"," 'тЦБрждрзЛржорж╛ржпрж╝': 890,\n"," 'рзБржиред': 891,\n"," 'тЦБржпрж╛ржУржпрж╝рж╛': 892,\n"," 'ржмржирзНржзрзБрж░рж╛': 893,\n"," 'ржкрзНрж░рж┐ржпрж╝': 894,\n"," 'рж╛рждрж╛': 895,\n"," 'тЦБржЕржЬрж╛ржирж╛\\n': 896,\n"," 'тЦБржЕржЬрж╛ржирж╛\\nржХржЦржиржУ': 897,\n"," 'тЦБржЕржЬрж╛ржирж╛\\n\\nЁЯМ╣ЁЯМ╣ржпржЦржи': 898,\n"," 'тЦБржХрзНржпрж╛ржкрж╢ржи,': 899,\n"," 'тЦБржЖрж╕рзЗ,': 900,\n"," 'тЦБржХрж┐ржЫрзБржЗ': 901,\n"," 'ржХрзЗрж░ржЗ': 902,\n"," 'тЦБрждрзБржЗ': 903,\n"," 'ржнрж╛ржЧ': 904,\n"," 'рж░рж┐\\n': 905,\n"," 'тЦБржорж╛ржирзБрж╖,': 906,\n"," 'тЦБржорж╛ржирзБрж╖ржЯрж┐': 907,\n"," '\\n\\nЁЯеАЁЯТЧржмржирзНржзрзБ': 908,\n"," '\\n\\nЁЯеАЁЯТЧржкрзНрж░рждрзНржпрзЗ': 909,\n"," '\\n\\nЁЯМ╣ЁЯМ╣ржмржирзНржзрзБрж░рж╛': 910,\n"," 'тЦБржнрж╛рж▓рзЛржмрж╛рж╕': 911,\n"," '\\nржПржХ': 912,\n"," '\\nржПржмржВ': 913,\n"," '\\nржПржоржи': 914,\n"," 'тЦБрж░рж╛ржЦрзЗ': 915,\n"," 'тЦБржЖржЫрзЗ,': 916,\n"," 'тЦБржпрж╛ржпрж╝,': 917,\n"," 'тЦБржпрж╛ржпрж╝ред\\n': 918,\n"," 'тЦБржжрзБржЯрзЛ': 919,\n"," 'тЦБржЕржирзЗржХрзЗржЗ': 920,\n"," '\\nржХрзЛржи': 921,\n"," '\\nржХрж╛рж░ржг': 922,\n"," 'рж╣рж╛ржо': 923,\n"," 'тЦБржЯрж╛ржи': 924,\n"," 'тЦБржЯрзБржХ': 925,\n"," 'тЦБржбрж╛': 926,\n"," 'тЦБржмрзНржпржм': 927,\n"," 'тЦБржХрзЗржоржи': 928,\n"," 'тЦБржирзЗржпрж╝': 929,\n"," 'тЦБржирзЗржпрж╝ред': 930,\n"," 'тЦБржирзЗржЗ,': 931,\n"," 'тЦБрж╕ржорзНржкрж░рзНржХ': 932,\n"," 'тЦБрж╢рзБрж░рзБ': 933,\n"," 'ржзрзНржпржорзЗ': 934,\n"," 'тЦБржЫржмрж┐': 935,\n"," 'тЦБржЫрж╛ржпрж╝рж╛рж░': 936,\n"," 'тЦБржЖржХрж╛рж╢рзЗ': 937,\n"," 'тЦБржмржирзНржзрзБрж░рж╛ржЗ': 938,\n"," 'тЦБржкрж░рзНржпрж╛ржпрж╝рзЗ': 939,\n"," 'тЦБржкрзГржерж┐ржмрзАрж░': 940,\n"," 'тЦБрждрж╛рж╣рж▓рзЗ': 941,\n"," 'ред\\nржЖржорж╛ржжрзЗрж░': 942,\n"," 'тЦБржмрж┐рж╢рзНржмрзЗрж░': 943,\n"," 'тЦБржмрж┐рж╢рзНржмрж╛рж╕': 944,\n"," 'тЦБржмрж┐рж╢рзНржмрж╕рзНржд': 945,\n"," 'тЦБржХрзЛржирзЛ': 946,\n"," 'тЦБржорж╛ржЭрзЗ': 947,\n"," 'тЦБржорж╛ржзрзНржпржорзЗ': 948,\n"," 'тЦБржХрж░рзБржиред': 949,\n"," 'тЦБржкрзНрж░рждрж┐ржЯрж┐': 950,\n"," 'тЦБржЦрзБржм': 951,\n"," 'тЦБрж╕рзБржирзНржжрж░,': 952,\n"," '\\nржкржбрж╝рзБржи': 953,\n"," 'nglis': 954,\n"," 'ржмрж╛рж░рзНржЯ': 955,\n"," 'ржпрж╝рж╛рж░рж┐': 956,\n"," 'тЦБржЖрждрзНржорж╛ржпрж╝': 957,\n"," 'тЦБржЖрждрзНржорж╛ржХрзЗ': 958,\n"," 'тЦБржпрждржХрзНрж╖': 959,\n"," 'тЦБржорзБржЫ': 960,\n"," 'тЦБржорзБржЫрзЗ': 961,\n"," 'тЦБрж╣рзГржжржпрж╝рзЗрж░': 962,\n"," 'тЦБрж╕рж╛рж╣рж╛ржпрзНржпрзЗрж░': 963,\n"," 'тЦБржЫрзЗрж▓рзЗржжрзЗрж░': 964,\n"," 'тЦБрж▓рзЛржХрзЗрж░': 965,\n"," 'тЦБрж▓рзЛржХрзЗржжрзЗрж░': 966,\n"," 'тЦБржмрзГрж╖рзНржЯрж┐рждрзЗ': 967,\n"," 'тЦБржЖржЬржХрзЗрж░': 968,\n"," 'тЦБрж╣рж╛рждрзЗрж░': 969,\n"," 'тЦБржЬрж┐ржЬрзНржЮ': 970,\n"," 'тЦБржнрж┐рждрж░': 971,\n"," 'тЦБрж╢рж┐ржЦрзЗржи': 972,\n"," 'тЦБрж╢рзЗржпрж╝рж╛рж░': 973,\n"," 'тЦБржлрзНрж░рзЗржирзНржб': 974,\n"," 'тЦБржлрзЗрж╕ржмрзБржХрзЗ': 975,\n"," 'тЦБржлрзЗрж╕ржмрзБржХрзЗрж░': 976,\n"," 'тЦБржЧрзЛрж▓рж╛ржкрзЗрж░': 977,\n"," 'тЦБржзрж░рзЗ': 978,\n"," 'тЦБрзирзз': 979,\n"," 'тЦБрж╕ржВржЧрзНрж░рж╣': 980,\n"," 'тЦБрж╣рж╛рж░рж┐ржпрж╝рзЗ': 981,\n"," 'тЦБржЕржирзБржнржм': 982,\n"," 'тЦБржнрзБрж▓': 983,\n"," 'тЦБрж╢рждрзНрж░рзБ': 984,\n"," 'тЦБрж╕рж╛ржЬрж┐ржпрж╝рзЗржЫрж┐ред': 985,\n"," 'тЦБржЙржжрзНржзрзГрждрж┐': 986,\n"," 'тЦБржлрзЗржмрзНрж░рзБ': 987,\n"," 'тЦБрждрзБрж▓рждрзЗ': 988,\n"," 'тЦБржЪрж╛ржЗржирж╛,': 989,\n"," 'тЦБржмрзЛржЭрзЗ,': 990,\n"," 'тЦБржорзЗржШрж▓рж╛': 991,\n"," 'book': 992,\n"," 'cebook': 993,\n"," 'ржлрзЗрж╕ржмрзБржХ': 994,\n"," 'рж╢рзИрж╢ржмрзЗрж░': 995,\n"," 'рж╕\\nржкрзГржерж┐ржмрзАрж░': 996,\n"," 'тЦБрж░рзЗржЦрзЗ': 997,\n"," 'тЦБFacebook': 998,\n"," 'тЦБName': 999,\n"," ...}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# vocab"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:21:37.288185Z","iopub.status.busy":"2024-04-02T09:21:37.287654Z","iopub.status.idle":"2024-04-02T09:21:37.308396Z","shell.execute_reply":"2024-04-02T09:21:37.306796Z","shell.execute_reply.started":"2024-04-02T09:21:37.288146Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["\n","tokenizer.train_from_iterator(\n","    iterator=text,\n","    vocab_size=30_000,\n","    min_frequency=5,\n","    show_progress=True,\n","    limit_alphabet=500,\n","    special_tokens=[\n","        \"<pad>\",\n","        \"<s>\",\n","        \"</s>\",\n","        \"<unk>\",\n","        '<mask>'\n","    ]\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.816570Z","iopub.status.busy":"2024-04-02T09:18:34.815937Z","iopub.status.idle":"2024-04-02T09:18:34.821046Z","shell.execute_reply":"2024-04-02T09:18:34.820308Z","shell.execute_reply.started":"2024-04-02T09:18:34.816533Z"},"trusted":true},"outputs":[],"source":["from tokenizers.processors import BertProcessing\n","tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.823225Z","iopub.status.busy":"2024-04-02T09:18:34.822541Z","iopub.status.idle":"2024-04-02T09:18:34.833708Z","shell.execute_reply":"2024-04-02T09:18:34.832649Z","shell.execute_reply.started":"2024-04-02T09:18:34.823189Z"},"trusted":true},"outputs":[],"source":["tokenizer.save(\"tokenizer.json\")"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.836338Z","iopub.status.busy":"2024-04-02T09:18:34.835377Z","iopub.status.idle":"2024-04-02T09:18:40.120461Z","shell.execute_reply":"2024-04-02T09:18:40.119153Z","shell.execute_reply.started":"2024-04-02T09:18:34.836306Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='', vocab_size=110, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import PreTrainedTokenizerFast\n","\n","transformer_tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_object=tokenizer #tokenizer_file=\"tokenizer.json\"\n",")\n","transformer_tokenizer"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:40.125801Z","iopub.status.busy":"2024-04-02T09:18:40.125217Z","iopub.status.idle":"2024-04-02T09:18:40.134986Z","shell.execute_reply":"2024-04-02T09:18:40.133816Z","shell.execute_reply.started":"2024-04-02T09:18:40.125768Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['тЦБ',\n"," 'ржЧ',\n"," 'рзЗрж▓',\n"," 'тЦБржЗржЙржирж┐ржЯрзЗржХ',\n"," 'тЦБржк',\n"," 'рзНрж░',\n"," 'ржб',\n"," 'рж╛',\n"," 'ржХрзНржЯ',\n"," 'рж╕',\n"," 'тЦБ(',\n"," 'ржм',\n"," 'рж┐',\n"," 'ржб',\n"," 'рж┐',\n"," ')']"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["test_text = normalize(\"ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ (ржмрж┐ржбрж┐)\")\n","transformer_tokenizer.tokenize(test_text)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:40.136429Z","iopub.status.busy":"2024-04-02T09:18:40.136044Z","iopub.status.idle":"2024-04-02T09:19:28.805005Z","shell.execute_reply":"2024-04-02T09:19:28.803933Z","shell.execute_reply.started":"2024-04-02T09:18:40.136375Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbaa6910d8354933aded80b548b40540","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Virus-Proton/CustomTokenizer/commit/24814882f2a22c2c36b7d1e0d2849dab431266c6', commit_message='Upload tokenizer', commit_description='', oid='24814882f2a22c2c36b7d1e0d2849dab431266c6', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["transformer_tokenizer.push_to_hub(\"Virus-Proton/CustomTokenizer\")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:28.807853Z","iopub.status.busy":"2024-04-02T09:19:28.806649Z","iopub.status.idle":"2024-04-02T09:19:42.107695Z","shell.execute_reply":"2024-04-02T09:19:42.105990Z","shell.execute_reply.started":"2024-04-02T09:19:28.807809Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75b99022c1fb4f79afb949515cfa6509","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9e2b0bd156b4d3aace00e5703ab21d3","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/4.88k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ea3a753e674472f879b2160fefb4951","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","custom_tokenizer = AutoTokenizer.from_pretrained(\"Virus-Proton/CustomTokenizer\")"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:42.110960Z","iopub.status.busy":"2024-04-02T09:19:42.110374Z","iopub.status.idle":"2024-04-02T09:19:42.121819Z","shell.execute_reply":"2024-04-02T09:19:42.120418Z","shell.execute_reply.started":"2024-04-02T09:19:42.110910Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [1, 59, 21, 87, 101, 88, 66, 28, 48, 76, 45, 74, 37, 49, 28, 49, 6, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_text = custom_tokenizer.encode_plus(test_text)\n","tokenized_text"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:42.124808Z","iopub.status.busy":"2024-04-02T09:19:42.124128Z","iopub.status.idle":"2024-04-02T09:19:55.751730Z","shell.execute_reply":"2024-04-02T09:19:55.750450Z","shell.execute_reply.started":"2024-04-02T09:19:42.124742Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-02 09:19:44.571283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-02 09:19:44.571528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-02 09:19:44.763675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"text/plain":["'<s> ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ (ржмрж┐ржбрж┐)</s>'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["custom_tokenizer.decode(tokenized_text['input_ids'])"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:55.754065Z","iopub.status.busy":"2024-04-02T09:19:55.753406Z","iopub.status.idle":"2024-04-02T09:19:55.762846Z","shell.execute_reply":"2024-04-02T09:19:55.761535Z","shell.execute_reply.started":"2024-04-02T09:19:55.754028Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['<s>',\n"," 'тЦБ',\n"," 'ржЧ',\n"," 'рзЗрж▓',\n"," 'тЦБржЗржЙржирж┐ржЯрзЗржХ',\n"," 'тЦБржк',\n"," 'рзНрж░',\n"," 'ржб',\n"," 'рж╛',\n"," 'ржХрзНржЯ',\n"," 'рж╕',\n"," 'тЦБ(',\n"," 'ржм',\n"," 'рж┐',\n"," 'ржб',\n"," 'рж┐',\n"," ')',\n"," '</s>']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["custom_tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])"]},{"cell_type":"markdown","metadata":{},"source":["# Advanced"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","        vocab_size (int, optional) тАУ The size of the final vocabulary, including all tokens and alphabet.\n","\n","        min_frequency (int, optional) тАУ The minimum frequency a pair should have in order to be merged.\n","\n","        show_progress (bool, optional) тАУ Whether to show progress bars while training.\n","\n","        special_tokens (List[Union[str, AddedToken]], optional) тАУ A list of special tokens the model should know of.\n","\n","        limit_alphabet (int, optional) тАУ The maximum different characters to keep in the alphabet.\n","\n","        initial_alphabet (List[str], optional) тАУ A list of characters to include in the initial alphabet, even if not seen in the training dataset. If the strings contain more than one character, only the first one is kept.\n","\n","        continuing_subword_prefix (str, optional) тАУ A prefix to be used for every subword that is not a beginning-of-word.\n","\n","        end_of_word_suffix (str, optional) тАУ A suffix to be used for every subword that is a end-of-word.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:38:30.051021Z","iopub.status.busy":"2024-04-02T09:38:30.050634Z","iopub.status.idle":"2024-04-02T09:38:36.166062Z","shell.execute_reply":"2024-04-02T09:38:36.164777Z","shell.execute_reply.started":"2024-04-02T09:38:30.050991Z"},"trusted":true},"outputs":[],"source":["import tokenizers\n","tokenizers.__version__\n","\n","from tokenizers import SentencePieceBPETokenizer\n","from tokenizers.processors import BertProcessing\n","from transformers import PreTrainedTokenizerFast"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:42:54.967272Z","iopub.status.busy":"2024-04-02T09:42:54.966727Z","iopub.status.idle":"2024-04-02T09:42:54.976349Z","shell.execute_reply":"2024-04-02T09:42:54.974723Z","shell.execute_reply.started":"2024-04-02T09:42:54.967220Z"},"trusted":true},"outputs":[],"source":["texts = ['рж╕рж╛ржнрж╛рж░рзЗрж░ ржХржмрж┐рж░ржкрзБрж░ ржмрж╛ржгрж┐ржЬрзНржпрж┐ржХ ржПрж▓рж╛ржХрж╛рж░ ржирж┐ржЬрж╕рзНржм ржлрзНржпрж╛ржХрзНржЯрж░рж┐рждрзЗ рж╣рзЯрзЗ ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ (ржмрж┐ржбрж┐) рж▓рж┐ржГ ржПрж░ ржжрж┐ржиржмрзНржпрж╛ржкрзА рж╕рзЗрж▓рж╕ ржХржиржлрж╛рж░рзЗржирзНрж╕ред ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐ ржЕржирзБрж╖рзНржарж┐ржд рж╣рзЯ рж╢ржирж┐ржмрж╛рж░, рззрззржЗ ржорж╛рж░рзНржЪ, рж╕рж╛рж░рж╛ ржжрзЗрж╢ ржерзЗржХрзЗ ржкрзНрж░рж╛рзЯ рзйрззрзж ржЬржи ржбрж┐рж▓рж╛рж░ ржПржЗ ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐рждрзЗ ржЕржВрж╢ржЧрзНрж░рж╣ржг ржХрж░рзЗржи ржПржмржВ ржЗржЙржирж┐ржЯрзЗржХрж░ ржлрзНржпрж╛ржХрзНржЯрж░рж┐ ржШрзБрж░рзЗ ржжрзЗржЦрж╛рж░ рж╕рзБржпрзЛржЧ ржкрж╛ржиредржПрж╕ржорзЯ ржЗржЙржирж┐ржЯрзЗржХрж░ ржмрзНржпржмрж╕рзНржерж╛ржкржирж╛ ржкрж░рж┐ржЪрж╛рж▓ржХ, ржЗржЮрзНржЬрж┐: ржЖржирж┐рж╕ ржЖрж╣ржорзЗржж ржбрж┐рж▓рж╛рж░ржжрзЗрж░ ржЙржжрзНржжрзЗрж╢рзЗ ржмржХрзНрждржмрзНржп рж░рж╛ржЦрзЗржи ржПржмржВ ржЗржЙржирж┐ржЯрзЗржХ ржХрзЗ ржмрж╛ржВрж▓рж╛ржжрзЗрж╢рзЗрж░ рж╕рзЗрж░рж╛ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХ ржмрзНрж░рзНржпрж╛ржирзНржб рж╣рж┐рж╕рзЗржмрзЗ ржЧрзЬрзЗ рждрзЛрж▓рж╛рж░ ржЖрж╢рзНржмрж╛рж╕ ржмрзНржпржХрзНржд ржХрж░рзЗржиред ржЗржЮрзНржЬрж┐: ржЖржирж┐рж╕ ржЖрж╣ржорзЗржж, ржЬрж╛рждрзАрзЯ ржПржмржВ ржмрж┐ржнрж╛ржЧрзАрзЯ ржкрж░рзНржпрж╛рзЯрзЗ рж╕рзЗрж░рж╛ ржбрж┐рж▓рж╛рж░ржжрзЗрж░ рж╣рж╛рждрзЗ рж╕рж╛рж░рзНржЯрж┐ржлрж┐ржХрзЗржЯ, ржХрзНрж░рзЗрж╕рзНржЯ ржУ ржкрзНрж░рж╛ржЗржЬ ржмржирзНржб рждрзБрж▓рзЗ ржжрзЗржиред ржмрж┐ржЬрзЯрзАрж░рж╛ рж╣рж▓рзЗржи рж░рзБржкрж╛рж▓рж┐ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), рж▓рж┐рзЯрж╛ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ (ржжрзНржмрж┐рждрзАрзЯ ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), ржоржжрж┐ржирж╛ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (рждрзГрждрзАрзЯ ржЬрж╛рждрзАрзЯ рж╕рзЗрж░рж╛), рж╕рзБржирзНржжрж░ржмржи ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржврж╛ржХрж╛ ржмрж┐ржнрж╛ржЧ), ржЖрж▓рж╛ржЙржжрзНржжрж┐ржи ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржЪрж┐ржЯрж╛ржЧржВ ржмрж┐ржнрж╛ржЧ), рж╢рж╛ржирзНржд ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (рж░рж╛ржЬрж╢рж╛рж╣рзА рж░ржВржкрзБрж░ ржмрж┐ржнрж╛ржЧ), ржирзБрж░рзБрж▓ ржПржирзНржЯрж╛рж░ржкрзНрж░рж╛ржЗржЬ (ржЦрзБрж▓ржирж╛ ржмрж┐ржнрж╛ржЧ), ржлрзЗрзЯрж╛рж░ ржнрж┐ржЙ ржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕ (ржмрж░рж┐рж╢рж╛рж▓ ржмрж┐ржнрж╛ржЧ) ржУ ржлрж┐ржбржмрзНржпрж╛ржХ ржХржорж┐ржЙржирж┐ржХрзЗрж╕ржи (рж╕рж┐рж▓рзЗржЯ ржмрж┐ржнрж╛ржЧ)ред ржХржиржлрж╛рж░рзЗржирзНрж╕ржЯрж┐рждрзЗ ржЖрж░ржУ ржмржХрзНрждржмрзНржп рж░рж╛ржЦрзЗржи ржЗржЙржирж┐ржЯрзЗржХрзЗрж░ ржмрж┐ржЬржирзЗрж╕ ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржЕрзНржпрж╛ржирзНржб ржкрзНрж▓рзНржпрж╛ржирж┐ржВ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржП ржХрзЗ ржПржо рж╣рж╛ржорж┐ржжрзБрж░ рж░рж╣ржорж╛ржи, рж╕рзЗрж▓рж╕ ржЕрзНржпрж╛ржирзНржб ржбрзЗржнрзЗрж▓ржкржорзЗржирзНржЯ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржП ржЯрж┐ ржПржо ржЖржЦрждрж╛рж░ рж╣рзЛрж╕рзЗржи ржУ ржЗржЙржирж┐ржЯрзЗржХрзЗрж░ рж╕рзЗрж▓рж╕ ржорзНржпрж╛ржирзЗржЬрж╛рж░ ржбрж┐рж▓рж╛рж░ ржЪрзНржпрж╛ржирзЗрж▓ ржЬрж╛ржХрж┐рж░ рж╣рзЛрж╕рзЗржиред -ржкрзНрж░рзЗрж╕ ржмрж┐ржЬрзНржЮржкрзНрждрж┐']"]},{"cell_type":"code","execution_count":31,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T10:09:59.410496Z","iopub.status.busy":"2024-04-02T10:09:59.410077Z","iopub.status.idle":"2024-04-02T10:09:59.422629Z","shell.execute_reply":"2024-04-02T10:09:59.421407Z","shell.execute_reply.started":"2024-04-02T10:09:59.410466Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on class SentencePieceBPETokenizer in module tokenizers.implementations.sentencepiece_bpe:\n","\n","class SentencePieceBPETokenizer(tokenizers.implementations.base_tokenizer.BaseTokenizer)\n"," |  SentencePieceBPETokenizer(vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = 'тЦБ', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |  \n"," |  SentencePiece BPE Tokenizer\n"," |  \n"," |  Represents the BPE algorithm, with the pretokenization used by SentencePiece\n"," |  \n"," |  Method resolution order:\n"," |      SentencePieceBPETokenizer\n"," |      tokenizers.implementations.base_tokenizer.BaseTokenizer\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = 'тЦБ', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  train(self, files: Union[str, List[str]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True)\n"," |      Train the model using the given files\n"," |  \n"," |  train_from_iterator(self, iterator: Union[Iterator[str], Iterator[Iterator[str]]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True, length: Optional[int] = None)\n"," |      Train the model using the given iterator\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods defined here:\n"," |  \n"," |  from_file(vocab_filename: str, merges_filename: str, **kwargs)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __slotnames__ = []\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  add_special_tokens(self, special_tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given special tokens to the vocabulary, and treat them as special tokens.\n"," |      \n"," |      The special tokens will never be processed by the model, and will be\n"," |      removed while decoding.\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of special tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  add_tokens(self, tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given tokens to the vocabulary\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the given list of ids to a string sequence\n"," |      \n"," |      Args:\n"," |          ids: List[unsigned int]:\n"," |              A list of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output string\n"," |      \n"," |      Returns:\n"," |          The decoded string\n"," |  \n"," |  decode_batch(self, sequences: List[List[int]], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the list of sequences to a list of string sequences\n"," |      \n"," |      Args:\n"," |          sequences: List[List[unsigned int]]:\n"," |              A list of sequence of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output strings\n"," |      \n"," |      Returns:\n"," |          A list of decoded strings\n"," |  \n"," |  enable_padding(self, direction: Optional[str] = 'right', pad_to_multiple_of: Optional[int] = None, pad_id: Optional[int] = 0, pad_type_id: Optional[int] = 0, pad_token: Optional[str] = '[PAD]', length: Optional[int] = None)\n"," |      Change the padding strategy\n"," |      \n"," |      Args:\n"," |          direction: (`optional`) str:\n"," |              Can be one of: `right` or `left`\n"," |      \n"," |          pad_to_multiple_of: (`optional`) unsigned int:\n"," |              If specified, the padding length should always snap to the next multiple of\n"," |              the given value. For example if we were going to pad with a length of 250 but\n"," |              `pad_to_multiple_of=8` then we will pad to 256.\n"," |      \n"," |          pad_id: (`optional`) unsigned int:\n"," |              The indice to be used when padding\n"," |      \n"," |          pad_type_id: (`optional`) unsigned int:\n"," |              The type indice to be used when padding\n"," |      \n"," |          pad_token: (`optional`) str:\n"," |              The pad token to be used when padding\n"," |      \n"," |          length: (`optional`) unsigned int:\n"," |              If specified, the length at which to pad. If not specified\n"," |              we pad using the size of the longest sequence in a batch\n"," |  \n"," |  enable_truncation(self, max_length: int, stride: Optional[int] = 0, strategy: Optional[str] = 'longest_first')\n"," |      Change the truncation options\n"," |      \n"," |      Args:\n"," |          max_length: unsigned int:\n"," |              The maximum length at which to truncate\n"," |      \n"," |          stride: (`optional`) unsigned int:\n"," |              The length of the previous first sequence to be included\n"," |              in the overflowing sequence\n"," |      \n"," |          strategy: (`optional`) str:\n"," |              Can be one of `longest_first`, `only_first` or `only_second`\n"," |  \n"," |  encode(self, sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Encode the given sequence and pair. This method can process raw text sequences as well\n"," |      as already pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          sequence: InputSequence:\n"," |              The sequence we want to encode. This sequence can be either raw text or\n"," |              pre-tokenized, according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          An Encoding\n"," |  \n"," |  encode_batch(self, inputs: List[Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]], is_pretokenized: bool = False, add_special_tokens: bool = True) -> List[tokenizers.Encoding]\n"," |      Encode the given inputs. This method accept both raw text sequences as well as already\n"," |      pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          inputs: List[EncodeInput]:\n"," |              A list of single sequences or pair sequences to encode. Each `EncodeInput` is\n"," |              expected to be of the following form:\n"," |                  `Union[InputSequence, Tuple[InputSequence, InputSequence]]`\n"," |      \n"," |              Each `InputSequence` can either be raw text or pre-tokenized,\n"," |              according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          A list of Encoding\n"," |  \n"," |  get_added_tokens_decoder(self) -> Dict[int, tokenizers.AddedToken]\n"," |      Returns the added reverse vocabulary\n"," |      \n"," |      Returns:\n"," |          The added vocabulary mapping ints to AddedTokens\n"," |  \n"," |  get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]\n"," |      Returns the vocabulary\n"," |      \n"," |      Args:\n"," |          with_added_tokens: boolean:\n"," |              Whether to include the added tokens in the vocabulary\n"," |      \n"," |      Returns:\n"," |          The vocabulary\n"," |  \n"," |  get_vocab_size(self, with_added_tokens: bool = True) -> int\n"," |      Return the size of vocabulary, with or without added tokens.\n"," |      \n"," |      Args:\n"," |          with_added_tokens: (`optional`) bool:\n"," |              Whether to count in added special tokens or not\n"," |      \n"," |      Returns:\n"," |          Size of vocabulary\n"," |  \n"," |  id_to_token(self, id: int) -> Optional[str]\n"," |      Convert the given token id to its corresponding string\n"," |      \n"," |      Args:\n"," |          token: id:\n"," |              The token id to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding string if it exists, None otherwise\n"," |  \n"," |  no_padding(self)\n"," |      Disable padding\n"," |  \n"," |  no_truncation(self)\n"," |      Disable truncation\n"," |  \n"," |  normalize(self, sequence: str) -> str\n"," |      Normalize the given sequence\n"," |      \n"," |      Args:\n"," |          sequence: str:\n"," |              The sequence to normalize\n"," |      \n"," |      Returns:\n"," |          The normalized string\n"," |  \n"," |  num_special_tokens_to_add(self, is_pair: bool) -> int\n"," |      Return the number of special tokens that would be added for single/pair sentences.\n"," |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n"," |      :return:\n"," |  \n"," |  post_process(self, encoding: tokenizers.Encoding, pair: Optional[tokenizers.Encoding] = None, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Apply all the post-processing steps to the given encodings.\n"," |      \n"," |      The various steps are:\n"," |          1. Truncate according to global params (provided to `enable_truncation`)\n"," |          2. Apply the PostProcessor\n"," |          3. Pad according to global params. (provided to `enable_padding`)\n"," |      \n"," |      Args:\n"," |          encoding: Encoding:\n"," |              The main Encoding to post process\n"," |      \n"," |          pair: Optional[Encoding]:\n"," |              An optional pair Encoding\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add special tokens\n"," |      \n"," |      Returns:\n"," |          The resulting Encoding\n"," |  \n"," |  save(self, path: str, pretty: bool = True)\n"," |      Save the current Tokenizer at the given path\n"," |      \n"," |      Args:\n"," |          path: str:\n"," |              A path to the destination Tokenizer file\n"," |  \n"," |  save_model(self, directory: str, prefix: Optional[str] = None)\n"," |      Save the current model to the given directory\n"," |      \n"," |      Args:\n"," |          directory: str:\n"," |              A path to the destination directory\n"," |      \n"," |          prefix: (Optional) str:\n"," |              An optional prefix, used to prefix each file name\n"," |  \n"," |  to_str(self, pretty: bool = False)\n"," |      Get a serialized JSON version of the Tokenizer as a str\n"," |      \n"," |      Args:\n"," |          pretty: bool:\n"," |              Whether the JSON string should be prettified\n"," |      \n"," |      Returns:\n"," |          str\n"," |  \n"," |  token_to_id(self, token: str) -> Optional[int]\n"," |      Convert the given token to its corresponding id\n"," |      \n"," |      Args:\n"," |          token: str:\n"," |              The token to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding id if it exists, None otherwise\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  padding\n"," |      Get the current padding parameters\n"," |      \n"," |      Returns:\n"," |          None if padding is disabled, a dict with the currently set parameters\n"," |          if the padding is enabled.\n"," |  \n"," |  truncation\n"," |      Get the current truncation parameters\n"," |      \n"," |      Returns:\n"," |          None if truncation is disabled, a dict with the current truncation parameters if\n"," |          truncation is enabled\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  decoder\n"," |  \n"," |  model\n"," |  \n"," |  normalizer\n"," |  \n"," |  post_processor\n"," |  \n"," |  pre_tokenizer\n","\n"]}],"source":["# help(SentencePieceBPETokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["### Train the tokenizer\n","\n","help(SentencePieceBPETokenizer)<br>\n","self, files: Union[str, List[str]], \n","vocab_size: int = 30000, min_frequency: int = 2, \n","special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], \n","limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:02:53.741021Z","iopub.status.busy":"2024-04-02T10:02:53.740527Z","iopub.status.idle":"2024-04-02T10:02:53.748354Z","shell.execute_reply":"2024-04-02T10:02:53.746374Z","shell.execute_reply.started":"2024-04-02T10:02:53.740988Z"},"trusted":true},"outputs":[],"source":["tokenizer = SentencePieceBPETokenizer()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:00:49.993492Z","iopub.status.busy":"2024-04-02T10:00:49.993002Z","iopub.status.idle":"2024-04-02T10:00:50.002277Z","shell.execute_reply":"2024-04-02T10:00:50.000633Z","shell.execute_reply.started":"2024-04-02T10:00:49.993460Z"},"trusted":true},"outputs":[],"source":["# special_token = [{\"id\":0,\"special\":True,\"content\":\"<s>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":1,\"special\":True,\"content\":\"<pad>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":2,\"special\":True,\"content\":\"</s>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":3,\"special\":True,\"content\":\"<unk>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":50264,\"special\":True,\"content\":\"<mask>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False}]"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:03:55.489071Z","iopub.status.busy":"2024-04-02T10:03:55.488348Z","iopub.status.idle":"2024-04-02T10:03:55.499638Z","shell.execute_reply":"2024-04-02T10:03:55.497961Z","shell.execute_reply.started":"2024-04-02T10:03:55.489035Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignored unknown kwarg option id\n"]},{"data":{"text/plain":["AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:05:52.067104Z","iopub.status.busy":"2024-04-02T10:05:52.066647Z","iopub.status.idle":"2024-04-02T10:05:52.086951Z","shell.execute_reply":"2024-04-02T10:05:52.085037Z","shell.execute_reply.started":"2024-04-02T10:05:52.067072Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["## Training\n","\n","tokenizer.train_from_iterator(\n","    texts,\n","    vocab_size=50_265,\n","    min_frequency=5, # rare word treatment\n","    show_progress=True,\n","    limit_alphabet=1000,\n","    special_tokens=[\n","        \"<s>\",\n","        \"</s>\",\n","        \"<unk>\" ,\n","        \"<pad>\",\n","    ]\n",")\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:21.341784Z","iopub.status.busy":"2024-04-02T10:12:21.340753Z","iopub.status.idle":"2024-04-02T10:12:21.352681Z","shell.execute_reply":"2024-04-02T10:12:21.350923Z","shell.execute_reply.started":"2024-04-02T10:12:21.341741Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["op={\"special\":True,\"content\":\"<mask>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False}\n","mask_token = tokenizers.AddedToken(**op)\n","tokenizer.add_tokens([mask_token])"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:25.491590Z","iopub.status.busy":"2024-04-02T10:12:25.491097Z","iopub.status.idle":"2024-04-02T10:12:25.498836Z","shell.execute_reply":"2024-04-02T10:12:25.497219Z","shell.execute_reply.started":"2024-04-02T10:12:25.491554Z"},"trusted":true},"outputs":[],"source":["## Add post processing\n","\n","tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","       \n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:29.671812Z","iopub.status.busy":"2024-04-02T10:12:29.671302Z","iopub.status.idle":"2024-04-02T10:12:29.677820Z","shell.execute_reply":"2024-04-02T10:12:29.676065Z","shell.execute_reply.started":"2024-04-02T10:12:29.671775Z"},"trusted":true},"outputs":[],"source":["tokenizer.save(\"tokenizer.json\")"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:31.358591Z","iopub.status.busy":"2024-04-02T10:12:31.358027Z","iopub.status.idle":"2024-04-02T10:12:31.370535Z","shell.execute_reply":"2024-04-02T10:12:31.367996Z","shell.execute_reply.started":"2024-04-02T10:12:31.358552Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'version': '1.0', 'truncation': None, 'padding': None, 'added_tokens': [{'id': 0, 'content': '<s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 1, 'content': '</s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 2, 'content': '<unk>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 3, 'content': '<pad>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 109, 'content': '<mask>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': False}], 'normalizer': {'type': 'NFKC'}, 'pre_tokenizer': {'type': 'Metaspace', 'replacement': 'тЦБ', 'add_prefix_space': True, 'prepend_scheme': 'always'}, 'post_processor': {'type': 'BertProcessing', 'sep': ['</s>', 1], 'cls': ['<s>', 0]}, 'decoder': {'type': 'Metaspace', 'replacement': 'тЦБ', 'add_prefix_space': True, 'prepend_scheme': 'always'}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False, 'vocab': {'<s>': 0, '</s>': 1, '<unk>': 2, '<pad>': 3, '(': 4, ')': 5, ',': 6, '-': 7, ':': 8, 'ред': 9, 'ржВ': 10, 'ржГ': 11, 'ржЕ': 12, 'ржЖ': 13, 'ржЗ': 14, 'ржЙ': 15, 'ржП': 16, 'ржУ': 17, 'ржХ': 18, 'ржЦ': 19, 'ржЧ': 20, 'ржШ': 21, 'ржЪ': 22, 'ржЬ': 23, 'ржЮ': 24, 'ржЯ': 25, 'ржа': 26, 'ржб': 27, 'ржв': 28, 'ржг': 29, 'ржд': 30, 'рже': 31, 'ржж': 32, 'ржи': 33, 'ржк': 34, 'ржл': 35, 'ржм': 36, 'ржн': 37, 'ржо': 38, 'ржп': 39, 'рж░': 40, 'рж▓': 41, 'рж╢': 42, 'рж╖': 43, 'рж╕': 44, 'рж╣': 45, 'рж╝': 46, 'рж╛': 47, 'рж┐': 48, 'рзА': 49, 'рзБ': 50, 'рзГ': 51, 'рзЗ': 52, 'рзЛ': 53, 'рзН': 54, 'рзж': 55, 'рзз': 56, 'рзй': 57, 'тЦБ': 58, 'рж╛рж░': 59, 'ржХрзН': 60, 'ржирж┐': 61, 'рзНржп': 62, 'тЦБржм': 63, 'ржпрж╝': 64, 'рзНрж░': 65, 'тЦБржЗ': 66, 'ржирзН': 67, 'тЦБрж╕': 68, 'рзНржпрж╛': 69, 'тЦБржП': 70, 'рзЗрж░': 71, 'рж▓рзЗ': 72, 'тЦБ(': 73, 'тЦБржХ': 74, 'ржХрзНржЯ': 75, 'тЦБржмрж┐': 76, 'рж╛ржЧ': 77, 'тЦБржЖ': 78, 'тЦБрж╣': 79, '),': 80, 'ржЙржирж┐': 81, 'ржнрж╛ржЧ': 82, 'рзАржпрж╝': 83, 'рзЗржХ': 84, 'рзЗржи': 85, 'рзЗрж▓': 86, 'тЦБржк': 87, 'рзНрж░ржирж┐': 88, 'тЦБржЗрж▓рзЗ': 89, 'ржХрзНржЯрзНрж░ржирж┐': 90, 'тЦБржмрж┐ржнрж╛ржЧ': 91, 'тЦБржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐': 92, 'ржЯрзЗржХ': 93, 'рждрзАржпрж╝': 94, 'рзЗржирзН': 95, 'тЦБржб': 96, 'ржХрзНрж╕': 97, 'тЦБржЗржЙржирж┐': 98, 'тЦБржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ржХрзНрж╕': 99, 'тЦБржЗржЙржирж┐ржЯрзЗржХ': 100, 'ржЬрж╛': 101, 'ржЯрж┐': 102, 'рж▓рж╛рж░': 103, 'рзБрж░': 104, 'тЦБржо': 105, 'тЦБрж░': 106, 'тЦБрж╕рзЗрж░': 107, 'тЦБрж╕рзЗрж░рж╛': 108}, 'merges': ['рж╛ рж░', 'ржХ рзН', 'ржи рж┐', 'рзН ржп', 'тЦБ ржм', 'ржп рж╝', 'рзН рж░', 'тЦБ ржЗ', 'ржи рзН', 'тЦБ рж╕', 'рзНржп рж╛', 'тЦБ ржП', 'рзЗ рж░', 'рж▓ рзЗ', 'тЦБ (', 'тЦБ ржХ', 'ржХрзН ржЯ', 'тЦБржм рж┐', 'рж╛ ржЧ', 'тЦБ ржЖ', 'тЦБ рж╣', ') ,', 'ржЙ ржирж┐', 'ржн рж╛ржЧ', 'рзА ржпрж╝', 'рзЗ ржХ', 'рзЗ ржи', 'рзЗ рж▓', 'тЦБ ржк', 'рзНрж░ ржирж┐', 'тЦБржЗ рж▓рзЗ', 'ржХрзНржЯ рзНрж░ржирж┐', 'тЦБржмрж┐ ржнрж╛ржЧ', 'тЦБржЗрж▓рзЗ ржХрзНржЯрзНрж░ржирж┐', 'ржЯ рзЗржХ', 'ржд рзАржпрж╝', 'рзЗ ржирзН', 'тЦБ ржб', 'ржХрзН рж╕', 'тЦБржЗ ржЙржирж┐', 'тЦБржЗрж▓рзЗржХрзНржЯрзНрж░ржирж┐ ржХрзНрж╕', 'тЦБржЗржЙржирж┐ ржЯрзЗржХ', 'ржЬ рж╛', 'ржЯ рж┐', 'рж▓ рж╛рж░', 'рзБ рж░', 'тЦБ ржо', 'тЦБ рж░', 'тЦБрж╕ рзЗрж░', 'тЦБрж╕рзЗрж░ рж╛']}}\n"]}],"source":["import json\n","with open('tokenizer.json', 'r') as f:\n","    d = json.load(f)\n","    print(d)\n","    "]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:48.319664Z","iopub.status.busy":"2024-04-02T10:12:48.319121Z","iopub.status.idle":"2024-04-02T10:12:48.331321Z","shell.execute_reply":"2024-04-02T10:12:48.329792Z","shell.execute_reply.started":"2024-04-02T10:12:48.319625Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='', vocab_size=109, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t109: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["## Wrap the tokenizer with existing hugging face tokenizer\n","\n","transformer_tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_object=tokenizer #tokenizer_file=\"tokenizer.json\"\n",")\n","transformer_tokenizer"]},{"cell_type":"code","execution_count":39,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T10:12:52.029941Z","iopub.status.busy":"2024-04-02T10:12:52.029541Z","iopub.status.idle":"2024-04-02T10:12:52.041546Z","shell.execute_reply":"2024-04-02T10:12:52.039867Z","shell.execute_reply.started":"2024-04-02T10:12:52.029911Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["['тЦБ',\n"," 'ржЧ',\n"," 'рзЗрж▓',\n"," 'тЦБржЗржЙржирж┐ржЯрзЗржХ',\n"," 'тЦБржк',\n"," 'рзНрж░',\n"," 'ржб',\n"," 'рж╛',\n"," 'ржХрзНржЯ',\n"," 'рж╕',\n"," 'тЦБ',\n"," '<mask>',\n"," 'тЦБ(',\n"," 'ржм',\n"," 'рж┐',\n"," 'ржб',\n"," 'рж┐',\n"," ')']"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["## Test it out\n","\n","test_text = \"ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ <mask> (ржмрж┐ржбрж┐)\"\n","# test_text = normalize(\"ржЧрзЗрж▓ ржЗржЙржирж┐ржЯрзЗржХ ржкрзНрж░ржбрж╛ржХрзНржЯрж╕ (ржмрж┐ржбрж┐)\")\n","transformer_tokenizer.tokenize(test_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:19:56.237714Z","iopub.status.idle":"2024-04-02T09:19:56.238542Z","shell.execute_reply":"2024-04-02T09:19:56.238251Z","shell.execute_reply.started":"2024-04-02T09:19:56.238225Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","hf_token = user_secrets.get_secret(\"hf_token\")\n","\n","\n","from huggingface_hub import login\n","login(token= hf_token, add_to_git_credential=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:19:56.240420Z","iopub.status.idle":"2024-04-02T09:19:56.241446Z","shell.execute_reply":"2024-04-02T09:19:56.240901Z","shell.execute_reply.started":"2024-04-02T09:19:56.240875Z"},"trusted":true},"outputs":[],"source":["## Push the tokenizer to hugging face hub\n","\n","transformer_tokenizer.push_to_hub(\"Virus-Proton/CustomTokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["### \n","\n","def create_tokenizer():\n","    tokenizer = SentencePieceBPETokenizer()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4514808,"sourceId":7727419,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
