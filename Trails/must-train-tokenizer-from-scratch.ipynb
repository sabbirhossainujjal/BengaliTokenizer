{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:13.633836Z","iopub.status.busy":"2024-04-02T09:18:13.633439Z","iopub.status.idle":"2024-04-02T09:18:30.819396Z","shell.execute_reply":"2024-04-02T09:18:30.817830Z","shell.execute_reply.started":"2024-04-02T09:18:13.633808Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m  DEPRECATION: emoji is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  DEPRECATION: ftfy is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  DEPRECATION: normalizer is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q git+https://github.com/csebuetnlp/normalizer"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:30.822739Z","iopub.status.busy":"2024-04-02T09:18:30.822309Z","iopub.status.idle":"2024-04-02T09:18:31.437791Z","shell.execute_reply":"2024-04-02T09:18:31.436548Z","shell.execute_reply.started":"2024-04-02T09:18:30.822704Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'kaggle_secrets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      2\u001b[0m user_secrets \u001b[38;5;241m=\u001b[39m UserSecretsClient()\n\u001b[1;32m      3\u001b[0m hf_token \u001b[38;5;241m=\u001b[39m user_secrets\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kaggle_secrets'"]}],"source":["# from kaggle_secrets import UserSecretsClient\n","# user_secrets = UserSecretsClient()\n","# hf_token = user_secrets.get_secret(\"hf_token\")\n","\n","\n","from huggingface_hub import login\n","login(token= hf_token, add_to_git_credential=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:31.439376Z","iopub.status.busy":"2024-04-02T09:18:31.439072Z","iopub.status.idle":"2024-04-02T09:18:31.444353Z","shell.execute_reply":"2024-04-02T09:18:31.443460Z","shell.execute_reply.started":"2024-04-02T09:18:31.439350Z"},"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","from normalizer import normalize"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:31.447458Z","iopub.status.busy":"2024-04-02T09:18:31.446742Z","iopub.status.idle":"2024-04-02T09:18:34.779271Z","shell.execute_reply":"2024-04-02T09:18:34.778172Z","shell.execute_reply.started":"2024-04-02T09:18:31.447421Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'সাভারের কবিরপুর বাণিজ্যিক এলাকার নিজস্ব ফ্যাক্টরিতে হয়ে গেল ইউনিটেক প্রডাক্টস (বিডি) লিঃ এর দিনব্যাপী সেলস কনফারেন্স। কনফারেন্সটি অনুষ্ঠিত হয় শনিবার, ১১ই মার্চ, সারা দেশ থেকে প্রায় ৩১০ জন ডিলার এই কনফারেন্সটিতে অংশগ্রহণ করেন এবং ইউনিটেকর ফ্যাক্টরি ঘুরে দেখার সুযোগ পান।এসময় ইউনিটেকর ব্যবস্থাপনা পরিচালক, ইঞ্জি: আনিস আহমেদ ডিলারদের উদ্দেশে বক্তব্য রাখেন এবং ইউনিটেক কে বাংলাদেশের সেরা ইলেক্ট্রনিক ব্র্যান্ড হিসেবে গড়ে তোলার আশ্বাস ব্যক্ত করেন। ইঞ্জি: আনিস আহমেদ, জাতীয় এবং বিভাগীয় পর্যায়ে সেরা ডিলারদের হাতে সার্টিফিকেট, ক্রেস্ট ও প্রাইজ বন্ড তুলে দেন। বিজয়ীরা হলেন রুপালি ইলেক্ট্রনিক্স (জাতীয় সেরা), লিয়া এন্টারপ্রাইজ (দ্বিতীয় জাতীয় সেরা), মদিনা ইলেক্ট্রনিক্স (তৃতীয় জাতীয় সেরা), সুন্দরবন ইলেক্ট্রনিক্স (ঢাকা বিভাগ), আলাউদ্দিন ইলেক্ট্রনিক্স (চিটাগং বিভাগ), শান্ত ইলেক্ট্রনিক্স (রাজশাহী রংপুর বিভাগ), নুরুল এন্টারপ্রাইজ (খুলনা বিভাগ), ফেয়ার ভিউ ইলেক্ট্রনিক্স (বরিশাল বিভাগ) ও ফিডব্যাক কমিউনিকেসন (সিলেট বিভাগ)। কনফারেন্সটিতে আরও বক্তব্য রাখেন ইউনিটেকের বিজনেস ডেভেলপমেন্ট অ্যান্ড প্ল্যানিং ম্যানেজার এ কে এম হামিদুর রহমান, সেলস অ্যান্ড ডেভেলপমেন্ট ম্যানেজার এ টি এম আখতার হোসেন ও ইউনিটেকের সেলস ম্যানেজার ডিলার চ্যানেল জাকির হোসেন। -প্রেস বিজ্ঞপ্তি'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"/kaggle/input/potrika-bangla-newspaper-datasets/Potrika-Newspaper Datasets in the Bangla Language/BalancedDataset/Economy_40k.csv\")\n","data['article'][0]"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.781154Z","iopub.status.busy":"2024-04-02T09:18:34.780844Z","iopub.status.idle":"2024-04-02T09:18:34.788699Z","shell.execute_reply":"2024-04-02T09:18:34.787513Z","shell.execute_reply.started":"2024-04-02T09:18:34.781130Z"},"trusted":true},"outputs":[],"source":["text = ['সাভারের কবিরপুর বাণিজ্যিক এলাকার নিজস্ব ফ্যাক্টরিতে হয়ে গেল ইউনিটেক প্রডাক্টস (বিডি) লিঃ এর দিনব্যাপী সেলস কনফারেন্স। কনফারেন্সটি অনুষ্ঠিত হয় শনিবার, ১১ই মার্চ, সারা দেশ থেকে প্রায় ৩১০ জন ডিলার এই কনফারেন্সটিতে অংশগ্রহণ করেন এবং ইউনিটেকর ফ্যাক্টরি ঘুরে দেখার সুযোগ পান।এসময় ইউনিটেকর ব্যবস্থাপনা পরিচালক, ইঞ্জি: আনিস আহমেদ ডিলারদের উদ্দেশে বক্তব্য রাখেন এবং ইউনিটেক কে বাংলাদেশের সেরা ইলেক্ট্রনিক ব্র্যান্ড হিসেবে গড়ে তোলার আশ্বাস ব্যক্ত করেন। ইঞ্জি: আনিস আহমেদ, জাতীয় এবং বিভাগীয় পর্যায়ে সেরা ডিলারদের হাতে সার্টিফিকেট, ক্রেস্ট ও প্রাইজ বন্ড তুলে দেন। বিজয়ীরা হলেন রুপালি ইলেক্ট্রনিক্স (জাতীয় সেরা), লিয়া এন্টারপ্রাইজ (দ্বিতীয় জাতীয় সেরা), মদিনা ইলেক্ট্রনিক্স (তৃতীয় জাতীয় সেরা), সুন্দরবন ইলেক্ট্রনিক্স (ঢাকা বিভাগ), আলাউদ্দিন ইলেক্ট্রনিক্স (চিটাগং বিভাগ), শান্ত ইলেক্ট্রনিক্স (রাজশাহী রংপুর বিভাগ), নুরুল এন্টারপ্রাইজ (খুলনা বিভাগ), ফেয়ার ভিউ ইলেক্ট্রনিক্স (বরিশাল বিভাগ) ও ফিডব্যাক কমিউনিকেসন (সিলেট বিভাগ)। কনফারেন্সটিতে আরও বক্তব্য রাখেন ইউনিটেকের বিজনেস ডেভেলপমেন্ট অ্যান্ড প্ল্যানিং ম্যানেজার এ কে এম হামিদুর রহমান, সেলস অ্যান্ড ডেভেলপমেন্ট ম্যানেজার এ টি এম আখতার হোসেন ও ইউনিটেকের সেলস ম্যানেজার ডিলার চ্যানেল জাকির হোসেন। -প্রেস বিজ্ঞপ্তি']"]},{"cell_type":"code","execution_count":41,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T09:26:25.452757Z","iopub.status.busy":"2024-04-02T09:26:25.452298Z","iopub.status.idle":"2024-04-02T09:26:25.465230Z","shell.execute_reply":"2024-04-02T09:26:25.463939Z","shell.execute_reply.started":"2024-04-02T09:26:25.452723Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on class SentencePieceBPETokenizer in module tokenizers.implementations.sentencepiece_bpe:\n","\n","class SentencePieceBPETokenizer(tokenizers.implementations.base_tokenizer.BaseTokenizer)\n"," |  SentencePieceBPETokenizer(vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = '▁', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |  \n"," |  SentencePiece BPE Tokenizer\n"," |  \n"," |  Represents the BPE algorithm, with the pretokenization used by SentencePiece\n"," |  \n"," |  Method resolution order:\n"," |      SentencePieceBPETokenizer\n"," |      tokenizers.implementations.base_tokenizer.BaseTokenizer\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = '▁', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  train(self, files: Union[str, List[str]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True)\n"," |      Train the model using the given files\n"," |  \n"," |  train_from_iterator(self, iterator: Union[Iterator[str], Iterator[Iterator[str]]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True, length: Optional[int] = None)\n"," |      Train the model using the given iterator\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods defined here:\n"," |  \n"," |  from_file(vocab_filename: str, merges_filename: str, **kwargs)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __slotnames__ = []\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  add_special_tokens(self, special_tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given special tokens to the vocabulary, and treat them as special tokens.\n"," |      \n"," |      The special tokens will never be processed by the model, and will be\n"," |      removed while decoding.\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of special tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  add_tokens(self, tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given tokens to the vocabulary\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the given list of ids to a string sequence\n"," |      \n"," |      Args:\n"," |          ids: List[unsigned int]:\n"," |              A list of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output string\n"," |      \n"," |      Returns:\n"," |          The decoded string\n"," |  \n"," |  decode_batch(self, sequences: List[List[int]], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the list of sequences to a list of string sequences\n"," |      \n"," |      Args:\n"," |          sequences: List[List[unsigned int]]:\n"," |              A list of sequence of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output strings\n"," |      \n"," |      Returns:\n"," |          A list of decoded strings\n"," |  \n"," |  enable_padding(self, direction: Optional[str] = 'right', pad_to_multiple_of: Optional[int] = None, pad_id: Optional[int] = 0, pad_type_id: Optional[int] = 0, pad_token: Optional[str] = '[PAD]', length: Optional[int] = None)\n"," |      Change the padding strategy\n"," |      \n"," |      Args:\n"," |          direction: (`optional`) str:\n"," |              Can be one of: `right` or `left`\n"," |      \n"," |          pad_to_multiple_of: (`optional`) unsigned int:\n"," |              If specified, the padding length should always snap to the next multiple of\n"," |              the given value. For example if we were going to pad with a length of 250 but\n"," |              `pad_to_multiple_of=8` then we will pad to 256.\n"," |      \n"," |          pad_id: (`optional`) unsigned int:\n"," |              The indice to be used when padding\n"," |      \n"," |          pad_type_id: (`optional`) unsigned int:\n"," |              The type indice to be used when padding\n"," |      \n"," |          pad_token: (`optional`) str:\n"," |              The pad token to be used when padding\n"," |      \n"," |          length: (`optional`) unsigned int:\n"," |              If specified, the length at which to pad. If not specified\n"," |              we pad using the size of the longest sequence in a batch\n"," |  \n"," |  enable_truncation(self, max_length: int, stride: Optional[int] = 0, strategy: Optional[str] = 'longest_first')\n"," |      Change the truncation options\n"," |      \n"," |      Args:\n"," |          max_length: unsigned int:\n"," |              The maximum length at which to truncate\n"," |      \n"," |          stride: (`optional`) unsigned int:\n"," |              The length of the previous first sequence to be included\n"," |              in the overflowing sequence\n"," |      \n"," |          strategy: (`optional`) str:\n"," |              Can be one of `longest_first`, `only_first` or `only_second`\n"," |  \n"," |  encode(self, sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Encode the given sequence and pair. This method can process raw text sequences as well\n"," |      as already pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          sequence: InputSequence:\n"," |              The sequence we want to encode. This sequence can be either raw text or\n"," |              pre-tokenized, according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          An Encoding\n"," |  \n"," |  encode_batch(self, inputs: List[Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]], is_pretokenized: bool = False, add_special_tokens: bool = True) -> List[tokenizers.Encoding]\n"," |      Encode the given inputs. This method accept both raw text sequences as well as already\n"," |      pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          inputs: List[EncodeInput]:\n"," |              A list of single sequences or pair sequences to encode. Each `EncodeInput` is\n"," |              expected to be of the following form:\n"," |                  `Union[InputSequence, Tuple[InputSequence, InputSequence]]`\n"," |      \n"," |              Each `InputSequence` can either be raw text or pre-tokenized,\n"," |              according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          A list of Encoding\n"," |  \n"," |  get_added_tokens_decoder(self) -> Dict[int, tokenizers.AddedToken]\n"," |      Returns the added reverse vocabulary\n"," |      \n"," |      Returns:\n"," |          The added vocabulary mapping ints to AddedTokens\n"," |  \n"," |  get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]\n"," |      Returns the vocabulary\n"," |      \n"," |      Args:\n"," |          with_added_tokens: boolean:\n"," |              Whether to include the added tokens in the vocabulary\n"," |      \n"," |      Returns:\n"," |          The vocabulary\n"," |  \n"," |  get_vocab_size(self, with_added_tokens: bool = True) -> int\n"," |      Return the size of vocabulary, with or without added tokens.\n"," |      \n"," |      Args:\n"," |          with_added_tokens: (`optional`) bool:\n"," |              Whether to count in added special tokens or not\n"," |      \n"," |      Returns:\n"," |          Size of vocabulary\n"," |  \n"," |  id_to_token(self, id: int) -> Optional[str]\n"," |      Convert the given token id to its corresponding string\n"," |      \n"," |      Args:\n"," |          token: id:\n"," |              The token id to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding string if it exists, None otherwise\n"," |  \n"," |  no_padding(self)\n"," |      Disable padding\n"," |  \n"," |  no_truncation(self)\n"," |      Disable truncation\n"," |  \n"," |  normalize(self, sequence: str) -> str\n"," |      Normalize the given sequence\n"," |      \n"," |      Args:\n"," |          sequence: str:\n"," |              The sequence to normalize\n"," |      \n"," |      Returns:\n"," |          The normalized string\n"," |  \n"," |  num_special_tokens_to_add(self, is_pair: bool) -> int\n"," |      Return the number of special tokens that would be added for single/pair sentences.\n"," |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n"," |      :return:\n"," |  \n"," |  post_process(self, encoding: tokenizers.Encoding, pair: Optional[tokenizers.Encoding] = None, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Apply all the post-processing steps to the given encodings.\n"," |      \n"," |      The various steps are:\n"," |          1. Truncate according to global params (provided to `enable_truncation`)\n"," |          2. Apply the PostProcessor\n"," |          3. Pad according to global params. (provided to `enable_padding`)\n"," |      \n"," |      Args:\n"," |          encoding: Encoding:\n"," |              The main Encoding to post process\n"," |      \n"," |          pair: Optional[Encoding]:\n"," |              An optional pair Encoding\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add special tokens\n"," |      \n"," |      Returns:\n"," |          The resulting Encoding\n"," |  \n"," |  save(self, path: str, pretty: bool = True)\n"," |      Save the current Tokenizer at the given path\n"," |      \n"," |      Args:\n"," |          path: str:\n"," |              A path to the destination Tokenizer file\n"," |  \n"," |  save_model(self, directory: str, prefix: Optional[str] = None)\n"," |      Save the current model to the given directory\n"," |      \n"," |      Args:\n"," |          directory: str:\n"," |              A path to the destination directory\n"," |      \n"," |          prefix: (Optional) str:\n"," |              An optional prefix, used to prefix each file name\n"," |  \n"," |  to_str(self, pretty: bool = False)\n"," |      Get a serialized JSON version of the Tokenizer as a str\n"," |      \n"," |      Args:\n"," |          pretty: bool:\n"," |              Whether the JSON string should be prettified\n"," |      \n"," |      Returns:\n"," |          str\n"," |  \n"," |  token_to_id(self, token: str) -> Optional[int]\n"," |      Convert the given token to its corresponding id\n"," |      \n"," |      Args:\n"," |          token: str:\n"," |              The token to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding id if it exists, None otherwise\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  padding\n"," |      Get the current padding parameters\n"," |      \n"," |      Returns:\n"," |          None if padding is disabled, a dict with the currently set parameters\n"," |          if the padding is enabled.\n"," |  \n"," |  truncation\n"," |      Get the current truncation parameters\n"," |      \n"," |      Returns:\n"," |          None if truncation is disabled, a dict with the current truncation parameters if\n"," |          truncation is enabled\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  decoder\n"," |  \n"," |  model\n"," |  \n"," |  normalizer\n"," |  \n"," |  post_processor\n"," |  \n"," |  pre_tokenizer\n","\n"]}],"source":["# help(SentencePieceBPETokenizer)\n","# self, files: Union[str, List[str]], \n","# vocab_size: int = 30000, min_frequency: int = 2, \n","# special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], \n","# limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from tokenizers import SentencePieceBPETokenizer\n","\n","tokenizer = SentencePieceBPETokenizer()\n","\n","# tokenizer_arguments = dict(\n","    \n","# )"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import json\n","with open('/home/virus_proton/Projects/P_Projects/LLM_Mastery/Tokenizer_train/vocab.json', 'r') as f:\n","    vocab = json.load(f)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["{'<pad>': 0,\n"," '<s>': 1,\n"," '</s>': 2,\n"," '<unk>': 3,\n"," '<mask>': 4,\n"," '\\n': 5,\n"," ',': 6,\n"," '-': 7,\n"," '.': 8,\n"," '4': 9,\n"," '5': 10,\n"," '6': 11,\n"," '7': 12,\n"," ':': 13,\n"," 'B': 14,\n"," 'E': 15,\n"," 'F': 16,\n"," 'N': 17,\n"," 'P': 18,\n"," 'V': 19,\n"," 'a': 20,\n"," 'b': 21,\n"," 'c': 22,\n"," 'e': 23,\n"," 'g': 24,\n"," 'h': 25,\n"," 'i': 26,\n"," 'k': 27,\n"," 'l': 28,\n"," 'm': 29,\n"," 'n': 30,\n"," 'o': 31,\n"," 's': 32,\n"," 't': 33,\n"," 'w': 34,\n"," 'y': 35,\n"," '|': 36,\n"," '।': 37,\n"," 'ঁ': 38,\n"," 'ং': 39,\n"," 'ঃ': 40,\n"," 'অ': 41,\n"," 'আ': 42,\n"," 'ই': 43,\n"," 'উ': 44,\n"," 'এ': 45,\n"," 'ও': 46,\n"," 'ক': 47,\n"," 'খ': 48,\n"," 'গ': 49,\n"," 'ঘ': 50,\n"," 'ঙ': 51,\n"," 'চ': 52,\n"," 'ছ': 53,\n"," 'জ': 54,\n"," 'ঝ': 55,\n"," 'ঞ': 56,\n"," 'ট': 57,\n"," 'ঠ': 58,\n"," 'ড': 59,\n"," 'ণ': 60,\n"," 'ত': 61,\n"," 'থ': 62,\n"," 'দ': 63,\n"," 'ধ': 64,\n"," 'ন': 65,\n"," 'প': 66,\n"," 'ফ': 67,\n"," 'ব': 68,\n"," 'ভ': 69,\n"," 'ম': 70,\n"," 'য': 71,\n"," 'র': 72,\n"," 'ল': 73,\n"," 'শ': 74,\n"," 'ষ': 75,\n"," 'স': 76,\n"," 'হ': 77,\n"," '়': 78,\n"," 'া': 79,\n"," 'ি': 80,\n"," 'ী': 81,\n"," 'ু': 82,\n"," 'ূ': 83,\n"," 'ৃ': 84,\n"," 'ে': 85,\n"," 'ৈ': 86,\n"," 'ো': 87,\n"," '্': 88,\n"," 'ৎ': 89,\n"," '০': 90,\n"," '১': 91,\n"," '২': 92,\n"," '৪': 93,\n"," '৫': 94,\n"," '\\u200d': 95,\n"," '–': 96,\n"," '—': 97,\n"," '▁': 98,\n"," '🌷': 99,\n"," '🌹': 100,\n"," '💗': 101,\n"," '💙': 102,\n"," '💚': 103,\n"," '🥀': 104,\n"," 'য়': 105,\n"," '▁ব': 106,\n"," '▁স': 107,\n"," 'ন্': 108,\n"," 'ার': 109,\n"," '▁ক': 110,\n"," 'ের': 111,\n"," '▁আ': 112,\n"," 'ধু': 113,\n"," 'ন্ধু': 114,\n"," '▁বন্ধু': 115,\n"," '▁এ': 116,\n"," '্য': 117,\n"," '▁ন': 118,\n"," '▁প': 119,\n"," '▁ত': 120,\n"," 'কে': 121,\n"," '▁হ': 122,\n"," '▁য': 123,\n"," '্র': 124,\n"," '▁ম': 125,\n"," '্ব': 126,\n"," 'ান': 127,\n"," 'য়ে': 128,\n"," '▁কর': 129,\n"," 'দের': 130,\n"," 'বে': 131,\n"," 'পন': 132,\n"," '▁জ': 133,\n"," '▁দ': 134,\n"," '▁আপন': 135,\n"," 'তে': 136,\n"," '▁অ': 137,\n"," '▁থ': 138,\n"," '।\\n': 139,\n"," '▁আম': 140,\n"," '্যা': 141,\n"," 'ত্ব': 142,\n"," '▁এক': 143,\n"," '▁ভ': 144,\n"," '▁না': 145,\n"," '▁থা': 146,\n"," '\\n\\n': 147,\n"," '▁নি': 148,\n"," 'টি': 149,\n"," 'খন': 150,\n"," 'ছে': 151,\n"," 'ায়': 152,\n"," 'তু': 153,\n"," 'াস': 154,\n"," '▁সম': 155,\n"," '▁শ': 156,\n"," '▁প্র': 157,\n"," 'বং': 158,\n"," 'রা': 159,\n"," 'লে': 160,\n"," '▁চ': 161,\n"," '▁সা': 162,\n"," '▁বন্ধুত্ব': 163,\n"," '▁–': 164,\n"," '▁এবং': 165,\n"," '▁করে': 166,\n"," 'থে': 167,\n"," '্ট': 168,\n"," '▁সে': 169,\n"," '▁কি': 170,\n"," 'াদের': 171,\n"," 'িয়': 172,\n"," 'নে': 173,\n"," 'িক': 174,\n"," 'ারা': 175,\n"," 'ড়': 176,\n"," 'র্': 177,\n"," '▁যে': 178,\n"," 'ত্য': 179,\n"," 'াল': 180,\n"," '▁উ': 181,\n"," '▁বি': 182,\n"," '▁আপনার': 183,\n"," '▁সব': 184,\n"," '▁তো': 185,\n"," '▁থাক': 186,\n"," 'াই': 187,\n"," 'াকে': 188,\n"," 'জন': 189,\n"," 'তি': 190,\n"," 'লি': 191,\n"," '▁স্ট': 192,\n"," '▁আপনি': 193,\n"," '▁সাথে': 194,\n"," 'তো': 195,\n"," '🥀💗': 196,\n"," 'টাস': 197,\n"," 'িব': 198,\n"," 'ন্য': 199,\n"," '▁পা': 200,\n"," '্যাটাস': 201,\n"," '▁নিয়ে': 202,\n"," '▁স্ট্যাটাস': 203,\n"," 'ক্': 204,\n"," 'িন': 205,\n"," '▁—': 206,\n"," 'য়।': 207,\n"," '▁সু': 208,\n"," '▁কখন': 209,\n"," '▁বন্ধুদের': 210,\n"," '▁পার': 211,\n"," 'তুন': 212,\n"," 'ওয়': 213,\n"," 'মি': 214,\n"," 'লা': 215,\n"," 'েয়ে': 216,\n"," '▁খ': 217,\n"," '▁কা': 218,\n"," '▁দে': 219,\n"," '▁থাকে': 220,\n"," '▁সময়': 221,\n"," 'িকার': 222,\n"," '▁তোম': 223,\n"," 'মন': 224,\n"," 'রে': 225,\n"," 'ীব': 226,\n"," 'ুন': 227,\n"," '▁গ': 228,\n"," '▁যা': 229,\n"," 'ানা': 230,\n"," '▁প্রিয়': 231,\n"," 'ত্যিকার': 232,\n"," 'বন্ধু': 233,\n"," '▁নতুন': 234,\n"," '্যাপ': 235,\n"," 'ত্যিকারের': 236,\n"," 'জানা': 237,\n"," 'প্র': 238,\n"," 'শন': 239,\n"," 'াত': 240,\n"," 'িয়ে': 241,\n"," '▁ফ': 242,\n"," 'ন্তু': 243,\n"," '▁কার': 244,\n"," '▁ক্যাপ': 245,\n"," '▁অজানা': 246,\n"," '▁একজন': 247,\n"," 'ওয়া': 248,\n"," '▁ক্যাপশন': 249,\n"," 'লো': 250,\n"," 'েন': 251,\n"," 'েকে': 252,\n"," 'োন': 253,\n"," '্প': 254,\n"," '▁আস': 255,\n"," '▁জন্য': 256,\n"," '▁জীব': 257,\n"," '▁আমি': 258,\n"," '▁সেই': 259,\n"," '▁কিন্তু': 260,\n"," 'ছু': 261,\n"," 'শে': 262,\n"," '▁বা': 263,\n"," '▁এমন': 264,\n"," '▁তাদের': 265,\n"," '▁তারা': 266,\n"," '▁একটি': 267,\n"," '▁ভাল': 268,\n"," '▁কিছু': 269,\n"," '▁কখনো': 270,\n"," 'কা': 271,\n"," 'কের': 272,\n"," 'ছি': 273,\n"," 'নের': 274,\n"," 'ুষ': 275,\n"," '্ম': 276,\n"," '▁তু': 277,\n"," '▁হয়': 278,\n"," '▁মতো': 279,\n"," 'ানুষ': 280,\n"," '▁আমরা': 281,\n"," 'ক্তি': 282,\n"," '▁তোমার': 283,\n"," '্পর্': 284,\n"," 'থিব': 285,\n"," 'দি': 286,\n"," 'ভা': 287,\n"," 'রি': 288,\n"," 'িত': 289,\n"," 'ৃথিব': 290,\n"," 'েস': 291,\n"," '🌹🌹': 292,\n"," '▁হল': 293,\n"," '▁মানুষ': 294,\n"," '▁আমাদের': 295,\n"," '▁না,': 296,\n"," '▁না।': 297,\n"," '\\n\\n🥀💗': 298,\n"," '\\n\\n🌹🌹': 299,\n"," '▁ভালো': 300,\n"," 'ৃথিবী': 301,\n"," '\\nএ': 302,\n"," 'কাল': 303,\n"," 'খে': 304,\n"," 'গু': 305,\n"," 'না': 306,\n"," 'শি': 307,\n"," 'ৃদ': 308,\n"," '▁রা': 309,\n"," '▁বে': 310,\n"," 'ন্দ': 311,\n"," 'েরা': 312,\n"," '▁আর': 313,\n"," '▁আছে': 314,\n"," '▁যায়': 315,\n"," '▁যারা': 316,\n"," 'ানে': 317,\n"," '▁দু': 318,\n"," '▁অনে': 319,\n"," '▁থেকে': 320,\n"," '▁আমার': 321,\n"," '▁বন্ধুত্বের': 322,\n"," '▁কারণ': 323,\n"," '\\nক': 324,\n"," 'চেয়ে': 325,\n"," 'বি': 326,\n"," 'বু': 327,\n"," 'শ্ব': 328,\n"," 'ষ্': 329,\n"," 'সা': 330,\n"," 'হা': 331,\n"," 'িস': 332,\n"," 'ুলে': 333,\n"," '্ত': 334,\n"," '▁ট': 335,\n"," '▁ড': 336,\n"," '▁র': 337,\n"," '▁🥀💗': 338,\n"," '🌷🌷': 339,\n"," '▁ব্য': 340,\n"," '▁সত্যিকারের': 341,\n"," 'ন্ত': 342,\n"," '▁কে': 343,\n"," '▁নে': 344,\n"," '▁হৃদ': 345,\n"," '▁একসা': 346,\n"," '▁সম্পর্': 347,\n"," '▁শু': 348,\n"," '▁সবচেয়ে': 349,\n"," '▁পাশে': 350,\n"," 'ভাবে': 351,\n"," '▁একসাথে': 352,\n"," 'দে': 353,\n"," 'ধ্য': 354,\n"," 'পদে': 355,\n"," 'সত্যিকারের': 356,\n"," 'ূল': 357,\n"," '▁ছ': 358,\n"," '▁আকা': 359,\n"," '▁বন্ধুরা': 360,\n"," '▁পর': 361,\n"," '▁পৃথিবী': 362,\n"," '▁তা': 363,\n"," '▁তার': 364,\n"," '▁হয়।': 365,\n"," '▁করি': 366,\n"," '▁করতে': 367,\n"," 'বে।': 368,\n"," 'বেন': 369,\n"," '▁আপনাকে': 370,\n"," '।\\nআ': 371,\n"," '▁ভা': 372,\n"," '\\n\\n🌷🌷': 373,\n"," '▁বিশ্ব': 374,\n"," '▁সবার': 375,\n"," '▁কাছে': 376,\n"," 'বন্ধুত্ব': 377,\n"," '\\n\\n🌹🌹য': 378,\n"," '▁অনেক': 379,\n"," 'খা': 380,\n"," 'খের': 381,\n"," 'খানে': 382,\n"," 'ঠিন': 383,\n"," 'দা': 384,\n"," 'নি': 385,\n"," 'মাত': 386,\n"," 'লার': 387,\n"," 'সম': 388,\n"," 'ির': 389,\n"," 'ে,': 390,\n"," '▁বড়': 391,\n"," 'ন্ধ': 392,\n"," '▁কোন': 393,\n"," '▁কঠিন': 394,\n"," '▁বন্ধুর': 395,\n"," '▁এই': 396,\n"," '▁যখন': 397,\n"," '▁যদি': 398,\n"," '▁মা': 399,\n"," '▁মি': 400,\n"," '▁মূল': 401,\n"," '▁করুন': 402,\n"," '▁জিন': 403,\n"," '▁দূ': 404,\n"," '।\\nপ্র': 405,\n"," '▁নিজ': 406,\n"," '▁প্রতি': 407,\n"," '▁চাই': 408,\n"," '▁চেয়ে': 409,\n"," '▁করে।': 410,\n"," 'ড়ে': 411,\n"," '▁পাওয়া': 412,\n"," '▁সুন্দ': 413,\n"," '▁পারেন': 414,\n"," '▁খু': 415,\n"," '▁জীবনে': 416,\n"," 'েসবু': 417,\n"," '▁রাখ': 418,\n"," '▁বেশি': 419,\n"," 'ষ্টি': 420,\n"," '▁সম্পর্কে': 421,\n"," '▁শুধু': 422,\n"," '▁ভাগ': 423,\n"," 'মাত্র': 424,\n"," '▁মূল্য': 425,\n"," '▁জিনিস': 426,\n"," '▁সুন্দর': 427,\n"," '\\nপ': 428,\n"," 'gl': 429,\n"," 'ngl': 430,\n"," 'এক': 431,\n"," 'চ্': 432,\n"," 'জে': 433,\n"," 'জ্': 434,\n"," 'জার': 435,\n"," 'ত্ম': 436,\n"," 'থা': 437,\n"," 'বার': 438,\n"," 'বান': 439,\n"," 'য্য': 440,\n"," 'ল,': 441,\n"," 'ুক': 442,\n"," 'ৃত': 443,\n"," 'োগ': 444,\n"," 'োলা': 445,\n"," '💙💚': 446,\n"," 'য়ার': 447,\n"," '▁সেরা': 448,\n"," '▁আত্ম': 449,\n"," '▁নয়।': 450,\n"," '▁তখন': 451,\n"," '▁হয়ে': 452,\n"," '▁হতে': 453,\n"," '▁যত': 454,\n"," '▁মু': 455,\n"," '▁মধ্য': 456,\n"," '▁অর্': 457,\n"," '।\\nয': 458,\n"," '▁আমাকে': 459,\n"," '▁ভুলে': 460,\n"," '\\n\\n💙💚': 461,\n"," 'খনও': 462,\n"," '▁চির': 463,\n"," '▁সাহা': 464,\n"," '▁উক্তি': 465,\n"," '▁বিপদে': 466,\n"," '▁থাকবে': 467,\n"," 'ক্ষ': 468,\n"," '▁পারে': 469,\n"," '▁থাকে।': 470,\n"," '▁হয়তো': 471,\n"," 'গুলি': 472,\n"," '▁🥀💗—': 473,\n"," '▁হৃদয়': 474,\n"," '▁আকাশ': 475,\n"," '\\n\\n🌷🌷সত্যিকারের': 476,\n"," '▁মধ্যে': 477,\n"," '▁চিরকাল': 478,\n"," '▁সাহায্য': 479,\n"," 'ঁজে': 480,\n"," 'ই,': 481,\n"," 'কার': 482,\n"," 'কৃত': 483,\n"," 'টা': 484,\n"," 'টাই': 485,\n"," 'ণের': 486,\n"," 'পর': 487,\n"," 'ব্': 488,\n"," 'বাস': 489,\n"," 'ভব': 490,\n"," 'মে': 491,\n"," 'যোগ': 492,\n"," 'রু': 493,\n"," 'র্ব': 494,\n"," 'রান': 495,\n"," 'লের': 496,\n"," 'শা': 497,\n"," 'স্ত': 498,\n"," 'হার': 499,\n"," 'িই': 500,\n"," 'িতে': 501,\n"," 'ৃষ্টি': 502,\n"," 'োর': 503,\n"," '্থ': 504,\n"," '্ষ': 505,\n"," '▁B': 506,\n"," '▁|': 507,\n"," '▁ছে': 508,\n"," '▁লো': 509,\n"," 'য়,': 510,\n"," '▁বো': 511,\n"," '▁বলে': 512,\n"," '▁বন্ধ': 513,\n"," '▁বৃষ্টি': 514,\n"," '▁সহ': 515,\n"," '▁স্ব': 516,\n"," '▁সর্ব': 517,\n"," 'ন্ড': 518,\n"," '▁আজ': 519,\n"," '▁আশা': 520,\n"," '▁এটি': 521,\n"," '্যন্ত': 522,\n"," '▁হাস': 523,\n"," '▁হাত': 524,\n"," '্রহ': 525,\n"," '্রু': 526,\n"," '▁মে': 527,\n"," '▁করা': 528,\n"," '▁জি': 529,\n"," '▁দিয়ে': 530,\n"," '।\\nক': 531,\n"," '।\\nএক': 532,\n"," '▁একই': 533,\n"," '▁ভিত': 534,\n"," '▁শি': 535,\n"," '▁শে': 536,\n"," '▁চো': 537,\n"," '▁করে,': 538,\n"," 'িয়া': 539,\n"," '▁যেতে': 540,\n"," 'ত্যে': 541,\n"," 'ত্যিই': 542,\n"," '▁উপ': 543,\n"," '▁সবসম': 544,\n"," '▁থাকব': 545,\n"," '▁স্ট্যাটাস\\n': 546,\n"," '▁দেখা': 547,\n"," '▁তোমাকে': 548,\n"," '▁গু': 549,\n"," '▁গে': 550,\n"," '▁গোলা': 551,\n"," '▁ফ্র': 552,\n"," '▁ফেসবু': 553,\n"," '▁আসে': 554,\n"," '▁জীবনের': 555,\n"," 'ছি।': 556,\n"," '▁তুমি': 557,\n"," '▁হয়,': 558,\n"," '\\n\\n🥀💗বন্ধুত্ব': 559,\n"," '▁ব্যক্তি': 560,\n"," '▁কেউ': 561,\n"," '।\\nআমি': 562,\n"," '▁মিল': 563,\n"," '।\\nপ্রিয়': 564,\n"," '▁খুঁজে': 565,\n"," '▁মূল্যবান': 566,\n"," 'চ্ছে': 567,\n"," '▁অর্থ': 568,\n"," '▁বন্ধন': 569,\n"," '▁সবসময়': 570,\n"," '▁গোলাপ': 571,\n"," '\\nন': 572,\n"," 'angl': 573,\n"," 'গ্রহ': 574,\n"," 'ছন্দ': 575,\n"," 'জি': 576,\n"," 'জীব': 577,\n"," 'জিয়ে': 578,\n"," 'তই': 579,\n"," 'তার': 580,\n"," 'থায়': 581,\n"," 'দিন': 582,\n"," 'নু': 583,\n"," 'ভি': 584,\n"," 'মাদের': 585,\n"," 'রল': 586,\n"," 'রের': 587,\n"," 'রত্ব': 588,\n"," 'সন': 589,\n"," 'াড়': 590,\n"," 'ীর': 591,\n"," 'ীয়': 592,\n"," 'ূর্': 593,\n"," 'ৃতি': 594,\n"," 'েম': 595,\n"," 'ৈরি': 596,\n"," 'োট': 597,\n"," '্ক': 598,\n"," '▁ও': 599,\n"," '▁ধ': 600,\n"," '▁ল': 601,\n"," '▁২': 602,\n"," '▁লি': 603,\n"," '▁লা': 604,\n"," '▁বল': 605,\n"," '▁বু': 606,\n"," '▁সং': 607,\n"," '▁সৎ': 608,\n"," '▁সকাল': 609,\n"," '▁সত্যিই': 610,\n"," 'ন্ট': 611,\n"," 'ন্না': 612,\n"," '▁কথা': 613,\n"," 'ধুর': 614,\n"," '▁পো': 615,\n"," '▁পেয়ে': 616,\n"," '▁পরি': 617,\n"," '▁তবে': 618,\n"," '▁তাই': 619,\n"," '▁তৈরি': 620,\n"," '▁হা': 621,\n"," '▁হার': 622,\n"," '▁হেন': 623,\n"," '▁মনে': 624,\n"," 'ানি': 625,\n"," '▁করার': 626,\n"," '▁জিব': 627,\n"," '▁দিব': 628,\n"," '▁দিতে': 629,\n"," '▁অন্য': 630,\n"," '▁অনু': 631,\n"," '।\\n\\n': 632,\n"," '।\\nস': 633,\n"," '।\\nবন্ধুত্ব': 634,\n"," '▁ভু': 635,\n"," '▁ভয়': 636,\n"," '▁নাম': 637,\n"," 'ছেন': 638,\n"," '▁শত': 639,\n"," '▁শিক': 640,\n"," '▁শব্': 641,\n"," '▁প্রকৃত': 642,\n"," 'লেন': 643,\n"," 'লেদের': 644,\n"," '▁সাজিয়ে': 645,\n"," '▁সেখানে': 646,\n"," 'ারাপ': 647,\n"," 'ড়ুন': 648,\n"," '▁উদ': 649,\n"," '▁বিরল': 650,\n"," '▁তোর': 651,\n"," '▁থাকার': 652,\n"," '▁থাকতে': 653,\n"," 'লিল': 654,\n"," 'লিজার': 655,\n"," '▁সুখ': 656,\n"," '▁সুযোগ': 657,\n"," '▁কখনও': 658,\n"," '▁পারবে': 659,\n"," '▁খারাপ': 660,\n"," '▁দেয়': 661,\n"," '▁দেয়।': 662,\n"," '▁যাও': 663,\n"," 'প্রত্যে': 664,\n"," '▁ফে': 665,\n"," '▁অজানা\\n\\n💙💚': 666,\n"," '▁অজানা\\n\\n🌷🌷সত্যিকারের': 667,\n"," '▁ক্যাপশন\\n': 668,\n"," '▁আসল': 669,\n"," '▁জীবন': 670,\n"," '▁বাড়': 671,\n"," 'কাতে': 672,\n"," '▁তুল': 673,\n"," '▁মানুষের': 674,\n"," 'গুলো': 675,\n"," '▁আরও': 676,\n"," '▁আরো': 677,\n"," '▁আছে।': 678,\n"," '▁দুর্': 679,\n"," '\\nকখনও': 680,\n"," '▁রঙ': 681,\n"," '▁হৃদয়ে': 682,\n"," '▁ছোট': 683,\n"," '▁পর্যন্ত': 684,\n"," '▁তাকাতে': 685,\n"," '\\n\\n🌹🌹যখন': 686,\n"," '▁বড়ই': 687,\n"," '▁দূরে': 688,\n"," '▁দূরত্ব': 689,\n"," '▁নিজের': 690,\n"," '▁নিজেকে': 691,\n"," '▁চাইনা': 692,\n"," '\\nপৃথিবী': 693,\n"," 'জ্ঞ': 694,\n"," '।\\nযে': 695,\n"," '▁Bangl': 696,\n"," '▁বোঝ': 697,\n"," '▁সর্বদা': 698,\n"," '▁হাসি': 699,\n"," '▁মেঘ': 700,\n"," '▁চোখের': 701,\n"," '\\nনতুন': 702,\n"," 'ূর্ণ': 703,\n"," '▁লাগ': 704,\n"," '▁জিবরান': 705,\n"," '▁শিক্ষ': 706,\n"," '▁শব্দ': 707,\n"," '▁Bangla': 708,\n"," ',\\n': 709,\n"," 'Fa': 710,\n"," 'Na': 711,\n"," 'bo': 712,\n"," 'ce': 713,\n"," 'is': 714,\n"," 'me': 715,\n"," 'ok': 716,\n"," 'oy': 717,\n"," 'ঁচ': 718,\n"," 'ইল': 719,\n"," 'উকে': 720,\n"," 'কো': 721,\n"," 'কলের': 722,\n"," 'খ্যা': 723,\n"," 'খেন': 724,\n"," 'গে': 725,\n"," 'ঙ্': 726,\n"," 'চের': 727,\n"," 'চিত': 728,\n"," 'ছর': 729,\n"," 'জের': 730,\n"," 'ঝে': 731,\n"," 'টো': 732,\n"," 'ঠিক': 733,\n"," 'ত্ত': 734,\n"," 'দু': 735,\n"," 'দৃ': 736,\n"," 'ধান': 737,\n"," 'ধৃতি': 738,\n"," 'পদের': 739,\n"," 'পূর্ণ': 740,\n"," 'ফেসবু': 741,\n"," 'বের': 742,\n"," 'ব্রু': 743,\n"," 'ভে': 744,\n"," 'মৎ': 745,\n"," 'মর্': 746,\n"," 'রো': 747,\n"," 'লী': 748,\n"," 'শ,': 749,\n"," 'শই': 750,\n"," 'শৈ': 751,\n"," 'শ্য': 752,\n"," 'শবের': 753,\n"," 'ষ্ট': 754,\n"," 'স্ট': 755,\n"," 'স\\nপৃথিবী': 756,\n"," 'হে': 757,\n"," 'হলে': 758,\n"," 'াঁ': 759,\n"," 'াম': 760,\n"," 'াবার': 761,\n"," 'াণের': 762,\n"," 'িও': 763,\n"," 'িচ': 764,\n"," 'িত্ব': 765,\n"," 'ি\\n\\n💙💚': 766,\n"," 'েছি': 767,\n"," 'েন্ড': 768,\n"," 'েলেন': 769,\n"," 'োজন': 770,\n"," '্ন': 771,\n"," '্ল': 772,\n"," '্লার': 773,\n"," '্ভব': 774,\n"," '্ধৃতি': 775,\n"," '▁টি': 776,\n"," '▁রে': 777,\n"," '▁Fa': 778,\n"," '▁Na': 779,\n"," '▁ঠিক': 780,\n"," 'য়োজন': 781,\n"," '▁বস': 782,\n"," '▁বন্য': 783,\n"," '▁বছর': 784,\n"," '▁সি': 785,\n"," '▁সতে': 786,\n"," '▁স্ম': 787,\n"," '▁স্পর্': 788,\n"," '▁সন্ধ': 789,\n"," '▁স্ক': 790,\n"," '▁সকলের': 791,\n"," 'ন্ন': 792,\n"," 'ন্ম': 793,\n"," 'ন্দের': 794,\n"," 'ারণ': 795,\n"," '▁আগ': 796,\n"," '▁আন': 797,\n"," '▁আলা': 798,\n"," '▁আছি': 799,\n"," '▁এত': 800,\n"," '▁একে': 801,\n"," '▁নয়,': 802,\n"," '▁পু': 803,\n"," '▁পছন্দ': 804,\n"," '▁তাকে': 805,\n"," '▁ততই': 806,\n"," 'কেই': 807,\n"," 'কেদের': 808,\n"," '▁হও': 809,\n"," '▁হো': 810,\n"," '▁হবে': 811,\n"," '▁হলে': 812,\n"," '▁হারা': 813,\n"," '▁হওয়া': 814,\n"," '▁হলো': 815,\n"," '▁হবে।': 816,\n"," '▁হেলেন': 817,\n"," '▁যাদের': 818,\n"," '▁মন': 819,\n"," '▁মার': 820,\n"," '▁মনের': 821,\n"," '▁মধুর': 822,\n"," 'ানের': 823,\n"," 'ানোর': 824,\n"," '▁করবেন': 825,\n"," 'বে,': 826,\n"," 'বে।\\nপ্রিয়': 827,\n"," '▁জে': 828,\n"," '▁জো': 829,\n"," '▁জন্ম': 830,\n"," '▁দর': 831,\n"," '▁দিন': 832,\n"," '▁দুন': 833,\n"," 'তে।\\n': 834,\n"," '▁অ্যা': 835,\n"," '▁অপর': 836,\n"," '▁অদৃ': 837,\n"," '।\\nবন্ধু': 838,\n"," '▁আমায়': 839,\n"," '্যায়ে': 840,\n"," '▁একমাত্র': 841,\n"," '▁একদিন': 842,\n"," '▁ভি': 843,\n"," '▁ভে': 844,\n"," '▁না।\\nএক': 845,\n"," '▁নিন': 846,\n"," '▁নিতে': 847,\n"," 'ছে।': 848,\n"," 'ায়ার': 849,\n"," 'ায়শই': 850,\n"," 'তুমি': 851,\n"," '▁সমস': 852,\n"," '▁সম্ম': 853,\n"," '▁সমস্ত': 854,\n"," '▁শক্তি': 855,\n"," '▁প্রেম': 856,\n"," '▁প্রায়শই': 857,\n"," '▁চলে': 858,\n"," '▁চমৎ': 859,\n"," '▁সাম': 860,\n"," '▁বন্ধুত্বই': 861,\n"," '▁বন্ধুত্বকে': 862,\n"," '▁করেন': 863,\n"," '▁কিলার': 864,\n"," 'ড়িয়ে': 865,\n"," '▁উঠ': 866,\n"," '▁উইল': 867,\n"," '▁বিভি': 868,\n"," '▁বিপদের': 869,\n"," '▁তো,': 870,\n"," '▁থাকবে।': 871,\n"," '▁থাকুক': 872,\n"," 'িবার': 873,\n"," '▁নিয়ে\\n': 874,\n"," '▁স্ট্যাটাস,': 875,\n"," '▁স্ট্যাটাসকে': 876,\n"," '▁সুখের': 877,\n"," '▁কখনই': 878,\n"," 'ওয়ার': 879,\n"," 'লাম': 880,\n"," '▁খুলে': 881,\n"," '▁খলিল': 882,\n"," '▁খাবার': 883,\n"," '▁কাটাই': 884,\n"," '▁কান্না': 885,\n"," '▁কাউকে': 886,\n"," '▁দেখ': 887,\n"," '▁থাকেন': 888,\n"," '▁সময়ের': 889,\n"," '▁তোমায়': 890,\n"," 'ুন।': 891,\n"," '▁যাওয়া': 892,\n"," 'বন্ধুরা': 893,\n"," 'প্রিয়': 894,\n"," 'াতা': 895,\n"," '▁অজানা\\n': 896,\n"," '▁অজানা\\nকখনও': 897,\n"," '▁অজানা\\n\\n🌹🌹যখন': 898,\n"," '▁ক্যাপশন,': 899,\n"," '▁আসে,': 900,\n"," '▁কিছুই': 901,\n"," 'কেরই': 902,\n"," '▁তুই': 903,\n"," 'ভাগ': 904,\n"," 'রি\\n': 905,\n"," '▁মানুষ,': 906,\n"," '▁মানুষটি': 907,\n"," '\\n\\n🥀💗বন্ধু': 908,\n"," '\\n\\n🥀💗প্রত্যে': 909,\n"," '\\n\\n🌹🌹বন্ধুরা': 910,\n"," '▁ভালোবাস': 911,\n"," '\\nএক': 912,\n"," '\\nএবং': 913,\n"," '\\nএমন': 914,\n"," '▁রাখে': 915,\n"," '▁আছে,': 916,\n"," '▁যায়,': 917,\n"," '▁যায়।\\n': 918,\n"," '▁দুটো': 919,\n"," '▁অনেকেই': 920,\n"," '\\nকোন': 921,\n"," '\\nকারণ': 922,\n"," 'হাম': 923,\n"," '▁টান': 924,\n"," '▁টুক': 925,\n"," '▁ডা': 926,\n"," '▁ব্যব': 927,\n"," '▁কেমন': 928,\n"," '▁নেয়': 929,\n"," '▁নেয়।': 930,\n"," '▁নেই,': 931,\n"," '▁সম্পর্ক': 932,\n"," '▁শুরু': 933,\n"," 'ধ্যমে': 934,\n"," '▁ছবি': 935,\n"," '▁ছায়ার': 936,\n"," '▁আকাশে': 937,\n"," '▁বন্ধুরাই': 938,\n"," '▁পর্যায়ে': 939,\n"," '▁পৃথিবীর': 940,\n"," '▁তাহলে': 941,\n"," '।\\nআমাদের': 942,\n"," '▁বিশ্বের': 943,\n"," '▁বিশ্বাস': 944,\n"," '▁বিশ্বস্ত': 945,\n"," '▁কোনো': 946,\n"," '▁মাঝে': 947,\n"," '▁মাধ্যমে': 948,\n"," '▁করুন।': 949,\n"," '▁প্রতিটি': 950,\n"," '▁খুব': 951,\n"," '▁সুন্দর,': 952,\n"," '\\nপড়ুন': 953,\n"," 'nglis': 954,\n"," 'বার্ট': 955,\n"," 'য়ারি': 956,\n"," '▁আত্মায়': 957,\n"," '▁আত্মাকে': 958,\n"," '▁যতক্ষ': 959,\n"," '▁মুছ': 960,\n"," '▁মুছে': 961,\n"," '▁হৃদয়ের': 962,\n"," '▁সাহায্যের': 963,\n"," '▁ছেলেদের': 964,\n"," '▁লোকের': 965,\n"," '▁লোকেদের': 966,\n"," '▁বৃষ্টিতে': 967,\n"," '▁আজকের': 968,\n"," '▁হাতের': 969,\n"," '▁জিজ্ঞ': 970,\n"," '▁ভিতর': 971,\n"," '▁শিখেন': 972,\n"," '▁শেয়ার': 973,\n"," '▁ফ্রেন্ড': 974,\n"," '▁ফেসবুকে': 975,\n"," '▁ফেসবুকের': 976,\n"," '▁গোলাপের': 977,\n"," '▁ধরে': 978,\n"," '▁২১': 979,\n"," '▁সংগ্রহ': 980,\n"," '▁হারিয়ে': 981,\n"," '▁অনুভব': 982,\n"," '▁ভুল': 983,\n"," '▁শত্রু': 984,\n"," '▁সাজিয়েছি।': 985,\n"," '▁উদ্ধৃতি': 986,\n"," '▁ফেব্রু': 987,\n"," '▁তুলতে': 988,\n"," '▁চাইনা,': 989,\n"," '▁বোঝে,': 990,\n"," '▁মেঘলা': 991,\n"," 'book': 992,\n"," 'cebook': 993,\n"," 'ফেসবুক': 994,\n"," 'শৈশবের': 995,\n"," 'স\\nপৃথিবীর': 996,\n"," '▁রেখে': 997,\n"," '▁Facebook': 998,\n"," '▁Name': 999,\n"," ...}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# vocab"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:21:37.288185Z","iopub.status.busy":"2024-04-02T09:21:37.287654Z","iopub.status.idle":"2024-04-02T09:21:37.308396Z","shell.execute_reply":"2024-04-02T09:21:37.306796Z","shell.execute_reply.started":"2024-04-02T09:21:37.288146Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["\n","tokenizer.train_from_iterator(\n","    iterator=text,\n","    vocab_size=30_000,\n","    min_frequency=5,\n","    show_progress=True,\n","    limit_alphabet=500,\n","    special_tokens=[\n","        \"<pad>\",\n","        \"<s>\",\n","        \"</s>\",\n","        \"<unk>\",\n","        '<mask>'\n","    ]\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.816570Z","iopub.status.busy":"2024-04-02T09:18:34.815937Z","iopub.status.idle":"2024-04-02T09:18:34.821046Z","shell.execute_reply":"2024-04-02T09:18:34.820308Z","shell.execute_reply.started":"2024-04-02T09:18:34.816533Z"},"trusted":true},"outputs":[],"source":["from tokenizers.processors import BertProcessing\n","tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.823225Z","iopub.status.busy":"2024-04-02T09:18:34.822541Z","iopub.status.idle":"2024-04-02T09:18:34.833708Z","shell.execute_reply":"2024-04-02T09:18:34.832649Z","shell.execute_reply.started":"2024-04-02T09:18:34.823189Z"},"trusted":true},"outputs":[],"source":["tokenizer.save(\"tokenizer.json\")"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:34.836338Z","iopub.status.busy":"2024-04-02T09:18:34.835377Z","iopub.status.idle":"2024-04-02T09:18:40.120461Z","shell.execute_reply":"2024-04-02T09:18:40.119153Z","shell.execute_reply.started":"2024-04-02T09:18:34.836306Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='', vocab_size=110, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import PreTrainedTokenizerFast\n","\n","transformer_tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_object=tokenizer #tokenizer_file=\"tokenizer.json\"\n",")\n","transformer_tokenizer"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:40.125801Z","iopub.status.busy":"2024-04-02T09:18:40.125217Z","iopub.status.idle":"2024-04-02T09:18:40.134986Z","shell.execute_reply":"2024-04-02T09:18:40.133816Z","shell.execute_reply.started":"2024-04-02T09:18:40.125768Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['▁',\n"," 'গ',\n"," 'েল',\n"," '▁ইউনিটেক',\n"," '▁প',\n"," '্র',\n"," 'ড',\n"," 'া',\n"," 'ক্ট',\n"," 'স',\n"," '▁(',\n"," 'ব',\n"," 'ি',\n"," 'ড',\n"," 'ি',\n"," ')']"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["test_text = normalize(\"গেল ইউনিটেক প্রডাক্টস (বিডি)\")\n","transformer_tokenizer.tokenize(test_text)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:18:40.136429Z","iopub.status.busy":"2024-04-02T09:18:40.136044Z","iopub.status.idle":"2024-04-02T09:19:28.805005Z","shell.execute_reply":"2024-04-02T09:19:28.803933Z","shell.execute_reply.started":"2024-04-02T09:18:40.136375Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbaa6910d8354933aded80b548b40540","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Virus-Proton/CustomTokenizer/commit/24814882f2a22c2c36b7d1e0d2849dab431266c6', commit_message='Upload tokenizer', commit_description='', oid='24814882f2a22c2c36b7d1e0d2849dab431266c6', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["transformer_tokenizer.push_to_hub(\"Virus-Proton/CustomTokenizer\")"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:28.807853Z","iopub.status.busy":"2024-04-02T09:19:28.806649Z","iopub.status.idle":"2024-04-02T09:19:42.107695Z","shell.execute_reply":"2024-04-02T09:19:42.105990Z","shell.execute_reply.started":"2024-04-02T09:19:28.807809Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75b99022c1fb4f79afb949515cfa6509","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9e2b0bd156b4d3aace00e5703ab21d3","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/4.88k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ea3a753e674472f879b2160fefb4951","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","custom_tokenizer = AutoTokenizer.from_pretrained(\"Virus-Proton/CustomTokenizer\")"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:42.110960Z","iopub.status.busy":"2024-04-02T09:19:42.110374Z","iopub.status.idle":"2024-04-02T09:19:42.121819Z","shell.execute_reply":"2024-04-02T09:19:42.120418Z","shell.execute_reply.started":"2024-04-02T09:19:42.110910Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [1, 59, 21, 87, 101, 88, 66, 28, 48, 76, 45, 74, 37, 49, 28, 49, 6, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_text = custom_tokenizer.encode_plus(test_text)\n","tokenized_text"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:42.124808Z","iopub.status.busy":"2024-04-02T09:19:42.124128Z","iopub.status.idle":"2024-04-02T09:19:55.751730Z","shell.execute_reply":"2024-04-02T09:19:55.750450Z","shell.execute_reply.started":"2024-04-02T09:19:42.124742Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-02 09:19:44.571283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-02 09:19:44.571528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-02 09:19:44.763675: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"text/plain":["'<s> গেল ইউনিটেক প্রডাক্টস (বিডি)</s>'"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["custom_tokenizer.decode(tokenized_text['input_ids'])"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:19:55.754065Z","iopub.status.busy":"2024-04-02T09:19:55.753406Z","iopub.status.idle":"2024-04-02T09:19:55.762846Z","shell.execute_reply":"2024-04-02T09:19:55.761535Z","shell.execute_reply.started":"2024-04-02T09:19:55.754028Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['<s>',\n"," '▁',\n"," 'গ',\n"," 'েল',\n"," '▁ইউনিটেক',\n"," '▁প',\n"," '্র',\n"," 'ড',\n"," 'া',\n"," 'ক্ট',\n"," 'স',\n"," '▁(',\n"," 'ব',\n"," 'ি',\n"," 'ড',\n"," 'ি',\n"," ')',\n"," '</s>']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["custom_tokenizer.convert_ids_to_tokens(tokenized_text['input_ids'])"]},{"cell_type":"markdown","metadata":{},"source":["# Advanced"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","        vocab_size (int, optional) – The size of the final vocabulary, including all tokens and alphabet.\n","\n","        min_frequency (int, optional) – The minimum frequency a pair should have in order to be merged.\n","\n","        show_progress (bool, optional) – Whether to show progress bars while training.\n","\n","        special_tokens (List[Union[str, AddedToken]], optional) – A list of special tokens the model should know of.\n","\n","        limit_alphabet (int, optional) – The maximum different characters to keep in the alphabet.\n","\n","        initial_alphabet (List[str], optional) – A list of characters to include in the initial alphabet, even if not seen in the training dataset. If the strings contain more than one character, only the first one is kept.\n","\n","        continuing_subword_prefix (str, optional) – A prefix to be used for every subword that is not a beginning-of-word.\n","\n","        end_of_word_suffix (str, optional) – A suffix to be used for every subword that is a end-of-word.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:38:30.051021Z","iopub.status.busy":"2024-04-02T09:38:30.050634Z","iopub.status.idle":"2024-04-02T09:38:36.166062Z","shell.execute_reply":"2024-04-02T09:38:36.164777Z","shell.execute_reply.started":"2024-04-02T09:38:30.050991Z"},"trusted":true},"outputs":[],"source":["import tokenizers\n","tokenizers.__version__\n","\n","from tokenizers import SentencePieceBPETokenizer\n","from tokenizers.processors import BertProcessing\n","from transformers import PreTrainedTokenizerFast"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T09:42:54.967272Z","iopub.status.busy":"2024-04-02T09:42:54.966727Z","iopub.status.idle":"2024-04-02T09:42:54.976349Z","shell.execute_reply":"2024-04-02T09:42:54.974723Z","shell.execute_reply.started":"2024-04-02T09:42:54.967220Z"},"trusted":true},"outputs":[],"source":["texts = ['সাভারের কবিরপুর বাণিজ্যিক এলাকার নিজস্ব ফ্যাক্টরিতে হয়ে গেল ইউনিটেক প্রডাক্টস (বিডি) লিঃ এর দিনব্যাপী সেলস কনফারেন্স। কনফারেন্সটি অনুষ্ঠিত হয় শনিবার, ১১ই মার্চ, সারা দেশ থেকে প্রায় ৩১০ জন ডিলার এই কনফারেন্সটিতে অংশগ্রহণ করেন এবং ইউনিটেকর ফ্যাক্টরি ঘুরে দেখার সুযোগ পান।এসময় ইউনিটেকর ব্যবস্থাপনা পরিচালক, ইঞ্জি: আনিস আহমেদ ডিলারদের উদ্দেশে বক্তব্য রাখেন এবং ইউনিটেক কে বাংলাদেশের সেরা ইলেক্ট্রনিক ব্র্যান্ড হিসেবে গড়ে তোলার আশ্বাস ব্যক্ত করেন। ইঞ্জি: আনিস আহমেদ, জাতীয় এবং বিভাগীয় পর্যায়ে সেরা ডিলারদের হাতে সার্টিফিকেট, ক্রেস্ট ও প্রাইজ বন্ড তুলে দেন। বিজয়ীরা হলেন রুপালি ইলেক্ট্রনিক্স (জাতীয় সেরা), লিয়া এন্টারপ্রাইজ (দ্বিতীয় জাতীয় সেরা), মদিনা ইলেক্ট্রনিক্স (তৃতীয় জাতীয় সেরা), সুন্দরবন ইলেক্ট্রনিক্স (ঢাকা বিভাগ), আলাউদ্দিন ইলেক্ট্রনিক্স (চিটাগং বিভাগ), শান্ত ইলেক্ট্রনিক্স (রাজশাহী রংপুর বিভাগ), নুরুল এন্টারপ্রাইজ (খুলনা বিভাগ), ফেয়ার ভিউ ইলেক্ট্রনিক্স (বরিশাল বিভাগ) ও ফিডব্যাক কমিউনিকেসন (সিলেট বিভাগ)। কনফারেন্সটিতে আরও বক্তব্য রাখেন ইউনিটেকের বিজনেস ডেভেলপমেন্ট অ্যান্ড প্ল্যানিং ম্যানেজার এ কে এম হামিদুর রহমান, সেলস অ্যান্ড ডেভেলপমেন্ট ম্যানেজার এ টি এম আখতার হোসেন ও ইউনিটেকের সেলস ম্যানেজার ডিলার চ্যানেল জাকির হোসেন। -প্রেস বিজ্ঞপ্তি']"]},{"cell_type":"code","execution_count":31,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T10:09:59.410496Z","iopub.status.busy":"2024-04-02T10:09:59.410077Z","iopub.status.idle":"2024-04-02T10:09:59.422629Z","shell.execute_reply":"2024-04-02T10:09:59.421407Z","shell.execute_reply.started":"2024-04-02T10:09:59.410466Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on class SentencePieceBPETokenizer in module tokenizers.implementations.sentencepiece_bpe:\n","\n","class SentencePieceBPETokenizer(tokenizers.implementations.base_tokenizer.BaseTokenizer)\n"," |  SentencePieceBPETokenizer(vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = '▁', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |  \n"," |  SentencePiece BPE Tokenizer\n"," |  \n"," |  Represents the BPE algorithm, with the pretokenization used by SentencePiece\n"," |  \n"," |  Method resolution order:\n"," |      SentencePieceBPETokenizer\n"," |      tokenizers.implementations.base_tokenizer.BaseTokenizer\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, vocab: Union[str, Dict[str, int], NoneType] = None, merges: Union[str, Dict[Tuple[int, int], Tuple[int, int]], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '<unk>', replacement: str = '▁', add_prefix_space: bool = True, dropout: Optional[float] = None, fuse_unk: Optional[bool] = False)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  train(self, files: Union[str, List[str]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True)\n"," |      Train the model using the given files\n"," |  \n"," |  train_from_iterator(self, iterator: Union[Iterator[str], Iterator[Iterator[str]]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True, length: Optional[int] = None)\n"," |      Train the model using the given iterator\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Static methods defined here:\n"," |  \n"," |  from_file(vocab_filename: str, merges_filename: str, **kwargs)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __slotnames__ = []\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __repr__(self)\n"," |      Return repr(self).\n"," |  \n"," |  add_special_tokens(self, special_tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given special tokens to the vocabulary, and treat them as special tokens.\n"," |      \n"," |      The special tokens will never be processed by the model, and will be\n"," |      removed while decoding.\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of special tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  add_tokens(self, tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n"," |      Add the given tokens to the vocabulary\n"," |      \n"," |      Args:\n"," |          tokens: List[Union[str, AddedToken]]:\n"," |              A list of tokens to add to the vocabulary. Each token can either be\n"," |              a string, or an instance of AddedToken\n"," |      \n"," |      Returns:\n"," |          The number of tokens that were added to the vocabulary\n"," |  \n"," |  decode(self, ids: List[int], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the given list of ids to a string sequence\n"," |      \n"," |      Args:\n"," |          ids: List[unsigned int]:\n"," |              A list of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output string\n"," |      \n"," |      Returns:\n"," |          The decoded string\n"," |  \n"," |  decode_batch(self, sequences: List[List[int]], skip_special_tokens: Optional[bool] = True) -> str\n"," |      Decode the list of sequences to a list of string sequences\n"," |      \n"," |      Args:\n"," |          sequences: List[List[unsigned int]]:\n"," |              A list of sequence of ids to be decoded\n"," |      \n"," |          skip_special_tokens: (`optional`) boolean:\n"," |              Whether to remove all the special tokens from the output strings\n"," |      \n"," |      Returns:\n"," |          A list of decoded strings\n"," |  \n"," |  enable_padding(self, direction: Optional[str] = 'right', pad_to_multiple_of: Optional[int] = None, pad_id: Optional[int] = 0, pad_type_id: Optional[int] = 0, pad_token: Optional[str] = '[PAD]', length: Optional[int] = None)\n"," |      Change the padding strategy\n"," |      \n"," |      Args:\n"," |          direction: (`optional`) str:\n"," |              Can be one of: `right` or `left`\n"," |      \n"," |          pad_to_multiple_of: (`optional`) unsigned int:\n"," |              If specified, the padding length should always snap to the next multiple of\n"," |              the given value. For example if we were going to pad with a length of 250 but\n"," |              `pad_to_multiple_of=8` then we will pad to 256.\n"," |      \n"," |          pad_id: (`optional`) unsigned int:\n"," |              The indice to be used when padding\n"," |      \n"," |          pad_type_id: (`optional`) unsigned int:\n"," |              The type indice to be used when padding\n"," |      \n"," |          pad_token: (`optional`) str:\n"," |              The pad token to be used when padding\n"," |      \n"," |          length: (`optional`) unsigned int:\n"," |              If specified, the length at which to pad. If not specified\n"," |              we pad using the size of the longest sequence in a batch\n"," |  \n"," |  enable_truncation(self, max_length: int, stride: Optional[int] = 0, strategy: Optional[str] = 'longest_first')\n"," |      Change the truncation options\n"," |      \n"," |      Args:\n"," |          max_length: unsigned int:\n"," |              The maximum length at which to truncate\n"," |      \n"," |          stride: (`optional`) unsigned int:\n"," |              The length of the previous first sequence to be included\n"," |              in the overflowing sequence\n"," |      \n"," |          strategy: (`optional`) str:\n"," |              Can be one of `longest_first`, `only_first` or `only_second`\n"," |  \n"," |  encode(self, sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Encode the given sequence and pair. This method can process raw text sequences as well\n"," |      as already pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          sequence: InputSequence:\n"," |              The sequence we want to encode. This sequence can be either raw text or\n"," |              pre-tokenized, according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          An Encoding\n"," |  \n"," |  encode_batch(self, inputs: List[Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]], is_pretokenized: bool = False, add_special_tokens: bool = True) -> List[tokenizers.Encoding]\n"," |      Encode the given inputs. This method accept both raw text sequences as well as already\n"," |      pre-tokenized sequences.\n"," |      \n"," |      Args:\n"," |          inputs: List[EncodeInput]:\n"," |              A list of single sequences or pair sequences to encode. Each `EncodeInput` is\n"," |              expected to be of the following form:\n"," |                  `Union[InputSequence, Tuple[InputSequence, InputSequence]]`\n"," |      \n"," |              Each `InputSequence` can either be raw text or pre-tokenized,\n"," |              according to the `is_pretokenized` argument:\n"," |      \n"," |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n"," |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n"," |                  `Union[List[str], Tuple[str]]`\n"," |      \n"," |          is_pretokenized: bool:\n"," |              Whether the input is already pre-tokenized.\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add the special tokens while encoding.\n"," |      \n"," |      Returns:\n"," |          A list of Encoding\n"," |  \n"," |  get_added_tokens_decoder(self) -> Dict[int, tokenizers.AddedToken]\n"," |      Returns the added reverse vocabulary\n"," |      \n"," |      Returns:\n"," |          The added vocabulary mapping ints to AddedTokens\n"," |  \n"," |  get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]\n"," |      Returns the vocabulary\n"," |      \n"," |      Args:\n"," |          with_added_tokens: boolean:\n"," |              Whether to include the added tokens in the vocabulary\n"," |      \n"," |      Returns:\n"," |          The vocabulary\n"," |  \n"," |  get_vocab_size(self, with_added_tokens: bool = True) -> int\n"," |      Return the size of vocabulary, with or without added tokens.\n"," |      \n"," |      Args:\n"," |          with_added_tokens: (`optional`) bool:\n"," |              Whether to count in added special tokens or not\n"," |      \n"," |      Returns:\n"," |          Size of vocabulary\n"," |  \n"," |  id_to_token(self, id: int) -> Optional[str]\n"," |      Convert the given token id to its corresponding string\n"," |      \n"," |      Args:\n"," |          token: id:\n"," |              The token id to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding string if it exists, None otherwise\n"," |  \n"," |  no_padding(self)\n"," |      Disable padding\n"," |  \n"," |  no_truncation(self)\n"," |      Disable truncation\n"," |  \n"," |  normalize(self, sequence: str) -> str\n"," |      Normalize the given sequence\n"," |      \n"," |      Args:\n"," |          sequence: str:\n"," |              The sequence to normalize\n"," |      \n"," |      Returns:\n"," |          The normalized string\n"," |  \n"," |  num_special_tokens_to_add(self, is_pair: bool) -> int\n"," |      Return the number of special tokens that would be added for single/pair sentences.\n"," |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n"," |      :return:\n"," |  \n"," |  post_process(self, encoding: tokenizers.Encoding, pair: Optional[tokenizers.Encoding] = None, add_special_tokens: bool = True) -> tokenizers.Encoding\n"," |      Apply all the post-processing steps to the given encodings.\n"," |      \n"," |      The various steps are:\n"," |          1. Truncate according to global params (provided to `enable_truncation`)\n"," |          2. Apply the PostProcessor\n"," |          3. Pad according to global params. (provided to `enable_padding`)\n"," |      \n"," |      Args:\n"," |          encoding: Encoding:\n"," |              The main Encoding to post process\n"," |      \n"," |          pair: Optional[Encoding]:\n"," |              An optional pair Encoding\n"," |      \n"," |          add_special_tokens: bool:\n"," |              Whether to add special tokens\n"," |      \n"," |      Returns:\n"," |          The resulting Encoding\n"," |  \n"," |  save(self, path: str, pretty: bool = True)\n"," |      Save the current Tokenizer at the given path\n"," |      \n"," |      Args:\n"," |          path: str:\n"," |              A path to the destination Tokenizer file\n"," |  \n"," |  save_model(self, directory: str, prefix: Optional[str] = None)\n"," |      Save the current model to the given directory\n"," |      \n"," |      Args:\n"," |          directory: str:\n"," |              A path to the destination directory\n"," |      \n"," |          prefix: (Optional) str:\n"," |              An optional prefix, used to prefix each file name\n"," |  \n"," |  to_str(self, pretty: bool = False)\n"," |      Get a serialized JSON version of the Tokenizer as a str\n"," |      \n"," |      Args:\n"," |          pretty: bool:\n"," |              Whether the JSON string should be prettified\n"," |      \n"," |      Returns:\n"," |          str\n"," |  \n"," |  token_to_id(self, token: str) -> Optional[int]\n"," |      Convert the given token to its corresponding id\n"," |      \n"," |      Args:\n"," |          token: str:\n"," |              The token to convert\n"," |      \n"," |      Returns:\n"," |          The corresponding id if it exists, None otherwise\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  padding\n"," |      Get the current padding parameters\n"," |      \n"," |      Returns:\n"," |          None if padding is disabled, a dict with the currently set parameters\n"," |          if the padding is enabled.\n"," |  \n"," |  truncation\n"," |      Get the current truncation parameters\n"," |      \n"," |      Returns:\n"," |          None if truncation is disabled, a dict with the current truncation parameters if\n"," |          truncation is enabled\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  decoder\n"," |  \n"," |  model\n"," |  \n"," |  normalizer\n"," |  \n"," |  post_processor\n"," |  \n"," |  pre_tokenizer\n","\n"]}],"source":["# help(SentencePieceBPETokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["### Train the tokenizer\n","\n","help(SentencePieceBPETokenizer)<br>\n","self, files: Union[str, List[str]], \n","vocab_size: int = 30000, min_frequency: int = 2, \n","special_tokens: List[Union[str, tokenizers.AddedToken]] = ['<unk>'], \n","limit_alphabet: int = 1000, initial_alphabet: List[str] = [], show_progress: bool = True"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:02:53.741021Z","iopub.status.busy":"2024-04-02T10:02:53.740527Z","iopub.status.idle":"2024-04-02T10:02:53.748354Z","shell.execute_reply":"2024-04-02T10:02:53.746374Z","shell.execute_reply.started":"2024-04-02T10:02:53.740988Z"},"trusted":true},"outputs":[],"source":["tokenizer = SentencePieceBPETokenizer()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:00:49.993492Z","iopub.status.busy":"2024-04-02T10:00:49.993002Z","iopub.status.idle":"2024-04-02T10:00:50.002277Z","shell.execute_reply":"2024-04-02T10:00:50.000633Z","shell.execute_reply.started":"2024-04-02T10:00:49.993460Z"},"trusted":true},"outputs":[],"source":["# special_token = [{\"id\":0,\"special\":True,\"content\":\"<s>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":1,\"special\":True,\"content\":\"<pad>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":2,\"special\":True,\"content\":\"</s>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":3,\"special\":True,\"content\":\"<unk>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False},\n","#                  {\"id\":50264,\"special\":True,\"content\":\"<mask>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False}]"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:03:55.489071Z","iopub.status.busy":"2024-04-02T10:03:55.488348Z","iopub.status.idle":"2024-04-02T10:03:55.499638Z","shell.execute_reply":"2024-04-02T10:03:55.497961Z","shell.execute_reply.started":"2024-04-02T10:03:55.489035Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Ignored unknown kwarg option id\n"]},{"data":{"text/plain":["AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:05:52.067104Z","iopub.status.busy":"2024-04-02T10:05:52.066647Z","iopub.status.idle":"2024-04-02T10:05:52.086951Z","shell.execute_reply":"2024-04-02T10:05:52.085037Z","shell.execute_reply.started":"2024-04-02T10:05:52.067072Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["## Training\n","\n","tokenizer.train_from_iterator(\n","    texts,\n","    vocab_size=50_265,\n","    min_frequency=5, # rare word treatment\n","    show_progress=True,\n","    limit_alphabet=1000,\n","    special_tokens=[\n","        \"<s>\",\n","        \"</s>\",\n","        \"<unk>\" ,\n","        \"<pad>\",\n","    ]\n",")\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:21.341784Z","iopub.status.busy":"2024-04-02T10:12:21.340753Z","iopub.status.idle":"2024-04-02T10:12:21.352681Z","shell.execute_reply":"2024-04-02T10:12:21.350923Z","shell.execute_reply.started":"2024-04-02T10:12:21.341741Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["op={\"special\":True,\"content\":\"<mask>\",\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False}\n","mask_token = tokenizers.AddedToken(**op)\n","tokenizer.add_tokens([mask_token])"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:25.491590Z","iopub.status.busy":"2024-04-02T10:12:25.491097Z","iopub.status.idle":"2024-04-02T10:12:25.498836Z","shell.execute_reply":"2024-04-02T10:12:25.497219Z","shell.execute_reply.started":"2024-04-02T10:12:25.491554Z"},"trusted":true},"outputs":[],"source":["## Add post processing\n","\n","tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","       \n",")"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:29.671812Z","iopub.status.busy":"2024-04-02T10:12:29.671302Z","iopub.status.idle":"2024-04-02T10:12:29.677820Z","shell.execute_reply":"2024-04-02T10:12:29.676065Z","shell.execute_reply.started":"2024-04-02T10:12:29.671775Z"},"trusted":true},"outputs":[],"source":["tokenizer.save(\"tokenizer.json\")"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:31.358591Z","iopub.status.busy":"2024-04-02T10:12:31.358027Z","iopub.status.idle":"2024-04-02T10:12:31.370535Z","shell.execute_reply":"2024-04-02T10:12:31.367996Z","shell.execute_reply.started":"2024-04-02T10:12:31.358552Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'version': '1.0', 'truncation': None, 'padding': None, 'added_tokens': [{'id': 0, 'content': '<s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 1, 'content': '</s>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 2, 'content': '<unk>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 3, 'content': '<pad>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': True}, {'id': 109, 'content': '<mask>', 'single_word': False, 'lstrip': False, 'rstrip': False, 'normalized': False, 'special': False}], 'normalizer': {'type': 'NFKC'}, 'pre_tokenizer': {'type': 'Metaspace', 'replacement': '▁', 'add_prefix_space': True, 'prepend_scheme': 'always'}, 'post_processor': {'type': 'BertProcessing', 'sep': ['</s>', 1], 'cls': ['<s>', 0]}, 'decoder': {'type': 'Metaspace', 'replacement': '▁', 'add_prefix_space': True, 'prepend_scheme': 'always'}, 'model': {'type': 'BPE', 'dropout': None, 'unk_token': '<unk>', 'continuing_subword_prefix': None, 'end_of_word_suffix': None, 'fuse_unk': False, 'byte_fallback': False, 'vocab': {'<s>': 0, '</s>': 1, '<unk>': 2, '<pad>': 3, '(': 4, ')': 5, ',': 6, '-': 7, ':': 8, '।': 9, 'ং': 10, 'ঃ': 11, 'অ': 12, 'আ': 13, 'ই': 14, 'উ': 15, 'এ': 16, 'ও': 17, 'ক': 18, 'খ': 19, 'গ': 20, 'ঘ': 21, 'চ': 22, 'জ': 23, 'ঞ': 24, 'ট': 25, 'ঠ': 26, 'ড': 27, 'ঢ': 28, 'ণ': 29, 'ত': 30, 'থ': 31, 'দ': 32, 'ন': 33, 'প': 34, 'ফ': 35, 'ব': 36, 'ভ': 37, 'ম': 38, 'য': 39, 'র': 40, 'ল': 41, 'শ': 42, 'ষ': 43, 'স': 44, 'হ': 45, '়': 46, 'া': 47, 'ি': 48, 'ী': 49, 'ু': 50, 'ৃ': 51, 'ে': 52, 'ো': 53, '্': 54, '০': 55, '১': 56, '৩': 57, '▁': 58, 'ার': 59, 'ক্': 60, 'নি': 61, '্য': 62, '▁ব': 63, 'য়': 64, '্র': 65, '▁ই': 66, 'ন্': 67, '▁স': 68, '্যা': 69, '▁এ': 70, 'ের': 71, 'লে': 72, '▁(': 73, '▁ক': 74, 'ক্ট': 75, '▁বি': 76, 'াগ': 77, '▁আ': 78, '▁হ': 79, '),': 80, 'উনি': 81, 'ভাগ': 82, 'ীয়': 83, 'েক': 84, 'েন': 85, 'েল': 86, '▁প': 87, '্রনি': 88, '▁ইলে': 89, 'ক্ট্রনি': 90, '▁বিভাগ': 91, '▁ইলেক্ট্রনি': 92, 'টেক': 93, 'তীয়': 94, 'েন্': 95, '▁ড': 96, 'ক্স': 97, '▁ইউনি': 98, '▁ইলেক্ট্রনিক্স': 99, '▁ইউনিটেক': 100, 'জা': 101, 'টি': 102, 'লার': 103, 'ুর': 104, '▁ম': 105, '▁র': 106, '▁সের': 107, '▁সেরা': 108}, 'merges': ['া র', 'ক ্', 'ন ি', '্ য', '▁ ব', 'য ়', '্ র', '▁ ই', 'ন ্', '▁ স', '্য া', '▁ এ', 'ে র', 'ল ে', '▁ (', '▁ ক', 'ক্ ট', '▁ব ি', 'া গ', '▁ আ', '▁ হ', ') ,', 'উ নি', 'ভ াগ', 'ী য়', 'ে ক', 'ে ন', 'ে ল', '▁ প', '্র নি', '▁ই লে', 'ক্ট ্রনি', '▁বি ভাগ', '▁ইলে ক্ট্রনি', 'ট েক', 'ত ীয়', 'ে ন্', '▁ ড', 'ক্ স', '▁ই উনি', '▁ইলেক্ট্রনি ক্স', '▁ইউনি টেক', 'জ া', 'ট ি', 'ল ার', 'ু র', '▁ ম', '▁ র', '▁স ের', '▁সের া']}}\n"]}],"source":["import json\n","with open('tokenizer.json', 'r') as f:\n","    d = json.load(f)\n","    print(d)\n","    "]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-02T10:12:48.319664Z","iopub.status.busy":"2024-04-02T10:12:48.319121Z","iopub.status.idle":"2024-04-02T10:12:48.331321Z","shell.execute_reply":"2024-04-02T10:12:48.329792Z","shell.execute_reply.started":"2024-04-02T10:12:48.319625Z"},"trusted":true},"outputs":[{"data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='', vocab_size=109, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t109: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n","}"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["## Wrap the tokenizer with existing hugging face tokenizer\n","\n","transformer_tokenizer = PreTrainedTokenizerFast(\n","    tokenizer_object=tokenizer #tokenizer_file=\"tokenizer.json\"\n",")\n","transformer_tokenizer"]},{"cell_type":"code","execution_count":39,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-04-02T10:12:52.029941Z","iopub.status.busy":"2024-04-02T10:12:52.029541Z","iopub.status.idle":"2024-04-02T10:12:52.041546Z","shell.execute_reply":"2024-04-02T10:12:52.039867Z","shell.execute_reply.started":"2024-04-02T10:12:52.029911Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["['▁',\n"," 'গ',\n"," 'েল',\n"," '▁ইউনিটেক',\n"," '▁প',\n"," '্র',\n"," 'ড',\n"," 'া',\n"," 'ক্ট',\n"," 'স',\n"," '▁',\n"," '<mask>',\n"," '▁(',\n"," 'ব',\n"," 'ি',\n"," 'ড',\n"," 'ি',\n"," ')']"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["## Test it out\n","\n","test_text = \"গেল ইউনিটেক প্রডাক্টস <mask> (বিডি)\"\n","# test_text = normalize(\"গেল ইউনিটেক প্রডাক্টস (বিডি)\")\n","transformer_tokenizer.tokenize(test_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:19:56.237714Z","iopub.status.idle":"2024-04-02T09:19:56.238542Z","shell.execute_reply":"2024-04-02T09:19:56.238251Z","shell.execute_reply.started":"2024-04-02T09:19:56.238225Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","hf_token = user_secrets.get_secret(\"hf_token\")\n","\n","\n","from huggingface_hub import login\n","login(token= hf_token, add_to_git_credential=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:19:56.240420Z","iopub.status.idle":"2024-04-02T09:19:56.241446Z","shell.execute_reply":"2024-04-02T09:19:56.240901Z","shell.execute_reply.started":"2024-04-02T09:19:56.240875Z"},"trusted":true},"outputs":[],"source":["## Push the tokenizer to hugging face hub\n","\n","transformer_tokenizer.push_to_hub(\"Virus-Proton/CustomTokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["### \n","\n","def create_tokenizer():\n","    tokenizer = SentencePieceBPETokenizer()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4514808,"sourceId":7727419,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
