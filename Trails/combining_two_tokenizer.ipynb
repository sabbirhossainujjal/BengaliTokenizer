{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given two tokenizers, combine them and create a new tokenizer\n",
    "Usage: python combine_tokenizers.py --tokenizer1 ../config/en/roberta_8 --tokenizer2 ../config/hi/roberta_8 --save_dir ../config/en/en_hi/roberta_8\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Libraries for tokenizer\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from timeit import default_timer as timer\n",
    "import sys\n",
    "\n",
    "def combine_tokenizers(args):\n",
    "    # Load both the json files, take the union, and store it\n",
    "    json1 = json.load(open(os.path.join(args.tokenizer1, 'vocab.json')))\n",
    "    json2 = json.load(open(os.path.join(args.tokenizer2, 'vocab.json')))\n",
    "\n",
    "    # Create a new vocabulary\n",
    "    new_vocab = {}\n",
    "    idx = 0\n",
    "    for word in json1.keys():\n",
    "        if word not in new_vocab.keys():\n",
    "            new_vocab[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    # Add words from second tokenizer\n",
    "    for word in json2.keys():\n",
    "        if word not in new_vocab.keys():\n",
    "            new_vocab[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    # Make the directory if necessary\n",
    "    if not os.path.exists(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "\n",
    "    # Save the vocab\n",
    "    with open(os.path.join(args.save_dir, 'vocab.json'), 'w') as fp:\n",
    "        json.dump(new_vocab, fp, ensure_ascii=False)\n",
    "\n",
    "    # Merge the two merges file. Don't handle duplicates here\n",
    "    # Concatenate them, but ignore the first line of the second file\n",
    "    os.system('cat {} > {}'.format(os.path.join(args.tokenizer1, 'merges.txt'), os.path.join(args.save_dir, 'merges.txt')))\n",
    "    os.system('tail -n +2 -q {} >> {}'.format(os.path.join(args.tokenizer2, 'merges.txt'), os.path.join(args.save_dir, 'merges.txt')))\n",
    "\n",
    "    # Save other files\n",
    "    os.system('cp {} {}'.format(os.path.join(args.tokenizer1, 'config.json'), args.save_dir))\n",
    "    os.system('cp {} {}'.format(os.path.join(args.tokenizer1, 'tokenizer_config.json'), args.save_dir))\n",
    "\n",
    "    # Instantiate the new tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.save_dir, use_fast=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Dataset Arguments\n",
    "    parser.add_argument(\"--tokenizer1\", type=str, required=True, help=\"\")\n",
    "    parser.add_argument(\"--tokenizer2\", type=str, required=True, help=\"\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, required=True, help=\"\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    combine_tokenizers(args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_m_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
