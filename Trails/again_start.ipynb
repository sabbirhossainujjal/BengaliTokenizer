{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data('./demo_1M.txt')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bengali_word_iterator(text:str):\n",
    "    for line in text:\n",
    "        yield re.findall(r'\\S+', line, flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    bengali_word_iterator(text=text),\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "len(tokenizer.get_vocab().items())\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data('./demo_1M.txt')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bengali_word_iterator(text:str):\n",
    "    for line in text:\n",
    "        yield re.findall(r'\\S+', line, flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import SentencePieceBPETokenizer, Tokenizer, AddedToken, trainers\n",
    "\n",
    "# class SpBPETokenizerTest(SentencePieceBPETokenizer):\n",
    "#     def __init__(self, forbidden_start_chars=None):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.forbidden_start_chars = forbidden_start_chars or {'ৎ', 'ং', 'ঃ'}\n",
    "#         self.initial_alphabets = bangla_alphabets + conjunct_consonants\n",
    "            \n",
    "#     def train_from_iterator(\n",
    "#         self,\n",
    "#         iterator: Iterator[str],\n",
    "#         vocab_size: int = 30000,\n",
    "#         min_frequency: int = 5,\n",
    "#         show_progress: bool = True,\n",
    "#         special_tokens: List[str] = None,\n",
    "#         initial_alphabet: List[str] = None,\n",
    "#         limit_alphabet: int = None,\n",
    "#     ) -> None:\n",
    "#         # Create a new tokenizer\n",
    "#         self._tokenizer = Tokenizer(self._tokenizer.model)\n",
    "        \n",
    "#         # Prepare special tokens\n",
    "#         special_tokens = special_tokens or []\n",
    "        \n",
    "#         # Add initial alphabet to special tokens\n",
    "#         if initial_alphabet is not None:\n",
    "#             self.initial_alphabets = initial_alphabet\n",
    "#         special_tokens.extend(self.initial_alphabets)\n",
    "        \n",
    "#         # Remove duplicates while preserving order\n",
    "#         special_tokens = list(dict.fromkeys(special_tokens))\n",
    "        \n",
    "#         # Modify the iterator for whitespace issue\n",
    "#         iterator = bengali_word_iterator(text=iterator)\n",
    "        \n",
    "#         # Train the tokenizer\n",
    "#         trainer = trainers.BpeTrainer(\n",
    "#             vocab_size=vocab_size,\n",
    "#             min_frequency=min_frequency,\n",
    "#             special_tokens=special_tokens,\n",
    "#             limit_alphabet=limit_alphabet,\n",
    "#             # initial_alphabet=None,  # We've included initial_alphabet in special_tokens\n",
    "#             show_progress=show_progress,\n",
    "#         )\n",
    "        \n",
    "#         self._tokenizer.train_from_iterator(\n",
    "#             iterator,\n",
    "#             trainer=trainer,\n",
    "#         )\n",
    "        \n",
    "#         # Remove forbidden characters from the beginning of tokens\n",
    "#         vocab = self._tokenizer.get_vocab()\n",
    "#         filtered_vocab = {token: idx for token, idx in vocab.items()\n",
    "#                         if not any(token.startswith(char) for char in self.forbidden_start_chars)}\n",
    "        \n",
    "#         # Reset the vocabulary with the filtered tokens\n",
    "#         new_tokenizer = Tokenizer(self._tokenizer.model)\n",
    "#         for token, idx in filtered_vocab.items():\n",
    "#             if token in special_tokens:\n",
    "#                 new_tokenizer.add_special_tokens([token])\n",
    "#             else:\n",
    "#                 new_tokenizer.add_tokens([token])\n",
    "        \n",
    "#         self._tokenizer = new_tokenizer\n",
    "\n",
    "#     def get_vocab(self):\n",
    "#         return self._tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    # bengali_word_iterator(text=text),\n",
    "    text,\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "len(tokenizer.get_vocab().items())\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "class SpBPETokenizerTest:\n",
    "    def __init__(self, forbidden_start_chars=None):\n",
    "        self.forbidden_start_chars = forbidden_start_chars or {'ৎ', 'ং', 'ঃ'}\n",
    "        self.initial_alphabets = bangla_alphabets + conjunct_consonants\n",
    "        self._tokenizer = Tokenizer(BPE())\n",
    "        self._tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        iterator: Iterator[str],\n",
    "        vocab_size: int = 30000,\n",
    "        min_frequency: int = 5,\n",
    "        show_progress: bool = True,\n",
    "        special_tokens: List[str] = None,\n",
    "        initial_alphabet: List[str] = None,\n",
    "        limit_alphabet: int = None,\n",
    "    ) -> None:\n",
    "        # Prepare special tokens\n",
    "        special_tokens = special_tokens or []\n",
    "        \n",
    "        # Add initial alphabet to special tokens\n",
    "        if initial_alphabet is not None:\n",
    "            self.initial_alphabets = initial_alphabet\n",
    "        special_tokens.extend(self.initial_alphabets)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        special_tokens = list(dict.fromkeys(special_tokens))\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            special_tokens=special_tokens,\n",
    "            limit_alphabet=limit_alphabet,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "        \n",
    "        self._tokenizer.train_from_iterator(\n",
    "            iterator,\n",
    "            trainer=trainer,\n",
    "        )\n",
    "        \n",
    "        # Remove forbidden characters from the beginning of tokens\n",
    "        vocab = self._tokenizer.get_vocab()\n",
    "        filtered_vocab = {token: idx for token, idx in vocab.items()\n",
    "                          if not any(token.startswith(char) for char in self.forbidden_start_chars)}\n",
    "        \n",
    "        # Reset the vocabulary with the filtered tokens\n",
    "        new_tokenizer = Tokenizer(BPE())\n",
    "        new_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "        for token, idx in filtered_vocab.items():\n",
    "            if token in special_tokens:\n",
    "                new_tokenizer.add_special_tokens([token])\n",
    "            else:\n",
    "                new_tokenizer.add_tokens([token])\n",
    "        \n",
    "        self._tokenizer = new_tokenizer\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._tokenizer.get_vocab()\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self._tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self._tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Union, Iterator, Dict, Tuple\n",
    "\n",
    "class SpBPETokenizerTest:\n",
    "    def __init__(self, forbidden_start_chars=None):\n",
    "        self.forbidden_start_chars = forbidden_start_chars or {'ৎ', 'ং', 'ঃ'}\n",
    "        self.initial_alphabets = bangla_alphabets + conjunct_consonants\n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.inverse_vocab: Dict[int, str] = {}\n",
    "        self.merges: List[Tuple[str, str]] = []\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        text_or_iterator: Union[str, Iterator[str]],\n",
    "        vocab_size: int = 3000,\n",
    "        min_frequency: int = 2,\n",
    "        show_progress: bool = True,\n",
    "        special_tokens: List[str] = None,\n",
    "        initial_alphabet: List[str] = None,\n",
    "        limit_alphabet: int = None,\n",
    "    ) -> None:\n",
    "        special_tokens = special_tokens or []\n",
    "        if initial_alphabet is not None:\n",
    "            self.initial_alphabets = initial_alphabet\n",
    "        \n",
    "        # Add special tokens and initial alphabet to vocabulary\n",
    "        for token in special_tokens + self.initial_alphabets:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.inverse_vocab[len(self.vocab) - 1] = token\n",
    "\n",
    "        # Handle both string and iterator input\n",
    "        if isinstance(text_or_iterator, str):\n",
    "            text = text_or_iterator\n",
    "        else:\n",
    "            text = ' '.join(text_or_iterator)\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = re.findall(r'\\S+', text, flags=re.UNICODE)\n",
    "\n",
    "        # Count initial character pairs\n",
    "        char_pairs = defaultdict(int)\n",
    "        for word in words:\n",
    "            chars = list(word)\n",
    "            for i in range(len(chars) - 1):\n",
    "                char_pairs[chars[i], chars[i+1]] += 1\n",
    "\n",
    "        # Perform BPE merges\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            if not char_pairs:\n",
    "                break\n",
    "            best_pair = max(char_pairs, key=char_pairs.get)\n",
    "            if char_pairs[best_pair] < min_frequency:\n",
    "                break\n",
    "            \n",
    "            new_token = ''.join(best_pair)\n",
    "            if new_token in self.vocab or any(new_token.startswith(char) for char in self.forbidden_start_chars):\n",
    "                del char_pairs[best_pair]\n",
    "                continue\n",
    "\n",
    "            self.merges.append(best_pair)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.inverse_vocab[len(self.vocab) - 1] = new_token\n",
    "\n",
    "            # Update character pairs\n",
    "            new_pairs = defaultdict(int)\n",
    "            for word in words:\n",
    "                new_word = word\n",
    "                i = 0\n",
    "                while i < len(new_word) - 1:\n",
    "                    if (new_word[i], new_word[i+1]) == best_pair:\n",
    "                        if i > 0:\n",
    "                            new_pairs[(new_word[i-1], new_token)] += 1\n",
    "                        if i < len(new_word) - 2:\n",
    "                            new_pairs[(new_token, new_word[i+2])] += 1\n",
    "                        new_word = new_word[:i] + new_token + new_word[i+2:]\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        if i < len(new_word) - 2:\n",
    "                            new_pairs[(new_word[i], new_word[i+1])] += 1\n",
    "                        i += 1\n",
    "                words[words.index(word)] = new_word\n",
    "\n",
    "            char_pairs = new_pairs\n",
    "\n",
    "            if show_progress and len(self.vocab) % 100 == 0:\n",
    "                print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return self.vocab\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        words = re.findall(r'\\S+', text, flags=re.UNICODE)\n",
    "        encoded = []\n",
    "        for word in words:\n",
    "            for merge in self.merges:\n",
    "                word = word.replace(''.join(merge), ''.join(merge).replace(' ', ''))\n",
    "            encoded.extend([self.vocab.get(char, self.vocab['<unk>']) for char in word])\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return ''.join(self.inverse_vocab.get(id, '<unk>') for id in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpBPETokenizerTest()\n",
    "tokenizer.train_from_iterator(\n",
    "    [text],  # Wrap your text in a list\n",
    "    vocab_size=3000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "print(\"Sample vocabulary items:\")\n",
    "for i, (token, id) in enumerate(vocab.items()):\n",
    "    print(f\"{token}: {id}\")\n",
    "    if i >= 20:  # Print first 20 items\n",
    "        break\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"আমি বাংলায় কথা বলি\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"আমি বাংলায় কথা বলি\"), len([16, 50, 63, 48, 62, 59, 53, 62, 51, 73, 26, 42, 62, 48, 53, 63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])\n",
    "# tokenizer.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = SpBPETokenizer()\n",
    "tokenizer = SpBPETokenizerTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens([\"নির্জন\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    bengali_word_iterator(text=text),\n",
    "    # text,\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens(['আপনার', 'মধ্যে'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer.get_vocab().items(), key=lambda kv:kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"বন্ধু মধ্যে\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_model('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once Again from the ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "from typing import List, Union, Iterator\n",
    "\n",
    "class BengaliTokenizer:\n",
    "    def __init__(self, model_prefix: str = \"bengali_tokenizer\"):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.sp_model = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        text_or_iterator: Union[str, Iterator[str]],\n",
    "        vocab_size: int = 3000,\n",
    "        model_type: str = \"bpe\",\n",
    "        special_tokens: List[str] = None,\n",
    "        initial_tokens: List[str] = None,\n",
    "        character_coverage: float = 1.0,\n",
    "        max_sentence_length: int = 4192,\n",
    "    ):\n",
    "        # Prepare the input text file\n",
    "        if isinstance(text_or_iterator, str):\n",
    "            with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text_or_iterator)\n",
    "        else:\n",
    "            with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for line in text_or_iterator:\n",
    "                    f.write(line + \"\\n\")\n",
    "\n",
    "        # Prepare user defined symbols (special tokens and initial tokens)\n",
    "        special_tokens = special_tokens or []\n",
    "        initial_tokens = initial_tokens or []\n",
    "        predefined_tokens = {\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"}\n",
    "        user_defined_symbols = [token for token in special_tokens + initial_tokens if token not in predefined_tokens]\n",
    "        control_symbols = [token for token in special_tokens if token not in predefined_tokens]\n",
    "\n",
    "        # Train the SentencePiece model\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=\"temp_input.txt\",\n",
    "            model_prefix=self.model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type=model_type,\n",
    "            character_coverage=character_coverage,\n",
    "            max_sentence_length=max_sentence_length,\n",
    "            pad_id=0,\n",
    "            bos_id=1,\n",
    "            eos_id=2,\n",
    "            unk_id=3,\n",
    "            user_defined_symbols=user_defined_symbols,\n",
    "            control_symbols=control_symbols,\n",
    "            input_sentence_size=10000000,\n",
    "            shuffle_input_sentence=True,\n",
    "            treat_whitespace_as_suffix=True,  # This ensures no space in tokens\n",
    "        )\n",
    "\n",
    "        # Load the trained model\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.load(f\"{self.model_prefix}.model\")\n",
    "\n",
    "        # Clean up temporary file\n",
    "        os.remove(\"temp_input.txt\")\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return self.sp_model.encode(text, out_type=int)\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return self.sp_model.decode(ids)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return {self.sp_model.id_to_piece(i): i for i in range(self.sp_model.get_piece_size())}\n",
    "\n",
    "    def token_to_id(self, token: str) -> int:\n",
    "        return self.sp_model.piece_to_id(token)\n",
    "\n",
    "    def id_to_token(self, id: int) -> str:\n",
    "        return self.sp_model.id_to_piece(id)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        self.sp_model.save(path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BengaliTokenizer()\n",
    "\n",
    "# Train the tokenizer\n",
    "special_tokens = [\"[mask]\"]  # Removed \"<unk>\"\n",
    "initial_tokens = [\"আ\", \"ই\", \"উ\", \"ঊ\", \"ঋ\", \"এ\", \"ঐ\", \"ও\", \"ঔ\", \"ক\", \"খ\", \"গ\", \"ঘ\", \"ঙ\"]  # Add your Bengali initial tokens here\n",
    "\n",
    "tokenizer.train(\n",
    "    text,  # Your Bengali text or iterator of texts\n",
    "    vocab_size=3000,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_tokens=initial_tokens,\n",
    ")\n",
    "\n",
    "# Check the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Sample vocabulary items:\")\n",
    "for i, (token, id) in enumerate(vocab.items()):\n",
    "    print(f\"{token}: {id}\")\n",
    "    if i >= 20:\n",
    "        break\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"আমি বাংলায় কথা বলি\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Save the model\n",
    "tokenizer.save_model(\"bengali_tokenizer.model\")\n",
    "\n",
    "# Load the model (you can do this in a separate script)\n",
    "# tokenizer = BengaliTokenizer()\n",
    "# tokenizer.load_model(\"bengali_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Define special tokens and initial tokens\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "initial_tokens = [\"অ\", \"আ\", \"ই\", \"ঈ\", \"উ\", \"ঊ\", \"ঋ\", \"এ\", \"ঐ\", \"ও\", \"ঔ\"] # Add more initial tokens as needed\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "# Train the tokenizer on your corpus\n",
    "tokenizer.train(\n",
    "    files=[\"demo_1M.txt\"],\n",
    "    vocab_size=30000,  # Adjust vocab size as needed\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=initial_tokens\n",
    ")\n",
    "\n",
    "# Save the tokenizer model\n",
    "tokenizer.save_model(\"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "# Your custom vocabulary list (Bangla words)\n",
    "custom_vocab = [\"বাংলা\", \"শব্দ\", \"তালিকা\"]  # Add your Bangla vocabulary here\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# Calculate the target vocabulary size\n",
    "target_vocab_size = 1000 + 35\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=text,\n",
    "    vocab_size=target_vocab_size,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "# Get the current vocabulary\n",
    "current_vocab = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "# Add custom vocabulary\n",
    "for word in custom_vocab:\n",
    "    if word not in current_vocab:\n",
    "        tokenizer.add_tokens([word])\n",
    "\n",
    "# Trim vocabulary if it exceeds the target size\n",
    "while len(tokenizer.get_vocab()) > target_vocab_size:\n",
    "    # Remove the least frequent token that's not in special_tokens or custom_vocab\n",
    "    vocab_with_counts = tokenizer.get_vocab(with_added_tokens=True)\n",
    "    sorted_vocab = sorted(vocab_with_counts.items(), key=lambda x: x[1])\n",
    "    for token, _ in sorted_vocab:\n",
    "        if token not in special_tokens and token not in custom_vocab:\n",
    "            tokenizer.token_to_id.pop(token, None)\n",
    "            break\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = \"./custom_tokenizer\"\n",
    "tokenizer.save_model(tokenizer_save_path)\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"গেল ইউনিটেক প্রডাক্টস (বিডি)\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(encoded.tokens)\n",
    "\n",
    "# # Verify custom vocabulary is included\n",
    "# for word in custom_vocab:\n",
    "#     print(f\"{word}: {tokenizer.token_to_id.get(word, 'Not in vocabulary')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer\n",
    "test_text = \"গেল ইউনিটেক প্রডাক্টস (বিডি)\"\n",
    "tokenizer.encode(test_text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# Step 1: Define your special tokens and predefined tokens separately\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "predefined_tokens = [\"বাংলা\", \"ভাষা\", \"টোকেনাইজার\"]  # Your predefined Bengali tokens\n",
    "\n",
    "# Step 2: Initialize the tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Step 3: Define the trainer\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=[]  # We'll let it learn the alphabet from the data\n",
    ")\n",
    "\n",
    "# Step 4: Define pre-tokenizer\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Step 5: Add predefined tokens to the tokenizer's vocabulary\n",
    "tokenizer.add_tokens(predefined_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the tokenizer\n",
    "files = [\"./demo_1M.txt\"]  # Add your Bengali text files here\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Step 7: Save the tokenizer\n",
    "tokenizer.save(\"./custom_tokenizer/bengali_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tokenizer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "# from typing import List, Optional\n",
    "\n",
    "# class BengaliTokenizer:\n",
    "#     def __init__(self, special_tokens: List[str], predefined_tokens: List[str]):\n",
    "#         self.special_tokens = special_tokens\n",
    "#         self.predefined_tokens = predefined_tokens\n",
    "#         self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#         self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "#     def train(self, files: List[str], vocab_size: Optional[int] = 30000):\n",
    "#         trainer = BpeTrainer(\n",
    "#             special_tokens=self.special_tokens + self.predefined_tokens,\n",
    "#             initial_alphabet=self.predefined_tokens,\n",
    "#             vocab_size=vocab_size\n",
    "#         )\n",
    "#         self.tokenizer.train(files, trainer)\n",
    "\n",
    "#     def save(self, path: str):\n",
    "#         self.tokenizer.save(path)\n",
    "        \n",
    "#     def encode(self, text: str):\n",
    "#         return self.tokenizer.encode(text)\n",
    "#     @classmethod\n",
    "#     def from_file(cls, path: str):\n",
    "#         tokenizer = Tokenizer.from_file(path)\n",
    "#         return cls([], [])  # Initialize with empty lists as we don't store these separately\n",
    "\n",
    "#     # def validate_predefined_tokens(self):\n",
    "#     #     vocab = self.tokenizer.get_vocab()\n",
    "#     #     missing_tokens = [token for token in self.predefined_tokens if token not in vocab]\n",
    "#     #     if missing_tokens:\n",
    "#     #         print(f\"Warning: The following predefined tokens are missing from the vocabulary: {missing_tokens}\")\n",
    "#     #     else:\n",
    "#     #         print(\"All predefined tokens are present in the vocabulary.\")\n",
    "        \n",
    "#     #     # Print the IDs of predefined tokens\n",
    "#     #     for token in self.predefined_tokens:\n",
    "#     #         if token in vocab:\n",
    "#     #             print(f\"Token: {token}, ID: {vocab[token]}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Usage example\n",
    "# special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<mask>\"]\n",
    "# predefined_tokens = bangla_alphabets + conjunct_consonants #[\"বাংলা\", \"ভাষা\", \"tokenizer\"]  # Example Bengali tokens\n",
    "\n",
    "# # Create and train the tokenizer\n",
    "# bengali_tokenizer = BengaliTokenizer(special_tokens, predefined_tokens)\n",
    "# files = [\"./demo_1M.txt\"]\n",
    "# bengali_tokenizer.train(files)\n",
    "\n",
    "# # Validate that predefined tokens are added\n",
    "# # bengali_tokenizer.validate_predefined_tokens()\n",
    "\n",
    "# # Save the tokenizer\n",
    "# bengali_tokenizer.save(\"bengali_tokenizer.json\")\n",
    "\n",
    "# # Load the tokenizer\n",
    "# loaded_tokenizer = BengaliTokenizer.from_file(\"bengali_tokenizer.json\")\n",
    "\n",
    "# # Test the tokenizer\n",
    "# bengali_text = \"আপনার বাংলা ভाষা tokenizer এখানে\"\n",
    "# encoded = loaded_tokenizer.encode(bengali_text)\n",
    "\n",
    "# print(\"Tokens:\", encoded.tokens)\n",
    "# print(\"IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['আপনার', 'বা', 'ং', 'লা', 'ভাষা', 'to', 'ken', 'iz', 'er', 'এখানে']\n",
      "IDs: [685, 221, 112, 237, 4785, 2065, 6761, 8652, 683, 1229]\n"
     ]
    }
   ],
   "source": [
    "## currently working\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from typing import List, Optional\n",
    "\n",
    "class BengaliTokenizer:\n",
    "    def __init__(self, special_tokens: List[str], predefined_tokens: List[str]):\n",
    "        self.special_tokens = special_tokens\n",
    "        self.predefined_tokens = predefined_tokens\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files: List[str], vocab_size: Optional[int] = 30000):\n",
    "        # Calculate the remaining vocab size after accounting for special and predefined tokens\n",
    "        remaining_vocab_size = vocab_size - len(self.special_tokens) - len(self.predefined_tokens)\n",
    "        \n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=remaining_vocab_size,\n",
    "            special_tokens=self.special_tokens,\n",
    "            initial_alphabet=self.predefined_tokens\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        self.tokenizer.train(files, trainer)\n",
    "        \n",
    "        # Add special tokens and predefined tokens to ensure they're in the vocabulary\n",
    "        all_tokens = self.special_tokens + self.predefined_tokens\n",
    "        new_tokens = [token for token in all_tokens if token not in self.tokenizer.get_vocab()]\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        self.tokenizer.save(path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path: str):\n",
    "        tokenizer = Tokenizer.from_file(path)\n",
    "        instance = cls([], [])\n",
    "        instance.tokenizer = tokenizer\n",
    "        return instance\n",
    "\n",
    "    def validate_tokens(self):\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        all_tokens = self.special_tokens + self.predefined_tokens\n",
    "        \n",
    "        print(\"Special and Predefined Tokens:\")\n",
    "        for token in all_tokens:\n",
    "            if token in vocab:\n",
    "                print(f\"Token: {token}, ID: {vocab[token]}\")\n",
    "            else:\n",
    "                print(f\"Warning: Token '{token}' not found in vocabulary\")\n",
    "\n",
    "        print(\"\\nFull Vocabulary:\")\n",
    "        sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "        for token, id in sorted_vocab:\n",
    "            print(f\"Token: {token}, ID: {id}\")\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "# Usage example\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "predefined_tokens = bangla_alphabets + conjunct_consonants\n",
    "\n",
    "\n",
    "# Create and train the tokenizer\n",
    "bengali_tokenizer = BengaliTokenizer(special_tokens, predefined_tokens)\n",
    "files = [\"demo_1M.txt\"]\n",
    "bengali_tokenizer.train(files)\n",
    "\n",
    "# Validate tokens\n",
    "# bengali_tokenizer.validate_tokens()\n",
    "\n",
    "# Save the tokenizer\n",
    "bengali_tokenizer.save(\"bengali_tokenizer.json\")\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = BengaliTokenizer.from_file(\"bengali_tokenizer.json\")\n",
    "\n",
    "# Test the tokenizer\n",
    "bengali_text = \"আপনার বাংলা ভাষা tokenizer এখানে\"\n",
    "encoded = loaded_tokenizer.encode(bengali_text)\n",
    "\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25388\n"
     ]
    }
   ],
   "source": [
    "# print(len(bengali_tokenizer.tokenizer.get_vocab()))\n",
    "vocab = bengali_tokenizer.tokenizer.get_vocab()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('·', 100),\n",
       " ('»', 101),\n",
       " ('¿', 102),\n",
       " ('Ñ', 103),\n",
       " ('í', 104),\n",
       " ('ó', 105),\n",
       " ('ú', 106),\n",
       " ('œ', 107),\n",
       " ('Ÿ', 108),\n",
       " ('।', 109),\n",
       " ('॥', 110),\n",
       " ('ঁ', 111),\n",
       " ('ং', 112),\n",
       " ('ঃ', 113),\n",
       " ('অ', 114),\n",
       " ('আ', 115),\n",
       " ('ই', 116),\n",
       " ('ঈ', 117),\n",
       " ('উ', 118),\n",
       " ('ঊ', 119),\n",
       " ('ঋ', 120),\n",
       " ('এ', 121),\n",
       " ('ঐ', 122),\n",
       " ('ও', 123),\n",
       " ('ঔ', 124),\n",
       " ('ক', 125),\n",
       " ('খ', 126),\n",
       " ('গ', 127),\n",
       " ('ঘ', 128),\n",
       " ('ঙ', 129),\n",
       " ('চ', 130),\n",
       " ('ছ', 131),\n",
       " ('জ', 132),\n",
       " ('ঝ', 133),\n",
       " ('ঞ', 134),\n",
       " ('ট', 135),\n",
       " ('ঠ', 136),\n",
       " ('ড', 137),\n",
       " ('ঢ', 138),\n",
       " ('ণ', 139),\n",
       " ('ত', 140),\n",
       " ('থ', 141),\n",
       " ('দ', 142),\n",
       " ('ধ', 143),\n",
       " ('ন', 144),\n",
       " ('প', 145),\n",
       " ('ফ', 146),\n",
       " ('ব', 147),\n",
       " ('ভ', 148),\n",
       " ('ম', 149),\n",
       " ('য', 150),\n",
       " ('র', 151),\n",
       " ('ল', 152),\n",
       " ('শ', 153),\n",
       " ('ষ', 154),\n",
       " ('স', 155),\n",
       " ('হ', 156),\n",
       " ('়', 157),\n",
       " ('া', 158),\n",
       " ('ি', 159),\n",
       " ('ী', 160),\n",
       " ('ু', 161),\n",
       " ('ূ', 162),\n",
       " ('ৃ', 163),\n",
       " ('ে', 164),\n",
       " ('ৈ', 165),\n",
       " ('ো', 166),\n",
       " ('ৌ', 167),\n",
       " ('্', 168),\n",
       " ('ৎ', 169),\n",
       " ('ৗ', 170),\n",
       " ('ড়', 171),\n",
       " ('ঢ়', 172),\n",
       " ('য়', 173),\n",
       " ('০', 174),\n",
       " ('১', 175),\n",
       " ('২', 176),\n",
       " ('৩', 177),\n",
       " ('৪', 178),\n",
       " ('৫', 179),\n",
       " ('৬', 180),\n",
       " ('৭', 181),\n",
       " ('৮', 182),\n",
       " ('৯', 183),\n",
       " ('৳', 184),\n",
       " ('৷', 185),\n",
       " ('\\u200b', 186),\n",
       " ('\\u200c', 187),\n",
       " ('\\u200d', 188),\n",
       " ('–', 189),\n",
       " ('—', 190),\n",
       " ('‘', 191),\n",
       " ('’', 192),\n",
       " ('“', 193),\n",
       " ('”', 194),\n",
       " ('•', 195),\n",
       " ('…', 196),\n",
       " ('›', 197),\n",
       " ('€', 198),\n",
       " ('℃', 199)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomBPETokenizer at 0x7ccde45b7160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [685, 1275, 860, 215, 1229, 2437, 324, 109], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer JSON file\n",
    "with open(\"bengali_tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "\n",
    "# Save the JSON content to a temporary file\n",
    "temp_json_path = \"temp_tokenizer.json\"\n",
    "with open(temp_json_path, \"w\", encoding=\"utf-8\") as temp_f:\n",
    "    json.dump(tokenizer_json, temp_f)\n",
    "\n",
    "# Create a PreTrainedTokenizerFast instance from the JSON file\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=temp_json_path)\n",
    "\n",
    "# Save the tokenizer in Hugging Face format\n",
    "# tokenizer.save_pretrained(\"path/to/save/hf_tokenizer\")\n",
    "\n",
    "# Example usage\n",
    "encoded = tokenizer.encode_plus(\"আপনার লেখা পাঠ্য এখানে লিখুন।\")\n",
    "print(encoded)\n",
    "\n",
    "# decoded = tokenizer.decode(encoded)\n",
    "# print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আপনার', 'লেখা', 'পাঠ', '্য', 'এখানে', 'লিখ', 'ুন', '।']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(\"আপনার লেখা পাঠ্য এখানে লিখুন।\")\n",
    "print(encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "class CustomBPETokenizer:\n",
    "    def __init__(self, vocab_size=30000, special_tokens=None, predefined_tokens=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = special_tokens if special_tokens is not None else [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "        self.predefined_tokens = predefined_tokens if predefined_tokens is not None else []\n",
    "        \n",
    "        self.tokenizer = Tokenizer(models.BPE())\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        self.tokenizer.decoder = decoders.ByteLevel()\n",
    "        self.tokenizer.post_processor = processors.ByteLevel()\n",
    "        \n",
    "    def train(self, files):\n",
    "        # Add predefined tokens as initial alphabet\n",
    "        initial_alphabet = list(set([token for token in self.predefined_tokens]))\n",
    "        \n",
    "        # Create BPE Trainer with initial alphabet and special tokens\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            special_tokens=self.special_tokens,\n",
    "            initial_alphabet=initial_alphabet\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        self.tokenizer.train(files, trainer)\n",
    "        \n",
    "    def save(self, path):\n",
    "        self.tokenizer.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "    predefined_tokens = [\"অ\", \"আ\", \"ই\", \"ঈ\", \"উ\", \"ঊ\", \"ঋ\", \"এ\", \"ঐ\", \"ও\", \"ঔ\", \n",
    "                        \"ক\", \"খ\", \"গ\", \"ঘ\", \"ঙ\", \"চ\", \"ছ\", \"জ\", \"ঝ\", \"ঞ\", \n",
    "                        \"ট\", \"ঠ\", \"ড\", \"ঢ\", \"ণ\", \"ত\", \"থ\", \"দ\", \"ধ\", \"ন\", \n",
    "                        \"প\", \"ফ\", \"ব\", \"ভ\", \"ম\", \"য\", \"র\", \"ল\", \"শ\", \"ষ\", \"স\", \"হ\",\n",
    "                        \"ড়\", \"ঢ়\", \"য়\", \"ৎ\", \"ং\", \"ঃ\", \"ঁ\"]  # Add more initial tokens as needed\n",
    "\n",
    "    files = [\"demo_1M.txt\"]\n",
    "    tokenizer = CustomBPETokenizer(vocab_size=30000, special_tokens=special_tokens, predefined_tokens=predefined_tokens)\n",
    "    tokenizer.train(files)\n",
    "    tokenizer.save(\"./tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from custom_tokenizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Checking all special and predefined tokens existance:\n",
      "All predifined and special tokens were found in the tokenizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "initial_tokens = bangla_alphabets + conjunct_consonants\n",
    "\n",
    "tokenizer = BengaliBPETokenizer()\n",
    "files = [\"demo_1M.txt\"]\n",
    "hf_tokenizer = tokenizer.train_tokenizer(\n",
    "    files=files,\n",
    "    vocab_size=30_000,\n",
    "    special_tokens= [\n",
    "        \n",
    "    ],\n",
    "    initial_tokens= initial_tokens,\n",
    "    min_frequency=2,\n",
    "    limit_alphabet=500,\n",
    "    show_progress=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "tokenizer.validate_predefined_tokens()\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"bengali_tokenizer_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আপনার', 'বা', 'ং', 'লা', 'ভাষা', 'to', 'ken', 'iz', 'er', 'এখানে']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bengali_text = \"আপনার বাংলা ভাষা tokenizer এখানে\"\n",
    "hf_tokenizer.encode_plus(bengali_text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7/07/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary saved to test_vocab.json\n",
      "\n",
      "\n",
      "\n",
      "SentencePieceBPE tokenizer training completed and saved to merge.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Step 1: Create vocab.json from a list of tokens\n",
    "def create_vocab_json(tokens, output_file='vocab.json'):\n",
    "    vocab = {token: i for i, token in enumerate(tokens)}\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Vocabulary saved to {output_file}\")\n",
    "\n",
    "# List of tokens (replace with your actual tokens)\n",
    "token_list = [\n",
    "    \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\",\n",
    "    \"ক্ক\",\n",
    "    \"ক্ট\",\n",
    "    \"ক্ট্র\",\n",
    "    \"ক্ত\",\n",
    "    \"ক্ত্র\",\n",
    "    \"ক্ন\",\n",
    "    \"ক্ব\",\n",
    "    \"ক্ম\",\n",
    "    \"ক্য\",\n",
    "]\n",
    "\n",
    "# Create vocab.json\n",
    "create_vocab_json(token_list, output_file='test_vocab.json')\n",
    "\n",
    "# Step 2: Load the created vocab.json\n",
    "with open('test_vocab.json', 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Step 3: Initialize the SentencePieceBPE tokenizer with the loaded vocabulary\n",
    "tokenizer = SentencePieceBPETokenizer(vocab=vocab)\n",
    "\n",
    "# Step 4: Define training parameters and train on new data\n",
    "trainer = tokenizer.train(\n",
    "    files=[\"/home/virus_proton/Projects/P_Projects/LLM_Mastery/Tokenizer_train/demo_1M.txt\"],  # Replace with your data file path\n",
    "    vocab_size=len(vocab),  # Keep the same vocab size\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Step 5: Save the updated tokenizer\n",
    "tokenizer.save(\"merge.txt\")\n",
    "\n",
    "print(\"SentencePieceBPE tokenizer training completed and saved to merge.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"/home/virus_proton/Projects/P_Projects/LLM_Mastery/Tokenizer_train/demo_1M.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "New SentencePieceBPE tokenizer with both old and new tokens has been trained and saved.\n",
      "New vocabulary saved to new_vocab.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Step 1: Load the existing vocab.json\n",
    "with open('test_vocab.json', 'r', encoding='utf-8') as f:\n",
    "    existing_vocab = json.load(f)\n",
    "\n",
    "# Step 2: Initialize the SentencePieceBPE tokenizer with the loaded vocabulary\n",
    "tokenizer = SentencePieceBPETokenizer(vocab=existing_vocab)\n",
    "\n",
    "# Step 3: Define training parameters and train on new data\n",
    "tokenizer.train(\n",
    "    files=[text_path],  # Replace with your new data file path\n",
    "    vocab_size=len(existing_vocab) + 1000,  # Increase vocab size to accommodate new tokens\n",
    "    min_frequency=2,\n",
    "    special_tokens=list(existing_vocab.keys())[:5],  # Assume first 5 tokens are special\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Step 4: Save the updated tokenizer\n",
    "tokenizer.save_model(\".\", \"new_tokenizer\")\n",
    "\n",
    "print(\"New SentencePieceBPE tokenizer with both old and new tokens has been trained and saved.\")\n",
    "\n",
    "# Optionally, you can save the new vocabulary separately\n",
    "new_vocab = tokenizer.get_vocab()\n",
    "with open('new_vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"New vocabulary saved to new_vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_m_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
