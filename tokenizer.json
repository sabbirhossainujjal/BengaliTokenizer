{
  "version": "1.0",
  "truncation": null,
  "padding": null,
  "added_tokens": [
    {
      "id": 0,
      "content": "<unk>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 1,
      "content": "<pad>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 2,
      "content": "<s>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 3,
      "content": "</s>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    },
    {
      "id": 4,
      "content": "<mask>",
      "single_word": false,
      "lstrip": false,
      "rstrip": false,
      "normalized": false,
      "special": true
    }
  ],
  "normalizer": {
    "type": "Sequence",
    "normalizers": [
      {
        "type": "NFC"
      }
    ]
  },
  "pre_tokenizer": {
    "type": "Sequence",
    "pretokenizers": [
      {
        "type": "Metaspace",
        "replacement": "â–",
        "prepend_scheme": "always",
        "split": true
      }
    ]
  },
  "post_processor": {
    "type": "TemplateProcessing",
    "single": [
      {
        "SpecialToken": {
          "id": "<s>",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "A",
          "type_id": 0
        }
      },
      {
        "SpecialToken": {
          "id": "</s>",
          "type_id": 0
        }
      }
    ],
    "pair": [
      {
        "Sequence": {
          "id": "A",
          "type_id": 0
        }
      },
      {
        "Sequence": {
          "id": "B",
          "type_id": 1
        }
      }
    ],
    "special_tokens": {
      "</s>": {
        "id": "</s>",
        "ids": [
          3
        ],
        "tokens": [
          "</s>"
        ]
      },
      "<s>": {
        "id": "<s>",
        "ids": [
          2
        ],
        "tokens": [
          "<s>"
        ]
      }
    }
  },
  "decoder": {
    "type": "Metaspace",
    "replacement": "_",
    "prepend_scheme": "always",
    "split": true
  },
  "model": {
    "type": "BPE",
    "dropout": null,
    "unk_token": "<unk>",
    "continuing_subword_prefix": null,
    "end_of_word_suffix": null,
    "fuse_unk": false,
    "byte_fallback": false,
    "ignore_merges": false,
    "vocab": {
      "<unk>": 0,
      "<pad>": 1,
      "<s>": 2,
      "</s>": 3,
      "<mask>": 4,
      "\n": 5,
      "!": 6,
      "\"": 7,
      "#": 8,
      "'": 9,
      "(": 10,
      ")": 11,
      "+": 12,
      ",": 13,
      "-": 14,
      ".": 15,
      "/": 16,
      "0": 17,
      "1": 18,
      "2": 19,
      "3": 20,
      "4": 21,
      "5": 22,
      "6": 23,
      "7": 24,
      "8": 25,
      "9": 26,
      ":": 27,
      ";": 28,
      "=": 29,
      "?": 30,
      "@": 31,
      "A": 32,
      "B": 33,
      "C": 34,
      "D": 35,
      "E": 36,
      "F": 37,
      "G": 38,
      "H": 39,
      "I": 40,
      "K": 41,
      "L": 42,
      "M": 43,
      "N": 44,
      "O": 45,
      "P": 46,
      "Q": 47,
      "R": 48,
      "S": 49,
      "T": 50,
      "U": 51,
      "W": 52,
      "X": 53,
      "Y": 54,
      "[": 55,
      "]": 56,
      "_": 57,
      "`": 58,
      "a": 59,
      "b": 60,
      "c": 61,
      "d": 62,
      "e": 63,
      "f": 64,
      "g": 65,
      "h": 66,
      "i": 67,
      "j": 68,
      "k": 69,
      "l": 70,
      "m": 71,
      "n": 72,
      "o": 73,
      "p": 74,
      "q": 75,
      "r": 76,
      "s": 77,
      "t": 78,
      "u": 79,
      "v": 80,
      "w": 81,
      "x": 82,
      "y": 83,
      "z": 84,
      "{": 85,
      "}": 86,
      "ÄŠ": 87,
      "Ä ": 88,
      "à¦•": 89,
      "à¦Ÿ": 90,
      "à¦¤": 91,
      "à¦¨": 92,
      "à¦¬": 93,
      "à¦®": 94,
      "à¦¯": 95,
      "à¦°": 96,
      "à§": 97,
      "â€”": 98,
      "â€™": 99,
      "â€œ": 100,
      "â€": 101,
      "â†": 102,
      "â†’": 103,
      "â–": 104,
      "âš ": 105,
      "ï¸": 106,
      "ğŸ¤—": 107,
      "â–t": 108,
      "in": 109,
      "â–a": 110,
      "en": 111,
      "er": 112,
      "â–th": 113,
      "',": 114,
      "â–to": 115,
      "â–'": 116,
      "at": 117,
      "â–w": 118,
      "or": 119,
      "on": 120,
      "se": 121,
      "iz": 122,
      "ing": 123,
      "ken": 124,
      "ra": 125,
      "â–the": 126,
      "keniz": 127,
      "ou": 128,
      "â–c": 129,
      "â–f": 130,
      "â–l": 131,
      "st": 132,
      "re": 133,
      "kenizer": 134,
      "â–\n": 135,
      "â–s": 136,
      "â–in": 137,
      "it": 138,
      "de": 139,
      "an": 140,
      "â–o": 141,
      "â–b": 142,
      "â–tokenizer": 143,
      "ll": 144,
      "rain": 145,
      "is": 146,
      "â–n": 147,
      "â–an": 148,
      "â–'Ä ": 149,
      "â–y": 150,
      "â–T": 151,
      "â–you": 152,
      "me": 153,
      "le": 154,
      "ce": 155,
      "â–of": 156,
      "ch": 157,
      "ion": 158,
      "ts": 159,
      "ex": 160,
      "pu": 161,
      "ge": 162,
      "ro": 163,
      "th": 164,
      "Ä Ä ": 165,
      "â–d": 166,
      "ata": 167,
      "â–we": 168,
      "â–on": 169,
      "ode": 170,
      "atase": 171,
      "â–p": 172,
      "â–u": 173,
      ":\n": 174,
      "ar": 175,
      "ct": 176,
      "ed": 177,
      "â–h": 178,
      "â–lo": 179,
      "to": 180,
      "â–m": 181,
      "â–ne": 182,
      "â–and": 183,
      "â–se": 184,
      "â–for": 185,
      "raining": 186,
      "mp": 187,
      "â–that": 188,
      ")\n": 189,
      "un": 190,
      "â–wh": 191,
      "â–re": 192,
      "â–Th": 193,
      "rom": 194,
      "00": 195,
      "â–li": 196,
      "pus": 197,
      "â–is": 198,
      "â–ex": 199,
      "\"\"": 200,
      "ame": 201,
      "ill": 202,
      "ent": 203,
      "ation": 204,
      "orpus": 205,
      "â–can": 206,
      "es": 207,
      "ua": 208,
      "â–1": 209,
      "â–g": 210,
      "â–i": 211,
      "â–sp": 212,
      "â–be": 213,
      "â–your": 214,
      "â–new": 215,
      "ad": 216,
      "ld": 217,
      "train": 218,
      "yth": 219,
      "â–will": 220,
      "â–use": 221,
      "mple": 222,
      "ly": 223,
      "ve": 224,
      "ran": 225,
      "ith": 226,
      "ataset": 227,
      ".\n": 228,
      "et": 229,
      "â–de": 230,
      "â–from": 231,
      "as": 232,
      "ace": 233,
      "ample": 234,
      "ow": 235,
      "t_": 236,
      "â–=": 237,
      "â–e": 238,
      "â–it": 239,
      "our": 240,
      "odel": 241,
      "\"]": 242,
      "[\"": 243,
      "_d": 244,
      "al": 245,
      "ke": 246,
      "lf": 247,
      "ot": 248,
      "â–A": 249,
      "â–training": 250,
      "ere": 251,
      "â–this": 252,
      "atasets": 253,
      "_c": 254,
      "ast": 255,
      "ec": 256,
      "fun": 257,
      "gua": 258,
      "ur": 259,
      "ut": 260,
      "â€™s": 261,
      "â–P": 262,
      "â–at": 263,
      "â–are": 264,
      "erat": 265,
      "â–corpus": 266,
      "angua": 267,
      "ction": 268,
      "â–model": 269,
      "func": 270,
      "erator": 271,
      "anguage": 272,
      "ic": 273,
      "â–C": 274,
      "â–I": 275,
      "ine": 276,
      "ers": 277,
      "â–token": 278,
      "â–with": 279,
      "â–language": 280,
      "â–dataset": 281,
      "ython": 282,
      "all": 283,
      "ave": 284,
      "ok": 285,
      "ver": 286,
      "â–\"": 287,
      "â–me": 288,
      "â–tex": 289,
      "â–al": 290,
      "â–as": 291,
      "â–co": 292,
      "â–whi": 293,
      "â–Python": 294,
      "()": 295,
      "mb": 296,
      "okenizer": 297,
      "pt": 298,
      "ring": 299,
      "â–(": 300,
      "ize": 301,
      "string": 302,
      "â–see": 303,
      "â–This": 304,
      "000": 305,
      "urn": 306,
      "â–texts": 307,
      "â–which": 308,
      "_n": 309,
      "ay": 310,
      "for": 311,
      "id": 312,
      "ir": 313,
      "lo": 314,
      "ss": 315,
      "size": 316,
      "turn": 317,
      "w_d": 318,
      "ÄŠÄ Ä ": 319,
      "â–F": 320,
      "â–ğŸ¤—": 321,
      "â–train": 322,
      "â–tokeniz": 323,
      "raw_d": 324,
      "â–same": 325,
      "â–pro": 326,
      "â–load": 327,
      "â–The": 328,
      "â–example": 329,
      "\"\"\"": 330,
      "â–spec": 331,
      "raw_datasets": 332,
      "â–speci": 333,
      "\":": 334,
      ",\n": 335,
      "_',": 336,
      "_string": 337,
      "bra": 338,
      "ha": 339,
      "ig": 340,
      "ry": 341,
      "s\n": 342,
      "sp": 343,
      "ust": 344,
      "à¦•à§": 345,
      "â–st": 346,
      "â–our": 347,
      "atch": 348,
      "â–fun": 349,
      "â–fast": 350,
      "â–by": 351,
      "â–but": 352,
      "â–tokenizer.": 353,
      "â–not": 354,
      "â–do": 355,
      "â–one": 356,
      "â–have": 357,
      "tokenizer": 358,
      "â–need": 359,
      "â–list": 360,
      "â–libra": 361,
      "train\"]": 362,
      "[\"train\"]": 363,
      "â–\"à¦•à§": 364,
      "raw_datasets[\"train\"]": 365,
      "â–function": 366,
      "ain": 367,
      "ake": 368,
      "ble": 369,
      "ds": 370,
      "gg": 371,
      "pre": 372,
      "sfor": 373,
      "â€™ll": 374,
      "â–D": 375,
      "â–H": 376,
      "â–R": 377,
      "self": 378,
      "out": 379,
      "â–code": 380,
      "â–so": 381,
      "and": 382,
      "ant": 383,
      "â–Training": 384,
      "le_": 385,
      "â–us": 386,
      "arch": 387,
      "â–look": 388,
      "â–1000": 389,
      "ransfor": 390,
      "â–tokenization": 391,
      "â–special": 392,
      "â–library": 393,
      "gging": 394,
      "ransform": 395,
      "),": 396,
      "]\n": 397,
      "`',": 398,
      "act": 399,
      "dent": 400,
      "ew": 401,
      "from": 402,
      "ime": 403,
      "lit": 404,
      "ole_": 405,
      "qu": 406,
      "ub": 407,
      "ugging": 408,
      "wh": 409,
      "Ä ',": 410,
      "â€™t": 411,
      "â–E": 412,
      "â–G": 413,
      "â–S": 414,
      "â–W": 415,
      "â–[": 416,
      "â–ge": 417,
      "â–'.": 418,
      "â–'func": 419,
      "â–'ÄŠÄ Ä ": 420,
      "â–'_',": 421,
      "â–want": 422,
      "ory": 423,
      "ould": 424,
      "â–cre": 425,
      "â–tokenizers": 426,
      "â–name": 427,
      "â–self": 428,
      "â–when": 429,
      "â–return": 430,
      "â–gen": 431,
      "â–if": 432,
      "â–def": 433,
      "_corpus": 434,
      "func_string": 435,
      "irst": 436,
      "igh": 437,
      "from_": 438,
      "ole_func_string": 439,
      "(',": 440,
      "):\n": 441,
      "Tokenizer": 442,
      "a',": 443,
      "ach": 444,
      "el": 445,
      "ist": 446,
      "mm": 447,
      "od": 448,
      "os": 449,
      "ost": 450,
      "pl": 451,
      "pen": 452,
      "rin": 453,
      "t',": 454,
      "uto": 455,
      "umb": 456,
      "ven": 457,
      "ving": 458,
      "â–+": 459,
      "â–L": 460,
      "â–N": 461,
      "â–O": 462,
      "â–',": 463,
      "â–or": 464,
      "â–ran": 465,
      "â–take": 466,
      "â–av": 467,
      "en(": 468,
      "â–')": 469,
      "search": 470,
      "â–fo": 471,
      "â–first": 472,
      "â–into": 473,
      "â–old": 474,
      "llow": 475,
      "â–'Ä ',": 476,
      "â–To": 477,
      "â–Transform": 478,
      "cess": 479,
      "â–prin": 480,
      "â–go": 481,
      "â–split": 482,
      "trained": 483,
      "ything": 484,
      "â–Auto": 485,
      "pter": 486,
      "self',": 487,
      "outpu": 488,
      "â–some": 489,
      "whole_func_string": 490,
      "â–'.',": 491,
      "â–creat": 492,
      "â–generator": 493,
      "umbers": 494,
      "â–',',": 495,
      "â–print": 496,
      "â–AutoTokenizer": 497,
      "\")\n": 498,
      "(\"": 499,
      "))\n": 500,
      "-n": 501,
      "-tokenizer": 502,
      "-search": 503,
      "0,": 504,
      ":',": 505,
      "__": 506,
      "_from_": 507,
      "ck": 508,
      "cr": 509,
      "dd": 510,
      "gor": 511,
      "il": 512,
      "just": 513,
      "ms": 514,
      "s,": 515,
      "so": 516,
      "ten": 517,
      "training": 518,
      "ul": 519,
      "um": 520,
      "â–2": 521,
      "â–3": 522,
      "â–B": 523,
      "â–\"\"\"": 524,
      "â–just": 525,
      "â–time": 526,
      "â–'size": 527,
      "â–wor": 528,
      "â–cor": 529,
      "â–con": 530,
      "â–call": 531,
      "â–le": 532,
      "â–inst": 533,
      "iterator": 534,
      "def": 535,
      "â–batch": 536,
      "â–tokenizer\n": 537,
      "â–Tokenizer": 538,
      "ge(": 539,
      "ther": 540,
      "â–weâ€™ll": 541,
      "â–only": 542,
      "â–log": 543,
      "tokeniz": 544,
      "â–like": 545,
      "â–exact": 546,
      "â–used": 547,
      "etâ€™s": 548,
      "et-tokenizer": 549,
      "t_size": 550,
      "â–each": 551,
      "[\"whole_func_string": 552,
      "â–As": 553,
      "â–If": 554,
      "â–In": 555,
      "â–tokens": 556,
      "very": 557,
      "â–meth": 558,
      "â–algor": 559,
      "â–also": 560,
      "_new": 561,
      "hapter": 562,
      "spon": 563,
      "pretrained": 564,
      "â–En": 565,
      "â–GP": 566,
      "â–We": 567,
      "â–self.": 568,
      "â–range(": 569,
      "â–follow": 570,
      "â–print(": 571,
      "-net-tokenizer": 572,
      "-search-net-tokenizer": 573,
      "_from_iterator": 574,
      "â–'size',": 575,
      "[\"whole_func_string\"]": 576,
      "â–algorith": 577,
      "_new_from_iterator": 578,
      "â–following": 579,
      "(ex": 580,
      "-c": 581,
      "Lay": 582,
      "Us": 583,
      "][\"whole_func_string\"]": 584,
      "_tokenizer": 585,
      "able": 586,
      "aving": 587,
      "ail": 588,
      "bi": 589,
      "bo": 590,
      "b',": 591,
      "code": 592,
      "di": 593,
      "eigh": 594,
      "e(ex": 595,
      "ebo": 596,
      "face": 597,
      "ie": 598,
      "if": 599,
      "las": 600,
      "mory": 601,
      "oc": 602,
      "ri": 603,
      "rit": 604,
      "s:": 605,
      "tain": 606,
      "ure": 607,
      "vi": 608,
      "wo": 609,
      "weigh": 610,
      "â–#": 611,
      "â–5": 612,
      "â–7": 613,
      "â–9": 614,
      "â–M": 615,
      "â–_": 616,
      "â–raw_datasets[\"train\"]": 617,
      "â–qu": 618,
      "â–very": 619,
      "â–tas": 620,
      "â–all": 621,
      "ents": 622,
      "â–'(',": 623,
      "â–'a',": 624,
      "â–won": 625,
      "â–would": 626,
      "â–writ": 627,
      "ses": 628,
      "ized": 629,
      "â–them": 630,
      "out_": 631,
      "â–ch": 632,
      "â–len(": 633,
      "â–scr": 634,
      "â–indent": 635,
      "â–'Ä t": 636,
      "â–'Ä `',": 637,
      "â–'Ä self',": 638,
      "the": 639,
      "Ä Ä Ä Ä ": 640,
      "â–pa": 641,
      "â–un": 642,
      "arn": 643,
      "â–most": 644,
      "ual": 645,
      "â–space": 646,
      "rand": 647,
      "â–itâ€™s": 648,
      "ourse": 649,
      "ote": 650,
      "inear": 651,
      "â–memory": 652,
      "():\n": 653,
      "â–process": 654,
      "â–star": 655,
      "â–notebo": 656,
      "â–Datasets": 657,
      "â–using": 658,
      "uggingface": 659,
      "â–['": 660,
      "â–get": 661,
      "â–'):',": 662,
      "â–create": 663,
      "umbers',": 664,
      "â–AutoTokenizer.": 665,
      "training_corpus": 666,
      "â–contain": 667,
      "â–learn": 668,
      "def',": 669,
      "â–Tokenizers": 670,
      "tokenize(ex": 671,
      "â–method": 672,
      "â–range(0,": 673,
      "â–print(l": 674,
      "â–algorithm": 675,
      "Layer": 676,
      "bias": 677,
      "â–task": 678,
      "â–wonâ€™t": 679,
      "â–written": 680,
      "out_ms": 681,
      "â–scratch": 682,
      "â–indentation": 683,
      "â–notebook": 684,
      "tokenize(example": 685,
      "!\n": 686,
      "''": 687,
      "']\n": 688,
      "+',": 689,
      "-2": 690,
      "Add": 691,
      "T-2": 692,
      "You": 693,
      "[i": 694,
      "_to": 695,
      "_pretrained": 696,
      "ab": 697,
      "ade": 698,
      "du": 699,
      "ect": 700,
      "ead": 701,
      "fo": 702,
      "gl": 703,
      "ire": 704,
      "ive": 705,
      "min": 706,
      "numbers',": 707,
      "ol": 708,
      "op": 709,
      "oid": 710,
      "pos": 711,
      "s',": 712,
      "ter": 713,
      "wor": 714,
      "yp": 715,
      "â€™re": 716,
      "â–4": 717,
      "â–6": 718,
      "â–8": 719,
      "â–:": 720,
      "â–r": 721,
      "â–Ä ": 722,
      "â–Us": 723,
      "â–You": 724,
      "â–typ": 725,
      "inpu": 726,
      "â–act": 727,
      "ence": 728,
      "â–'ÄŠ": 729,
      "â–'`',": 730,
      "ath": 731,
      "ature": 732,
      "ort": 733,
      "one": 734,
      "semb": 735,
      "â–line": 736,
      "â–letâ€™s": 737,
      "return": 738,
      "respon": 739,
      "â–sh": 740,
      "â–sub": 741,
      "â–inpu": 742,
      "â–info": 743,
      "itory": 744,
      "â–bit": 745,
      "ish": 746,
      "â–anything": 747,
      "â–'Ä +',": 748,
      "â–'Ä return": 749,
      "les": 750,
      "â–data": 751,
      "â–once": 752,
      "â–pre": 753,
      "â–here": 754,
      "â–hand": 755,
      "â–hel": 756,
      "â–loop": 757,
      "mport": 758,
      "â–repre": 759,
      "ame',": 760,
      "entation": 761,
      "â–1,": 762,
      "â–import": 763,
      "train_new_from_iterator": 764,
      "t_id": 765,
      "t_training_corpus": 766,
      "â–ever": 767,
      "â–iterator": 768,
      "_dataset": 769,
      "â–training_corpus": 770,
      "_cb": 771,
      "_code": 772,
      "â–models": 773,
      "ication": 774,
      "â–Chapter": 775,
      "â–It": 776,
      "â–comp": 777,
      "â–comm": 778,
      "_name',": 779,
      "load": 780,
      "â–Face": 781,
      "â–Fast": 782,
      "â–load_dataset": 783,
      "\"\"\"',": 784,
      "â–does": 785,
      "â–one\n": 786,
      "â–needs": 787,
      "â–lists": 788,
      "raw_datasets[\"train\"]),": 789,
      "â–Here": 790,
      "â–Hugging": 791,
      "â–Rust": 792,
      "â–1000][\"whole_func_string\"]": 793,
      "â–tokenization\n": 794,
      "â–get_training_corpus": 795,
      "â–'func_code": 796,
      "â–'ÄŠÄ Ä Ä ',": 797,
      "â–'ÄŠÄ Ä Ä Ä Ä Ä ": 798,
      "istic": 799,
      "â–avail": 800,
      "â–avoid": 801,
      "â–old_tokenizer": 802,
      "â–Transformer": 803,
      "â–Transformers": 804,
      "output',": 805,
      "output_size": 806,
      "(\"code": 807,
      "â–2,": 808,
      "â–timeout_ms": 809,
      "â–work": 810,
      "â–instead": 811,
      "â–Engl": 812,
      "â–GPT-2": 813,
      "-search-net-tokenizer\")\n": 814,
      "â–len(raw_datasets[\"train\"]),": 815,
      "â–spaces": 816,
      "uggingface-c": 817,
      "tokenize(example)\n": 818,
      "aded": 819,
      "pository": 820,
      "â–Using": 821,
      "â–subwor": 822,
      "â–info_cb": 823,
      "â–'Ä return',": 824,
      "â–help": 825,
      "â–repres": 826,
      "t_idx": 827,
      "â–everything": 828,
      "â–'ÄŠÄ Ä Ä Ä Ä Ä Ä ',": 829,
      "â–available": 830,
      "â–English": 831,
      "\",": 832,
      "'\n": 833,
      "'s": 834,
      "(g": 835,
      "()\n": 836,
      "(self": 837,
      ").": 838,
      "-t": 839,
      "12": 840,
      "3\n": 841,
      "=',": 842,
      "AY": 843,
      "DA": 844,
      "KAY": 845,
      "LP": 846,
      "Linear": 847,
      "Net": 848,
      "PI": 849,
      "Se": 850,
      "UDA": 851,
      "_h": 852,
      "_lo": 853,
      "__',": 854,
      "`.": 855,
      "aw": 856,
      "ait": 857,
      "age": 858,
      "ack": 859,
      "bj": 860,
      "b']\n": 861,
      "cc": 862,
      "ci": 863,
      "con": 864,
      "cce": 865,
      "cstring": 866,
      "clas": 867,
      "d',": 868,
      "ding": 869,
      "ea": 870,
      "ep": 871,
      "est": 872,
      "eature": 873,
      "ff": 874,
      "gin": 875,
      "gra": 876,
      "hen": 877,
      "huggingface-c": 878,
      "ice": 879,
      "ild": 880,
      "imple": 881,
      "iving": 882,
      "lin": 883,
      "ling": 884,
      "low": 885,
      "lum": 886,
      "mal": 887,
      "mall": 888,
      "ments": 889,
      "mized": 890,
      "nt": 891,
      "n',": 892,
      "nâ€™t": 893,
      "other": 894,
      "rch": 895,
      "s.": 896,
      "s:\n": 897,
      "sul": 898,
      "ting": 899,
      "tere": 900,
      "uge": 901,
      "ues": 902,
      "ules": 903,
      "uild": 904,
      "vid": 905,
      "way": 906,
      "wice": 907,
      "x',": 908,
      "ymb": 909,
      "zer": 910,
      "{\n": 911,
      "à¦°\":": 912,
      "à§à¦°\":": 913,
      "â–Q": 914,
      "â–`": 915,
      "â–x": 916,
      "â–}": 917,
      "â–ÄŠ": 918,
      "â–â€”": 919,
      "â–pu": 920,
      "â–)\n": 921,
      "â–ent": 922,
      "â–ÄŠÄ Ä ": 923,
      "â–output_size": 924,
      "â–transform": 925,
      "â–two": 926,
      "â–twice": 927,
      "init": 928,
      "inut": 929,
      "â–ad": 930,
      "â–able": 931,
      "â–add": 932,
      "en))\n": 933,
      "enote": 934,
      "er.\n": 935,
      "â–then": 936,
      "â–thing": 937,
      "â–thre": 938,
      "â–than": 939,
      "â–there": 940,
      "â–torch": 941,
      "â–'self',": 942,
      "â–'__": 943,
      "â–'b',": 944,
      "â–'weigh": 945,
      "â–'bias": 946,
      "â–'Add": 947,
      "ater": 948,
      "â–was": 949,
      "â–way": 950,
      "â–wait": 951,
      "ore": 952,
      "orch": 953,
      "ormal": 954,
      "kens',": 955,
      "rall": 956,
      "ract": 957,
      "â–cha": 958,
      "â–chapter": 959,
      "â–clas": 960,
      "â–fi": 961,
      "â–few": 962,
      "â–feature": 963,
      "â–later": 964,
      "rect": 965,
      "repository": 966,
      "â–su": 967,
      "â–sent": 968,
      "â–sample": 969,
      "â–sour": 970,
      "â–save": 971,
      "â–saw": 972,
      "â–small": 973,
      "â–symb": 974,
      "â–in,": 975,
      "â–indi": 976,
      "â–intere": 977,
      "aning": 978,
      "ance": 979,
      "â–other": 980,
      "â–obj": 981,
      "â–bo": 982,
      "â–blo": 983,
      "â–brand": 984,
      "â–tokenizer,": 985,
      "lly": 986,
      "â–nex": 987,
      "â–any": 988,
      "â–another": 989,
      "â–'Ä ad": 990,
      "â–'Ä and": 991,
      "â–'Ä a',": 992,
      "â–'Ä b',": 993,
      "â–'Ä the": 994,
      "â–'Ä def',": 995,
      "â–'Ä numbers',": 996,
      "â–'Ä \"\"\"',": 997,
      "â–'Ä output',": 998,
      "â–'Ä =',": 999,
      "â–'Ä __',": 1000,
      "â–'Ä b']\n": 1001,
      "â–'Ä x',": 1002,
      "â–yie": 1003,
      "â–youâ€™re": 1004,
      "lements": 1005,
      "ions": 1006,
      "rodu": 1007,
      "â–dis": 1008,
      "â–denote": 1009,
      "odeSe": 1010,
      "â–par": 1011,
      "â–pow": 1012,
      "â–pic": 1013,
      "ary": 1014,
      "arat": 1015,
      "aring": 1016,
      "â–has": 1017,
      "â–how": 1018,
      "â–having": 1019,
      "â–huge": 1020,
      "â–loaded": 1021,
      "â–mill": 1022,
      "â–make": 1023,
      "â–minut": 1024,
      "â–set": 1025,
      "â–section": 1026,
      "uning": 1027,
      "â–why": 1028,
      "â–what": 1029,
      "â–requ": 1030,
      "â–respon": 1031,
      "â–resul": 1032,
      "â–That": 1033,
      "push": 1034,
      "â–exec": 1035,
      "es)": 1036,
      "es,": 1037,
      "ually": 1038,
      "â–1.": 1039,
      "â–10": 1040,
      "â–12": 1041,
      "â–gra": 1042,
      "â–ident": 1043,
      "â–newlin": 1044,
      "ly,": 1045,
      "â–depen": 1046,
      "â–deter": 1047,
      "â–even": 1048,
      "â–elements": 1049,
      "_doc": 1050,
      "â–API": 1051,
      "â–this,": 1052,
      "eck": 1053,
      "â–corpus\n": 1054,
      "â–corpus.\n": 1055,
      "ction\n": 1056,
      "â–Course": 1057,
      "â–CUDA": 1058,
      "â–CodeSe": 1059,
      "ine-t": 1060,
      "â–tokens\n": 1061,
      "â–language.": 1062,
      "â–dataset.": 1063,
      "ally": 1064,
      "â–alway": 1065,
      "â–colum": 1066,
      "â–(\n": 1067,
      "â–(th": 1068,
      "fore": 1069,
      "â–For": 1070,
      "â–train_new_from_iterator": 1071,
      "â–tokenized": 1072,
      "â–provi": 1073,
      "â–progra": 1074,
      "â–example,": 1075,
      "â–example:\n": 1076,
      "â–specif": 1077,
      "_string',": 1078,
      "space": 1079,
      "â–stat": 1080,
      "â–tokenizer.tokenize(example)\n": 1081,
      "â–docstring": 1082,
      "â–\"à¦•à§à¦Ÿ": 1083,
      "â–\"à¦•à§à¦¤": 1084,
      "â–functions": 1085,
      "â–code-search-net-tokenizer": 1086,
      "archNet": 1087,
      "â–1000)\n": 1088,
      "â–library\n": 1089,
      "â–Even": 1090,
      "â–Saving": 1091,
      "â–'func_doc": 1092,
      "â–tokenizers'": 1093,
      "â–namespace": 1094,
      "â–self,": 1095,
      "â–returns": 1096,
      "â–define": 1097,
      "ight": 1098,
      "from_pretrained": 1099,
      "ist(g": 1100,
      "mming": 1101,
      "place": 1102,
      "plication": 1103,
      "â–Letâ€™s": 1104,
      "â–Linear": 1105,
      "â–Note": 1106,
      "â–NLP": 1107,
      "â–Open": 1108,
      "â–')',": 1109,
      "â–going": 1110,
      "â–generator,": 1111,
      "__(self": 1112,
      "umentation": 1113,
      "â–3,": 1114,
      "â–Build": 1115,
      "â–\"\"\"\n": 1116,
      "â–words": 1117,
      "â–correct": 1118,
      "â–instance": 1119,
      "â–batches": 1120,
      "â–login": 1121,
      "â–exactly": 1122,
      "t_size,": 1123,
      "â–Assemb": 1124,
      "â–Weâ€™ll": 1125,
      "â–self.weigh": 1126,
      "â–self.bias": 1127,
      "iece": 1128,
      "ify": 1129,
      "ries": 1130,
      "wo',": 1131,
      "â–7,": 1132,
      "â–Model": 1133,
      "â–raw_datasets[\"train\"]\n": 1134,
      "â–raw_datasets[\"train\"][i": 1135,
      "ses(": 1136,
      "â–check": 1137,
      "â–'Ä torch": 1138,
      "â–'Ä two',": 1139,
      "â–parall": 1140,
      "â–processing": 1141,
      "â–start_idx": 1142,
      "â–['def',": 1143,
      "â–AutoTokenizer.train_new_from_iterator": 1144,
      "â–AutoTokenizer.from_pretrained": 1145,
      "â–contains": 1146,
      "â–method:\n": 1147,
      "â–print(len(": 1148,
      "â–print(list(g": 1149,
      "â–algorithm.": 1150,
      "â–indentation,": 1151,
      "â–notebook,": 1152,
      "â–notebook_lo": 1153,
      "_tokens',": 1154,
      "_pretrained()": 1155,
      "ab\n": 1156,
      "â–rules": 1157,
      "â–type": 1158,
      "input',": 1159,
      "â–actually": 1160,
      "â–'ÄŠ',": 1161,
      "ather": 1162,
      "responses(": 1163,
      "â–should": 1164,
      "â–inputs": 1165,
      "â–once.": 1166,
      "â–1,000": 1167,
      "â–Itâ€™s": 1168,
      "â–command": 1169,
      "â–1000][\"whole_func_string\"]\n": 1170,
      "â–get_training_corpus():\n": 1171,
      "â–old_tokenizer.": 1172,
      "(\"code-search-net-tokenizer\")\n": 1173,
      "â–timeout_ms=": 1174,
      "â–working": 1175,
      "â–subwords": 1176,
      "â–represents": 1177,
      "_hub": 1178,
      "conds": 1179,
      "ccep": 1180,
      "estion": 1181,
      "huggingface-course": 1182,
      "vidual": 1183,
      "zeros": 1184,
      "â–pure": 1185,
      "â–three": 1186,
      "â–torch.": 1187,
      "â–'__(',": 1188,
      "â–'bias',": 1189,
      "â–'Add',": 1190,
      "â–charact": 1191,
      "â–samples": 1192,
      "â–source": 1193,
      "â–symbol": 1194,
      "â–individual": 1195,
      "â–interest": 1196,
      "â–object": 1197,
      "â–block": 1198,
      "â–next": 1199,
      "â–'Ä add',": 1200,
      "â–'Ä and',": 1201,
      "â–'Ä the',": 1202,
      "â–yield": 1203,
      "roduction\n": 1204,
      "â–powers": 1205,
      "â–pick": 1206,
      "â–require": 1207,
      "â–results": 1208,
      "â–execut": 1209,
      "â–identify": 1210,
      "â–newlines,": 1211,
      "â–depend": 1212,
      "â–determin": 1213,
      "â–CodeSearchNet": 1214,
      "ine-tuning": 1215,
      "â–always": 1216,
      "â–column": 1217,
      "â–provide": 1218,
      "â–programming": 1219,
      "â–code-search-net-tokenizer,": 1220,
      "â–'func_documentation": 1221,
      "â–LinearLayer": 1222,
      "__(self,": 1223,
      "â–Building": 1224,
      "â–correctly": 1225,
      "â–instance,": 1226,
      "â–Assembling": 1227,
      "â–'Ä torch',": 1228,
      "â–parallel": 1229,
      "â–AutoTokenizer.train_new_from_iterator()": 1230,
      "â–AutoTokenizer.from_pretrained(\"": 1231,
      "â–print(list(gen))\n": 1232,
      "â–notebook_login": 1233
    },
    "merges": [
      "â– t",
      "i n",
      "â– a",
      "e n",
      "e r",
      "â–t h",
      "' ,",
      "â–t o",
      "â– '",
      "a t",
      "â– w",
      "o r",
      "o n",
      "s e",
      "i z",
      "in g",
      "k en",
      "r a",
      "â–th e",
      "ken iz",
      "o u",
      "â– c",
      "â– f",
      "â– l",
      "s t",
      "r e",
      "keniz er",
      "â– \n",
      "â– s",
      "â– in",
      "i t",
      "d e",
      "a n",
      "â– o",
      "â– b",
      "â–to kenizer",
      "l l",
      "ra in",
      "i s",
      "â– n",
      "â–a n",
      "â–' Ä ",
      "â– y",
      "â– T",
      "â–y ou",
      "m e",
      "l e",
      "c e",
      "â–o f",
      "c h",
      "i on",
      "t s",
      "e x",
      "p u",
      "g e",
      "r o",
      "t h",
      "Ä  Ä ",
      "â– d",
      "at a",
      "â–w e",
      "â– on",
      "o de",
      "ata se",
      "â– p",
      "â– u",
      ": \n",
      "a r",
      "c t",
      "e d",
      "â– h",
      "â–l o",
      "t o",
      "â– m",
      "â–n e",
      "â–an d",
      "â– se",
      "â–f or",
      "rain ing",
      "m p",
      "â–th at",
      ") \n",
      "u n",
      "â–w h",
      "â– re",
      "â–T h",
      "ro m",
      "0 0",
      "â–l i",
      "pu s",
      "â– is",
      "â– ex",
      "\" \"",
      "a me",
      "i ll",
      "en t",
      "at ion",
      "or pus",
      "â–c an",
      "e s",
      "u a",
      "â– 1",
      "â– g",
      "â– i",
      "â–s p",
      "â–b e",
      "â–you r",
      "â–ne w",
      "a d",
      "l d",
      "t rain",
      "y th",
      "â–w ill",
      "â–u se",
      "mp le",
      "l y",
      "v e",
      "ra n",
      "it h",
      "atase t",
      ". \n",
      "e t",
      "â– de",
      "â–f rom",
      "a s",
      "a ce",
      "a mple",
      "o w",
      "t _",
      "â– =",
      "â– e",
      "â– it",
      "ou r",
      "ode l",
      "\" ]",
      "[ \"",
      "_ d",
      "a l",
      "k e",
      "l f",
      "o t",
      "â– A",
      "â–t raining",
      "er e",
      "â–th is",
      "atase ts",
      "_ c",
      "a st",
      "e c",
      "f un",
      "g ua",
      "u r",
      "u t",
      "â€™ s",
      "â– P",
      "â–a t",
      "â–a re",
      "er at",
      "â–c orpus",
      "an gua",
      "ct ion",
      "â–m odel",
      "fun c",
      "erat or",
      "angua ge",
      "i c",
      "â– C",
      "â– I",
      "in e",
      "er s",
      "â–to ken",
      "â–w ith",
      "â–l anguage",
      "â–d ataset",
      "yth on",
      "a ll",
      "a ve",
      "o k",
      "v er",
      "â– \"",
      "â– me",
      "â–t ex",
      "â–a l",
      "â–a s",
      "â–c o",
      "â–wh i",
      "â–P ython",
      "( )",
      "m b",
      "o kenizer",
      "p t",
      "r ing",
      "â– (",
      "iz e",
      "st ring",
      "â–se e",
      "â–Th is",
      "00 0",
      "ur n",
      "â–tex ts",
      "â–whi ch",
      "_ n",
      "a y",
      "f or",
      "i d",
      "i r",
      "l o",
      "s s",
      "s ize",
      "t urn",
      "w _d",
      "ÄŠ Ä Ä ",
      "â– F",
      "â– ğŸ¤—",
      "â–t rain",
      "â–to keniz",
      "ra w_d",
      "â–s ame",
      "â–p ro",
      "â–lo ad",
      "â–Th e",
      "â–ex ample",
      "\"\" \"",
      "â–sp ec",
      "raw_d atasets",
      "â–spec i",
      "\" :",
      ", \n",
      "_ ',",
      "_ string",
      "b ra",
      "h a",
      "i g",
      "r y",
      "s \n",
      "s p",
      "u st",
      "à¦• à§",
      "â– st",
      "â– our",
      "at ch",
      "â–f un",
      "â–f ast",
      "â–b y",
      "â–b ut",
      "â–tokenizer .",
      "â–n ot",
      "â–d o",
      "â–on e",
      "â–h ave",
      "to kenizer",
      "â–ne ed",
      "â–li st",
      "â–li bra",
      "train \"]",
      "[\" train\"]",
      "â–\" à¦•à§",
      "raw_datasets [\"train\"]",
      "â–fun ction",
      "a in",
      "a ke",
      "b le",
      "d s",
      "g g",
      "p re",
      "s for",
      "â€™ ll",
      "â– D",
      "â– H",
      "â– R",
      "se lf",
      "ou t",
      "â–c ode",
      "â–s o",
      "an d",
      "an t",
      "â–T raining",
      "le _",
      "â–u s",
      "ar ch",
      "â–lo ok",
      "â–1 000",
      "ran sfor",
      "â–tokeniz ation",
      "â–speci al",
      "â–libra ry",
      "gg ing",
      "ransfor m",
      ") ,",
      "] \n",
      "` ',",
      "a ct",
      "d ent",
      "e w",
      "f rom",
      "i me",
      "l it",
      "o le_",
      "q u",
      "u b",
      "u gging",
      "w h",
      "Ä  ',",
      "â€™ t",
      "â– E",
      "â– G",
      "â– S",
      "â– W",
      "â– [",
      "â– ge",
      "â–' .",
      "â–' func",
      "â–' ÄŠÄ Ä ",
      "â–' _',",
      "â–w ant",
      "or y",
      "ou ld",
      "â–c re",
      "â–tokenizer s",
      "â–n ame",
      "â–se lf",
      "â–wh en",
      "â–re turn",
      "â–g en",
      "â–i f",
      "â–de f",
      "_c orpus",
      "func _string",
      "ir st",
      "ig h",
      "from _",
      "ole_ func_string",
      "( ',",
      ") :\n",
      "T okenizer",
      "a ',",
      "a ch",
      "e l",
      "i st",
      "m m",
      "o d",
      "o s",
      "o st",
      "p l",
      "p en",
      "r in",
      "t ',",
      "u to",
      "u mb",
      "v en",
      "v ing",
      "â– +",
      "â– L",
      "â– N",
      "â– O",
      "â– ',",
      "â– or",
      "â– ran",
      "â–t ake",
      "â–a v",
      "en (",
      "â–' )",
      "se arch",
      "â–f o",
      "â–f irst",
      "â–in to",
      "â–o ld",
      "ll ow",
      "â–'Ä  ',",
      "â–T o",
      "â–T ransform",
      "ce ss",
      "â–p rin",
      "â–g o",
      "â–sp lit",
      "train ed",
      "yth ing",
      "â–A uto",
      "pt er",
      "self ',",
      "out pu",
      "â–so me",
      "wh ole_func_string",
      "â–'. ',",
      "â–cre at",
      "â–gen erator",
      "umb ers",
      "â–', ',",
      "â–prin t",
      "â–Auto Tokenizer",
      "\" )\n",
      "( \"",
      ") )\n",
      "- n",
      "- tokenizer",
      "- search",
      "0 ,",
      ": ',",
      "_ _",
      "_ from_",
      "c k",
      "c r",
      "d d",
      "g or",
      "i l",
      "j ust",
      "m s",
      "s ,",
      "s o",
      "t en",
      "t raining",
      "u l",
      "u m",
      "â– 2",
      "â– 3",
      "â– B",
      "â– \"\"\"",
      "â– just",
      "â–t ime",
      "â–' size",
      "â–w or",
      "â–c or",
      "â–c on",
      "â–c all",
      "â–l e",
      "â–in st",
      "it erator",
      "de f",
      "â–b atch",
      "â–tokenizer \n",
      "â–T okenizer",
      "ge (",
      "th er",
      "â–we â€™ll",
      "â–on ly",
      "â–lo g",
      "to keniz",
      "â–li ke",
      "â–ex act",
      "â–use d",
      "et â€™s",
      "et -tokenizer",
      "t_ size",
      "â–e ach",
      "[\" whole_func_string",
      "â–A s",
      "â–I f",
      "â–I n",
      "â–token s",
      "ver y",
      "â–me th",
      "â–al gor",
      "â–al so",
      "_n ew",
      "ha pter",
      "sp on",
      "pre trained",
      "â–E n",
      "â–G P",
      "â–W e",
      "â–self .",
      "â–ran ge(",
      "â–fo llow",
      "â–print (",
      "-n et-tokenizer",
      "-search -net-tokenizer",
      "_from_ iterator",
      "â–'size ',",
      "[\"whole_func_string \"]",
      "â–algor ith",
      "_new _from_iterator",
      "â–follow ing",
      "( ex",
      "- c",
      "L ay",
      "U s",
      "] [\"whole_func_string\"]",
      "_ tokenizer",
      "a ble",
      "a ving",
      "a il",
      "b i",
      "b o",
      "b ',",
      "c ode",
      "d i",
      "e igh",
      "e (ex",
      "e bo",
      "f ace",
      "i e",
      "i f",
      "l as",
      "m ory",
      "o c",
      "r i",
      "r it",
      "s :",
      "t ain",
      "u re",
      "v i",
      "w o",
      "w eigh",
      "â– #",
      "â– 5",
      "â– 7",
      "â– 9",
      "â– M",
      "â– _",
      "â– raw_datasets[\"train\"]",
      "â– qu",
      "â– very",
      "â–t as",
      "â–a ll",
      "en ts",
      "â–' (',",
      "â–' a',",
      "â–w on",
      "â–w ould",
      "â–w rit",
      "se s",
      "iz ed",
      "â–the m",
      "ou t_",
      "â–c h",
      "â–l en(",
      "â–s cr",
      "â–in dent",
      "â–'Ä  t",
      "â–'Ä  `',",
      "â–'Ä  self',",
      "th e",
      "Ä Ä  Ä Ä ",
      "â–p a",
      "â–u n",
      "ar n",
      "â–m ost",
      "ua l",
      "â–sp ace",
      "ran d",
      "â–it â€™s",
      "our se",
      "ot e",
      "ine ar",
      "â–me mory",
      "() :\n",
      "â–pro cess",
      "â–st ar",
      "â–not ebo",
      "â–D atasets",
      "â–us ing",
      "ugging face",
      "â–[ '",
      "â–ge t",
      "â–') :',",
      "â–creat e",
      "umbers ',",
      "â–AutoTokenizer .",
      "training _corpus",
      "â–con tain",
      "â–le arn",
      "def ',",
      "â–Tokenizer s",
      "tokeniz e(ex",
      "â–meth od",
      "â–range( 0,",
      "â–print( l",
      "â–algorith m",
      "Lay er",
      "bi as",
      "â–tas k",
      "â–won â€™t",
      "â–writ ten",
      "out_ ms",
      "â–scr atch",
      "â–indent ation",
      "â–notebo ok",
      "tokenize(ex ample",
      "! \n",
      "' '",
      "' ]\n",
      "+ ',",
      "- 2",
      "A dd",
      "T -2",
      "Y ou",
      "[ i",
      "_ to",
      "_ pretrained",
      "a b",
      "a de",
      "d u",
      "e ct",
      "e ad",
      "f o",
      "g l",
      "i re",
      "i ve",
      "m in",
      "n umbers',",
      "o l",
      "o p",
      "o id",
      "p os",
      "s ',",
      "t er",
      "w or",
      "y p",
      "â€™ re",
      "â– 4",
      "â– 6",
      "â– 8",
      "â– :",
      "â– r",
      "â– Ä ",
      "â– Us",
      "â– You",
      "â–t yp",
      "in pu",
      "â–a ct",
      "en ce",
      "â–' ÄŠ",
      "â–' `',",
      "at h",
      "at ure",
      "or t",
      "on e",
      "se mb",
      "â–l ine",
      "â–l etâ€™s",
      "re turn",
      "re spon",
      "â–s h",
      "â–s ub",
      "â–in pu",
      "â–in fo",
      "it ory",
      "â–b it",
      "is h",
      "â–an ything",
      "â–'Ä  +',",
      "â–'Ä  return",
      "le s",
      "â–d ata",
      "â–on ce",
      "â–p re",
      "â–h ere",
      "â–h and",
      "â–h el",
      "â–lo op",
      "mp ort",
      "â–re pre",
      "ame ',",
      "ent ation",
      "â–1 ,",
      "â–i mport",
      "train _new_from_iterator",
      "t_ id",
      "t_ training_corpus",
      "â–e ver",
      "â–it erator",
      "_d ataset",
      "â–training _corpus",
      "_c b",
      "_c ode",
      "â–model s",
      "ic ation",
      "â–C hapter",
      "â–I t",
      "â–co mp",
      "â–co mm",
      "_n ame',",
      "lo ad",
      "â–F ace",
      "â–F ast",
      "â–load _dataset",
      "\"\"\" ',",
      "â–do es",
      "â–one \n",
      "â–need s",
      "â–list s",
      "raw_datasets[\"train\"] ),",
      "â–H ere",
      "â–H ugging",
      "â–R ust",
      "â–1000 ][\"whole_func_string\"]",
      "â–tokenization \n",
      "â–ge t_training_corpus",
      "â–'func _code",
      "â–'ÄŠÄ Ä  Ä ',",
      "â–'ÄŠÄ Ä  Ä Ä Ä Ä ",
      "ist ic",
      "â–av ail",
      "â–av oid",
      "â–old _tokenizer",
      "â–Transform er",
      "â–Transform ers",
      "outpu t',",
      "outpu t_size",
      "(\" code",
      "â–2 ,",
      "â–time out_ms",
      "â–wor k",
      "â–inst ead",
      "â–En gl",
      "â–GP T-2",
      "-search-net-tokenizer \")\n",
      "â–len( raw_datasets[\"train\"]),",
      "â–space s",
      "uggingface -c",
      "tokenize(example )\n",
      "ade d",
      "pos itory",
      "â–Us ing",
      "â–sub wor",
      "â–info _cb",
      "â–'Ä return ',",
      "â–hel p",
      "â–repre s",
      "t_id x",
      "â–ever ything",
      "â–'ÄŠÄ Ä Ä Ä Ä Ä  Ä ',",
      "â–avail able",
      "â–Engl ish",
      "\" ,",
      "' \n",
      "' s",
      "( g",
      "( )\n",
      "( self",
      ") .",
      "- t",
      "1 2",
      "3 \n",
      "= ',",
      "A Y",
      "D A",
      "K AY",
      "L P",
      "L inear",
      "N et",
      "P I",
      "S e",
      "U DA",
      "_ h",
      "_ lo",
      "_ _',",
      "` .",
      "a w",
      "a it",
      "a ge",
      "a ck",
      "b j",
      "b ']\n",
      "c c",
      "c i",
      "c on",
      "c ce",
      "c string",
      "c las",
      "d ',",
      "d ing",
      "e a",
      "e p",
      "e st",
      "e ature",
      "f f",
      "g in",
      "g ra",
      "h en",
      "h uggingface-c",
      "i ce",
      "i ld",
      "i mple",
      "i ving",
      "l in",
      "l ing",
      "l ow",
      "l um",
      "m al",
      "m all",
      "m ents",
      "m ized",
      "n t",
      "n ',",
      "n â€™t",
      "o ther",
      "r ch",
      "s .",
      "s :\n",
      "s ul",
      "t ing",
      "t ere",
      "u ge",
      "u es",
      "u les",
      "u ild",
      "v id",
      "w ay",
      "w ice",
      "x ',",
      "y mb",
      "z er",
      "{ \n",
      "à¦° \":",
      "à§ à¦°\":",
      "â– Q",
      "â– `",
      "â– x",
      "â– }",
      "â– ÄŠ",
      "â– â€”",
      "â– pu",
      "â– )\n",
      "â– ent",
      "â– ÄŠÄ Ä ",
      "â– output_size",
      "â–t ransform",
      "â–t wo",
      "â–t wice",
      "in it",
      "in ut",
      "â–a d",
      "â–a ble",
      "â–a dd",
      "en ))\n",
      "en ote",
      "er .\n",
      "â–th en",
      "â–th ing",
      "â–th re",
      "â–th an",
      "â–th ere",
      "â–to rch",
      "â–' self',",
      "â–' __",
      "â–' b',",
      "â–' weigh",
      "â–' bias",
      "â–' Add",
      "at er",
      "â–w as",
      "â–w ay",
      "â–w ait",
      "or e",
      "or ch",
      "or mal",
      "ken s',",
      "ra ll",
      "ra ct",
      "â–c ha",
      "â–c hapter",
      "â–c las",
      "â–f i",
      "â–f ew",
      "â–f eature",
      "â–l ater",
      "re ct",
      "re pository",
      "â–s u",
      "â–s ent",
      "â–s ample",
      "â–s our",
      "â–s ave",
      "â–s aw",
      "â–s mall",
      "â–s ymb",
      "â–in ,",
      "â–in di",
      "â–in tere",
      "an ing",
      "an ce",
      "â–o ther",
      "â–o bj",
      "â–b o",
      "â–b lo",
      "â–b rand",
      "â–tokenizer ,",
      "ll y",
      "â–n ex",
      "â–an y",
      "â–an other",
      "â–'Ä  ad",
      "â–'Ä  and",
      "â–'Ä  a',",
      "â–'Ä  b',",
      "â–'Ä  the",
      "â–'Ä  def',",
      "â–'Ä  numbers',",
      "â–'Ä  \"\"\"',",
      "â–'Ä  output',",
      "â–'Ä  =',",
      "â–'Ä  __',",
      "â–'Ä  b']\n",
      "â–'Ä  x',",
      "â–y ie",
      "â–you â€™re",
      "le ments",
      "ion s",
      "ro du",
      "â–d is",
      "â–d enote",
      "ode Se",
      "â–p ar",
      "â–p ow",
      "â–p ic",
      "ar y",
      "ar at",
      "ar ing",
      "â–h as",
      "â–h ow",
      "â–h aving",
      "â–h uge",
      "â–lo aded",
      "â–m ill",
      "â–m ake",
      "â–m inut",
      "â–se t",
      "â–se ction",
      "un ing",
      "â–wh y",
      "â–wh at",
      "â–re qu",
      "â–re spon",
      "â–re sul",
      "â–Th at",
      "pus h",
      "â–ex ec",
      "es )",
      "es ,",
      "ua lly",
      "â–1 .",
      "â–1 0",
      "â–1 2",
      "â–g ra",
      "â–i dent",
      "â–new lin",
      "ly ,",
      "â–de pen",
      "â–de ter",
      "â–e ven",
      "â–e lements",
      "_d oc",
      "â–A PI",
      "â–this ,",
      "ec k",
      "â–corpus \n",
      "â–corpus .\n",
      "ction \n",
      "â–C ourse",
      "â–C UDA",
      "â–C odeSe",
      "ine -t",
      "â–token s\n",
      "â–language .",
      "â–dataset .",
      "all y",
      "â–al way",
      "â–co lum",
      "â–( \n",
      "â–( th",
      "for e",
      "â–F or",
      "â–train _new_from_iterator",
      "â–tokeniz ed",
      "â–pro vi",
      "â–pro gra",
      "â–example ,",
      "â–example :\n",
      "â–speci f",
      "_string ',",
      "sp ace",
      "â–st at",
      "â–tokenizer. tokenize(example)\n",
      "â–do cstring",
      "â–\"à¦•à§ à¦Ÿ",
      "â–\"à¦•à§ à¦¤",
      "â–function s",
      "â–code -search-net-tokenizer",
      "arch Net",
      "â–1000 )\n",
      "â–library \n",
      "â–E ven",
      "â–S aving",
      "â–'func _doc",
      "â–tokenizers '",
      "â–name space",
      "â–self ,",
      "â–return s",
      "â–def ine",
      "igh t",
      "from_ pretrained",
      "ist (g",
      "mm ing",
      "pl ace",
      "pl ication",
      "â–L etâ€™s",
      "â–L inear",
      "â–N ote",
      "â–N LP",
      "â–O pen",
      "â–') ',",
      "â–go ing",
      "â–generator ,",
      "__ (self",
      "um entation",
      "â–3 ,",
      "â–B uild",
      "â–\"\"\" \n",
      "â–wor ds",
      "â–cor rect",
      "â–inst ance",
      "â–batch es",
      "â–log in",
      "â–exact ly",
      "t_size ,",
      "â–As semb",
      "â–We â€™ll",
      "â–self. weigh",
      "â–self. bias",
      "ie ce",
      "if y",
      "ri es",
      "wo ',",
      "â–7 ,",
      "â–M odel",
      "â–raw_datasets[\"train\"] \n",
      "â–raw_datasets[\"train\"] [i",
      "ses (",
      "â–ch eck",
      "â–'Ä t orch",
      "â–'Ä t wo',",
      "â–pa rall",
      "â–process ing",
      "â–star t_idx",
      "â–[' def',",
      "â–AutoTokenizer. train_new_from_iterator",
      "â–AutoTokenizer. from_pretrained",
      "â–contain s",
      "â–method :\n",
      "â–print(l en(",
      "â–print(l ist(g",
      "â–algorithm .",
      "â–indentation ,",
      "â–notebook ,",
      "â–notebook _lo",
      "_to kens',",
      "_pretrained ()",
      "ab \n",
      "â–r ules",
      "â–typ e",
      "inpu t',",
      "â–act ually",
      "â–'ÄŠ ',",
      "ath er",
      "respon ses(",
      "â–sh ould",
      "â–inpu ts",
      "â–once .",
      "â–1, 000",
      "â–It â€™s",
      "â–comm and",
      "â–1000][\"whole_func_string\"] \n",
      "â–get_training_corpus ():\n",
      "â–old_tokenizer .",
      "(\"code -search-net-tokenizer\")\n",
      "â–timeout_ms =",
      "â–work ing",
      "â–subwor ds",
      "â–repres ents",
      "_h ub",
      "con ds",
      "cce p",
      "est ion",
      "huggingface-c ourse",
      "vid ual",
      "zer os",
      "â–pu re",
      "â–thre e",
      "â–torch .",
      "â–'__ (',",
      "â–'bias ',",
      "â–'Add ',",
      "â–cha ract",
      "â–sample s",
      "â–sour ce",
      "â–symb ol",
      "â–indi vidual",
      "â–intere st",
      "â–obj ect",
      "â–blo ck",
      "â–nex t",
      "â–'Ä ad d',",
      "â–'Ä and ',",
      "â–'Ä the ',",
      "â–yie ld",
      "rodu ction\n",
      "â–pow ers",
      "â–pic k",
      "â–requ ire",
      "â–resul ts",
      "â–exec ut",
      "â–ident ify",
      "â–newlin es,",
      "â–depen d",
      "â–deter min",
      "â–CodeSe archNet",
      "ine-t uning",
      "â–alway s",
      "â–colum n",
      "â–provi de",
      "â–progra mming",
      "â–code-search-net-tokenizer ,",
      "â–'func_doc umentation",
      "â–Linear Layer",
      "__(self ,",
      "â–Build ing",
      "â–correct ly",
      "â–instance ,",
      "â–Assemb ling",
      "â–'Ä torch ',",
      "â–parall el",
      "â–AutoTokenizer.train_new_from_iterator ()",
      "â–AutoTokenizer.from_pretrained (\"",
      "â–print(list(g en))\n",
      "â–notebook_lo gin"
    ]
  }
}