{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tokenizers\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagex/Sabbir/BengaliTokenizer/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from custom_tokenizer_bpe import *\n",
    "from characters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer JSON file\n",
    "# with open(\"./bengali_tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     tokenizer_json = json.load(f)\n",
    "\n",
    "# # Save the JSON content to a temporary file\n",
    "# temp_json_path = \"temp_tokenizer.json\"\n",
    "# with open(temp_json_path, \"w\", encoding=\"utf-8\") as temp_f:\n",
    "#     json.dump(tokenizer_json, temp_f, ensure_ascii=False)\n",
    "\n",
    "# Create a PreTrainedTokenizerFast instance from the JSON file\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file='bengali_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' আমাদের কোম্পানি ভোটদান পণ্য তৈরীর চীন মধ্যে নেতা এক।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আমাদের',\n",
       " 'কোম্পানি',\n",
       " 'ভোট',\n",
       " 'দান',\n",
       " 'পণ্য',\n",
       " 'তৈরীর',\n",
       " 'চীন',\n",
       " 'মধ্যে',\n",
       " 'নেতা',\n",
       " 'এক',\n",
       " '।']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<mask>\"]\n",
    "# for sp in special_tokens:\n",
    "#     op={\"special\":True,\"content\":sp,\"single_word\":False,\"lstrip\":False,\"rstrip\":False,\"normalized\":False}\n",
    "#     mask_token = tokenizers.AddedToken(**op)\n",
    "#     tokenizer.add_tokens([mask_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/storage2/llm_data/data_all_text/AllTextData/ai4bharat.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/banglaLM_process_v1.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/BanglaLM_process_v2.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/BanglaLM_raw.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2012.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2013_20.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2013_48.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_15.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_23.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_35.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_41.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_42.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_49.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2014_52.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_06.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_11.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_14.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_18.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_22.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_27.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_32.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_35.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_40.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2015_48.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2016_30.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2016_50.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/bn.2017_17.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/cc_100.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/output_text_file.txt',\n",
       " '/storage2/llm_data/data_all_text/AllTextData/raw_bangla_for_BERT.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "p = \"/storage2/llm_data/data_all_text/AllTextData\"\n",
    "d = os.listdir(p)\n",
    "dd = [os.path.join(p, l) for l in d]\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/aci-mis-team/BengaliBPETokenizer/commit/2a1c35127e4ad7c0e285b034e0df20be137737b3', commit_message='Upload tokenizer', commit_description='', oid='2a1c35127e4ad7c0e285b034e0df20be137737b3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\n",
    "    repo_id=\"aci-mis-team/BengaliBPETokenizer\",\n",
    "    token=os.getenv('hf_token')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"aci-mis-team/BengaliBPETokenizer\",\n",
    "    token=os.getenv('hf_token')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [864, 1540, 1994, 1084, 2582, 16650, 2494, 732, 1175, 554, 339], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/storagex/Sabbir/BengaliTokenizer/characters.txt\", 'r') as f:\n",
    "    char = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"multi_char.txt\", 'w') as f:\n",
    "    for i in range(100):\n",
    "        f.write(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/storagex/Sabbir/BengaliTokenizer/characters.txt', 'r') as f:\n",
    "    c = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file='bengali_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁আলাদা',\n",
       " '।',\n",
       " '▁সঠিক',\n",
       " '▁অস্তিত্ব',\n",
       " '▁নিয়ে',\n",
       " '▁বেঁচে',\n",
       " '▁থাকার',\n",
       " '▁অধিকার',\n",
       " '▁খু',\n",
       " 'ন্ন',\n",
       " '▁হয়',\n",
       " '▁এদের']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode(\"আলাদা। সঠিক অস্তিত্ব নিয়ে বেঁচে থাকার অধিকার খুন্ন হয় এদের\", out_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7fb280c21260> >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load SentencePiece model\n",
    "sp_model = spm.SentencePieceProcessor(\"bengali_tokenizer.model\")\n",
    "# sp_model.load()\n",
    "\n",
    "# # Convert to HuggingFace tokenizer\n",
    "# tokenizer = PreTrainedTokenizerFast(tokenizer_object=sp_model, model_file=\"bengali_tokenizer.model\")\n",
    "\n",
    "# # Save the tokenizer\n",
    "# # tokenizer.save_pretrained(\"./hf_tokenizer\")\n",
    "sp_model\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ত্ব',\n",
       " '▁ত',\n",
       " '্ম',\n",
       " '▁ত',\n",
       " '্ম',\n",
       " '্য',\n",
       " '▁ত্য',\n",
       " '▁ত্র',\n",
       " '▁ত্র',\n",
       " '্য',\n",
       " '▁থ',\n",
       " '্ব',\n",
       " '▁থ',\n",
       " '্য',\n",
       " '▁থ্র',\n",
       " '▁দ',\n",
       " '্গ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.encode(\"ত্ব ত্ম ত্ম্য ত্য ত্র ত্র্য থ্ব থ্য থ্র দ্গ\", out_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
