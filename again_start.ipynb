{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data('./demo_1M.txt')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bengali_word_iterator(text:str):\n",
    "    for line in text:\n",
    "        yield re.findall(r'\\S+', line, flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    bengali_word_iterator(text=text),\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "len(tokenizer.get_vocab().items())\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data('./demo_1M.txt')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bengali_word_iterator(text:str):\n",
    "    for line in text:\n",
    "        yield re.findall(r'\\S+', line, flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import SentencePieceBPETokenizer, Tokenizer, AddedToken, trainers\n",
    "\n",
    "# class SpBPETokenizerTest(SentencePieceBPETokenizer):\n",
    "#     def __init__(self, forbidden_start_chars=None):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.forbidden_start_chars = forbidden_start_chars or {'ৎ', 'ং', 'ঃ'}\n",
    "#         self.initial_alphabets = bangla_alphabets + conjunct_consonants\n",
    "            \n",
    "#     def train_from_iterator(\n",
    "#         self,\n",
    "#         iterator: Iterator[str],\n",
    "#         vocab_size: int = 30000,\n",
    "#         min_frequency: int = 5,\n",
    "#         show_progress: bool = True,\n",
    "#         special_tokens: List[str] = None,\n",
    "#         initial_alphabet: List[str] = None,\n",
    "#         limit_alphabet: int = None,\n",
    "#     ) -> None:\n",
    "#         # Create a new tokenizer\n",
    "#         self._tokenizer = Tokenizer(self._tokenizer.model)\n",
    "        \n",
    "#         # Prepare special tokens\n",
    "#         special_tokens = special_tokens or []\n",
    "        \n",
    "#         # Add initial alphabet to special tokens\n",
    "#         if initial_alphabet is not None:\n",
    "#             self.initial_alphabets = initial_alphabet\n",
    "#         special_tokens.extend(self.initial_alphabets)\n",
    "        \n",
    "#         # Remove duplicates while preserving order\n",
    "#         special_tokens = list(dict.fromkeys(special_tokens))\n",
    "        \n",
    "#         # Modify the iterator for whitespace issue\n",
    "#         iterator = bengali_word_iterator(text=iterator)\n",
    "        \n",
    "#         # Train the tokenizer\n",
    "#         trainer = trainers.BpeTrainer(\n",
    "#             vocab_size=vocab_size,\n",
    "#             min_frequency=min_frequency,\n",
    "#             special_tokens=special_tokens,\n",
    "#             limit_alphabet=limit_alphabet,\n",
    "#             # initial_alphabet=None,  # We've included initial_alphabet in special_tokens\n",
    "#             show_progress=show_progress,\n",
    "#         )\n",
    "        \n",
    "#         self._tokenizer.train_from_iterator(\n",
    "#             iterator,\n",
    "#             trainer=trainer,\n",
    "#         )\n",
    "        \n",
    "#         # Remove forbidden characters from the beginning of tokens\n",
    "#         vocab = self._tokenizer.get_vocab()\n",
    "#         filtered_vocab = {token: idx for token, idx in vocab.items()\n",
    "#                         if not any(token.startswith(char) for char in self.forbidden_start_chars)}\n",
    "        \n",
    "#         # Reset the vocabulary with the filtered tokens\n",
    "#         new_tokenizer = Tokenizer(self._tokenizer.model)\n",
    "#         for token, idx in filtered_vocab.items():\n",
    "#             if token in special_tokens:\n",
    "#                 new_tokenizer.add_special_tokens([token])\n",
    "#             else:\n",
    "#                 new_tokenizer.add_tokens([token])\n",
    "        \n",
    "#         self._tokenizer = new_tokenizer\n",
    "\n",
    "#     def get_vocab(self):\n",
    "#         return self._tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    # bengali_word_iterator(text=text),\n",
    "    text,\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "len(tokenizer.get_vocab().items())\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, trainers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "class SpBPETokenizerTest:\n",
    "    def __init__(self, forbidden_start_chars=None):\n",
    "        self.forbidden_start_chars = forbidden_start_chars or {'ৎ', 'ং', 'ঃ'}\n",
    "        self.initial_alphabets = bangla_alphabets + conjunct_consonants\n",
    "        self._tokenizer = Tokenizer(BPE())\n",
    "        self._tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        iterator: Iterator[str],\n",
    "        vocab_size: int = 30000,\n",
    "        min_frequency: int = 5,\n",
    "        show_progress: bool = True,\n",
    "        special_tokens: List[str] = None,\n",
    "        initial_alphabet: List[str] = None,\n",
    "        limit_alphabet: int = None,\n",
    "    ) -> None:\n",
    "        # Prepare special tokens\n",
    "        special_tokens = special_tokens or []\n",
    "        \n",
    "        # Add initial alphabet to special tokens\n",
    "        if initial_alphabet is not None:\n",
    "            self.initial_alphabets = initial_alphabet\n",
    "        special_tokens.extend(self.initial_alphabets)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        special_tokens = list(dict.fromkeys(special_tokens))\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            special_tokens=special_tokens,\n",
    "            limit_alphabet=limit_alphabet,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "        \n",
    "        self._tokenizer.train_from_iterator(\n",
    "            iterator,\n",
    "            trainer=trainer,\n",
    "        )\n",
    "        \n",
    "        # Remove forbidden characters from the beginning of tokens\n",
    "        vocab = self._tokenizer.get_vocab()\n",
    "        filtered_vocab = {token: idx for token, idx in vocab.items()\n",
    "                          if not any(token.startswith(char) for char in self.forbidden_start_chars)}\n",
    "        \n",
    "        # Reset the vocabulary with the filtered tokens\n",
    "        new_tokenizer = Tokenizer(BPE())\n",
    "        new_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "        for token, idx in filtered_vocab.items():\n",
    "            if token in special_tokens:\n",
    "                new_tokenizer.add_special_tokens([token])\n",
    "            else:\n",
    "                new_tokenizer.add_tokens([token])\n",
    "        \n",
    "        self._tokenizer = new_tokenizer\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._tokenizer.get_vocab()\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self._tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self._tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Union, Iterator, Dict, Tuple\n",
    "\n",
    "class SpBPETokenizerTest:\n",
    "    def __init__(self, forbidden_start_chars=None):\n",
    "        self.forbidden_start_chars = forbidden_start_chars or {'ৎ', 'ং', 'ঃ'}\n",
    "        self.initial_alphabets = bangla_alphabets + conjunct_consonants\n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.inverse_vocab: Dict[int, str] = {}\n",
    "        self.merges: List[Tuple[str, str]] = []\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        text_or_iterator: Union[str, Iterator[str]],\n",
    "        vocab_size: int = 3000,\n",
    "        min_frequency: int = 2,\n",
    "        show_progress: bool = True,\n",
    "        special_tokens: List[str] = None,\n",
    "        initial_alphabet: List[str] = None,\n",
    "        limit_alphabet: int = None,\n",
    "    ) -> None:\n",
    "        special_tokens = special_tokens or []\n",
    "        if initial_alphabet is not None:\n",
    "            self.initial_alphabets = initial_alphabet\n",
    "        \n",
    "        # Add special tokens and initial alphabet to vocabulary\n",
    "        for token in special_tokens + self.initial_alphabets:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = len(self.vocab)\n",
    "                self.inverse_vocab[len(self.vocab) - 1] = token\n",
    "\n",
    "        # Handle both string and iterator input\n",
    "        if isinstance(text_or_iterator, str):\n",
    "            text = text_or_iterator\n",
    "        else:\n",
    "            text = ' '.join(text_or_iterator)\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        words = re.findall(r'\\S+', text, flags=re.UNICODE)\n",
    "\n",
    "        # Count initial character pairs\n",
    "        char_pairs = defaultdict(int)\n",
    "        for word in words:\n",
    "            chars = list(word)\n",
    "            for i in range(len(chars) - 1):\n",
    "                char_pairs[chars[i], chars[i+1]] += 1\n",
    "\n",
    "        # Perform BPE merges\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            if not char_pairs:\n",
    "                break\n",
    "            best_pair = max(char_pairs, key=char_pairs.get)\n",
    "            if char_pairs[best_pair] < min_frequency:\n",
    "                break\n",
    "            \n",
    "            new_token = ''.join(best_pair)\n",
    "            if new_token in self.vocab or any(new_token.startswith(char) for char in self.forbidden_start_chars):\n",
    "                del char_pairs[best_pair]\n",
    "                continue\n",
    "\n",
    "            self.merges.append(best_pair)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.inverse_vocab[len(self.vocab) - 1] = new_token\n",
    "\n",
    "            # Update character pairs\n",
    "            new_pairs = defaultdict(int)\n",
    "            for word in words:\n",
    "                new_word = word\n",
    "                i = 0\n",
    "                while i < len(new_word) - 1:\n",
    "                    if (new_word[i], new_word[i+1]) == best_pair:\n",
    "                        if i > 0:\n",
    "                            new_pairs[(new_word[i-1], new_token)] += 1\n",
    "                        if i < len(new_word) - 2:\n",
    "                            new_pairs[(new_token, new_word[i+2])] += 1\n",
    "                        new_word = new_word[:i] + new_token + new_word[i+2:]\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        if i < len(new_word) - 2:\n",
    "                            new_pairs[(new_word[i], new_word[i+1])] += 1\n",
    "                        i += 1\n",
    "                words[words.index(word)] = new_word\n",
    "\n",
    "            char_pairs = new_pairs\n",
    "\n",
    "            if show_progress and len(self.vocab) % 100 == 0:\n",
    "                print(f\"Vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "    def get_vocab(self) -> Dict[str, int]:\n",
    "        return self.vocab\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        words = re.findall(r'\\S+', text, flags=re.UNICODE)\n",
    "        encoded = []\n",
    "        for word in words:\n",
    "            for merge in self.merges:\n",
    "                word = word.replace(''.join(merge), ''.join(merge).replace(' ', ''))\n",
    "            encoded.extend([self.vocab.get(char, self.vocab['<unk>']) for char in word])\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return ''.join(self.inverse_vocab.get(id, '<unk>') for id in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpBPETokenizerTest()\n",
    "tokenizer.train_from_iterator(\n",
    "    [text],  # Wrap your text in a list\n",
    "    vocab_size=3000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "print(\"Sample vocabulary items:\")\n",
    "for i, (token, id) in enumerate(vocab.items()):\n",
    "    print(f\"{token}: {id}\")\n",
    "    if i >= 20:  # Print first 20 items\n",
    "        break\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"আমি বাংলায় কথা বলি\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"আমি বাংলায় কথা বলি\"), len([16, 50, 63, 48, 62, 59, 53, 62, 51, 73, 26, 42, 62, 48, 53, 63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])\n",
    "# tokenizer.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = SpBPETokenizer()\n",
    "tokenizer = SpBPETokenizerTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens([\"নির্জন\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    bengali_word_iterator(text=text),\n",
    "    # text,\n",
    "    vocab_size=30000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=[\n",
    "        \"<pad>\",\n",
    "        \"<s>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        '<mask>'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens(['আপনার', 'মধ্যে'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(tokenizer.get_vocab().items(), key=lambda kv:kv[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"বন্ধু মধ্যে\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_model('./')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once Again from the ground SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "from typing import List, Union, Iterator\n",
    "\n",
    "class BengaliTokenizer:\n",
    "    def __init__(self, model_prefix: str = \"bengali_tokenizer\"):\n",
    "        self.model_prefix = model_prefix\n",
    "        self.sp_model = None\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        text_or_iterator: Union[str, Iterator[str]],\n",
    "        vocab_size: int = 3000,\n",
    "        model_type: str = \"bpe\",\n",
    "        special_tokens: List[str] = None,\n",
    "        initial_tokens: List[str] = None,\n",
    "        character_coverage: float = 1.0,\n",
    "        max_sentence_length: int = 4192,\n",
    "    ):\n",
    "        # Prepare the input text file\n",
    "        if isinstance(text_or_iterator, str):\n",
    "            with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text_or_iterator)\n",
    "        else:\n",
    "            with open(\"temp_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                for line in text_or_iterator:\n",
    "                    f.write(line + \"\\n\")\n",
    "\n",
    "        # Prepare user defined symbols (special tokens and initial tokens)\n",
    "        special_tokens = special_tokens or []\n",
    "        initial_tokens = initial_tokens or []\n",
    "        predefined_tokens = {\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"}\n",
    "        user_defined_symbols = [token for token in special_tokens + initial_tokens if token not in predefined_tokens]\n",
    "        control_symbols = [token for token in special_tokens if token not in predefined_tokens]\n",
    "\n",
    "        # Train the SentencePiece model\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=\"temp_input.txt\",\n",
    "            model_prefix=self.model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            model_type=model_type,\n",
    "            character_coverage=character_coverage,\n",
    "            max_sentence_length=max_sentence_length,\n",
    "            pad_id=0,\n",
    "            bos_id=1,\n",
    "            eos_id=2,\n",
    "            unk_id=3,\n",
    "            user_defined_symbols=user_defined_symbols,\n",
    "            control_symbols=control_symbols,\n",
    "            input_sentence_size=10000000,\n",
    "            shuffle_input_sentence=True,\n",
    "            treat_whitespace_as_suffix=True,  # This ensures no space in tokens\n",
    "        )\n",
    "\n",
    "        # Load the trained model\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.load(f\"{self.model_prefix}.model\")\n",
    "\n",
    "        # Clean up temporary file\n",
    "        os.remove(\"temp_input.txt\")\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        return self.sp_model.encode(text, out_type=int)\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        return self.sp_model.decode(ids)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return {self.sp_model.id_to_piece(i): i for i in range(self.sp_model.get_piece_size())}\n",
    "\n",
    "    def token_to_id(self, token: str) -> int:\n",
    "        return self.sp_model.piece_to_id(token)\n",
    "\n",
    "    def id_to_token(self, id: int) -> str:\n",
    "        return self.sp_model.id_to_piece(id)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        self.sp_model.save(path)\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        self.sp_model = spm.SentencePieceProcessor()\n",
    "        self.sp_model.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BengaliTokenizer()\n",
    "\n",
    "# Train the tokenizer\n",
    "special_tokens = [\"[mask]\"]  # Removed \"<unk>\"\n",
    "initial_tokens = [\"আ\", \"ই\", \"উ\", \"ঊ\", \"ঋ\", \"এ\", \"ঐ\", \"ও\", \"ঔ\", \"ক\", \"খ\", \"গ\", \"ঘ\", \"ঙ\"]  # Add your Bengali initial tokens here\n",
    "\n",
    "tokenizer.train(\n",
    "    text,  # Your Bengali text or iterator of texts\n",
    "    vocab_size=3000,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_tokens=initial_tokens,\n",
    ")\n",
    "\n",
    "# Check the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Sample vocabulary items:\")\n",
    "for i, (token, id) in enumerate(vocab.items()):\n",
    "    print(f\"{token}: {id}\")\n",
    "    if i >= 20:\n",
    "        break\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"আমি বাংলায় কথা বলি\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Save the model\n",
    "tokenizer.save_model(\"bengali_tokenizer.model\")\n",
    "\n",
    "# Load the model (you can do this in a separate script)\n",
    "# tokenizer = BengaliTokenizer()\n",
    "# tokenizer.load_model(\"bengali_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Define special tokens and initial tokens\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "initial_tokens = [\"অ\", \"আ\", \"ই\", \"ঈ\", \"উ\", \"ঊ\", \"ঋ\", \"এ\", \"ঐ\", \"ও\", \"ঔ\"] # Add more initial tokens as needed\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "# Train the tokenizer on your corpus\n",
    "tokenizer.train(\n",
    "    files=[\"demo_1M.txt\"],\n",
    "    vocab_size=30000,  # Adjust vocab size as needed\n",
    "    min_frequency=2,\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=initial_tokens\n",
    ")\n",
    "\n",
    "# Save the tokenizer model\n",
    "tokenizer.save_model(\"./\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "# Your custom vocabulary list (Bangla words)\n",
    "custom_vocab = [\"বাংলা\", \"শব্দ\", \"তালিকা\"]  # Add your Bangla vocabulary here\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "\n",
    "# Calculate the target vocabulary size\n",
    "target_vocab_size = 1000 + 35\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=text,\n",
    "    vocab_size=target_vocab_size,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "\n",
    "# Get the current vocabulary\n",
    "current_vocab = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "# Add custom vocabulary\n",
    "for word in custom_vocab:\n",
    "    if word not in current_vocab:\n",
    "        tokenizer.add_tokens([word])\n",
    "\n",
    "# Trim vocabulary if it exceeds the target size\n",
    "while len(tokenizer.get_vocab()) > target_vocab_size:\n",
    "    # Remove the least frequent token that's not in special_tokens or custom_vocab\n",
    "    vocab_with_counts = tokenizer.get_vocab(with_added_tokens=True)\n",
    "    sorted_vocab = sorted(vocab_with_counts.items(), key=lambda x: x[1])\n",
    "    for token, _ in sorted_vocab:\n",
    "        if token not in special_tokens and token not in custom_vocab:\n",
    "            tokenizer.token_to_id.pop(token, None)\n",
    "            break\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = \"./custom_tokenizer\"\n",
    "tokenizer.save_model(tokenizer_save_path)\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"গেল ইউনিটেক প্রডাক্টস (বিডি)\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(encoded.tokens)\n",
    "\n",
    "# # Verify custom vocabulary is included\n",
    "# for word in custom_vocab:\n",
    "#     print(f\"{word}: {tokenizer.token_to_id.get(word, 'Not in vocabulary')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer\n",
    "test_text = \"গেল ইউনিটেক প্রডাক্টস (বিডি)\"\n",
    "tokenizer.encode(test_text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# Step 1: Define your special tokens and predefined tokens separately\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "predefined_tokens = [\"বাংলা\", \"ভাষা\", \"টোকেনাইজার\"]  # Your predefined Bengali tokens\n",
    "\n",
    "# Step 2: Initialize the tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Step 3: Define the trainer\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=special_tokens,\n",
    "    initial_alphabet=[]  # We'll let it learn the alphabet from the data\n",
    ")\n",
    "\n",
    "# Step 4: Define pre-tokenizer\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Step 5: Add predefined tokens to the tokenizer's vocabulary\n",
    "tokenizer.add_tokens(predefined_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the tokenizer\n",
    "files = [\"./demo_1M.txt\"]  # Add your Bengali text files here\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Step 7: Save the tokenizer\n",
    "tokenizer.save(\"./custom_tokenizer/bengali_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tokenizer_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "# from typing import List, Optional\n",
    "\n",
    "# class BengaliTokenizer:\n",
    "#     def __init__(self, special_tokens: List[str], predefined_tokens: List[str]):\n",
    "#         self.special_tokens = special_tokens\n",
    "#         self.predefined_tokens = predefined_tokens\n",
    "#         self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "#         self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "#     def train(self, files: List[str], vocab_size: Optional[int] = 30000):\n",
    "#         trainer = BpeTrainer(\n",
    "#             special_tokens=self.special_tokens + self.predefined_tokens,\n",
    "#             initial_alphabet=self.predefined_tokens,\n",
    "#             vocab_size=vocab_size\n",
    "#         )\n",
    "#         self.tokenizer.train(files, trainer)\n",
    "\n",
    "#     def save(self, path: str):\n",
    "#         self.tokenizer.save(path)\n",
    "        \n",
    "#     def encode(self, text: str):\n",
    "#         return self.tokenizer.encode(text)\n",
    "#     @classmethod\n",
    "#     def from_file(cls, path: str):\n",
    "#         tokenizer = Tokenizer.from_file(path)\n",
    "#         return cls([], [])  # Initialize with empty lists as we don't store these separately\n",
    "\n",
    "#     # def validate_predefined_tokens(self):\n",
    "#     #     vocab = self.tokenizer.get_vocab()\n",
    "#     #     missing_tokens = [token for token in self.predefined_tokens if token not in vocab]\n",
    "#     #     if missing_tokens:\n",
    "#     #         print(f\"Warning: The following predefined tokens are missing from the vocabulary: {missing_tokens}\")\n",
    "#     #     else:\n",
    "#     #         print(\"All predefined tokens are present in the vocabulary.\")\n",
    "        \n",
    "#     #     # Print the IDs of predefined tokens\n",
    "#     #     for token in self.predefined_tokens:\n",
    "#     #         if token in vocab:\n",
    "#     #             print(f\"Token: {token}, ID: {vocab[token]}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Usage example\n",
    "# special_tokens = [\"<s>\", \"</s>\", \"<pad>\", \"<mask>\"]\n",
    "# predefined_tokens = bangla_alphabets + conjunct_consonants #[\"বাংলা\", \"ভাষা\", \"tokenizer\"]  # Example Bengali tokens\n",
    "\n",
    "# # Create and train the tokenizer\n",
    "# bengali_tokenizer = BengaliTokenizer(special_tokens, predefined_tokens)\n",
    "# files = [\"./demo_1M.txt\"]\n",
    "# bengali_tokenizer.train(files)\n",
    "\n",
    "# # Validate that predefined tokens are added\n",
    "# # bengali_tokenizer.validate_predefined_tokens()\n",
    "\n",
    "# # Save the tokenizer\n",
    "# bengali_tokenizer.save(\"bengali_tokenizer.json\")\n",
    "\n",
    "# # Load the tokenizer\n",
    "# loaded_tokenizer = BengaliTokenizer.from_file(\"bengali_tokenizer.json\")\n",
    "\n",
    "# # Test the tokenizer\n",
    "# bengali_text = \"আপনার বাংলা ভाষা tokenizer এখানে\"\n",
    "# encoded = loaded_tokenizer.encode(bengali_text)\n",
    "\n",
    "# print(\"Tokens:\", encoded.tokens)\n",
    "# print(\"IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['আপনার', 'বা', 'ং', 'লা', 'ভাষা', 'to', 'ken', 'iz', 'er', 'এখানে']\n",
      "IDs: [685, 221, 112, 237, 4785, 2065, 6761, 8652, 683, 1229]\n"
     ]
    }
   ],
   "source": [
    "## currently working\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from typing import List, Optional\n",
    "\n",
    "class BengaliTokenizer:\n",
    "    def __init__(self, special_tokens: List[str], predefined_tokens: List[str]):\n",
    "        self.special_tokens = special_tokens\n",
    "        self.predefined_tokens = predefined_tokens\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files: List[str], vocab_size: Optional[int] = 30000):\n",
    "        # Calculate the remaining vocab size after accounting for special and predefined tokens\n",
    "        remaining_vocab_size = vocab_size - len(self.special_tokens) - len(self.predefined_tokens)\n",
    "        \n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=remaining_vocab_size,\n",
    "            special_tokens=self.special_tokens,\n",
    "            initial_alphabet=self.predefined_tokens\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        self.tokenizer.train(files, trainer)\n",
    "        \n",
    "        # Add special tokens and predefined tokens to ensure they're in the vocabulary\n",
    "        all_tokens = self.special_tokens + self.predefined_tokens\n",
    "        new_tokens = [token for token in all_tokens if token not in self.tokenizer.get_vocab()]\n",
    "        self.tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        self.tokenizer.save(path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path: str):\n",
    "        tokenizer = Tokenizer.from_file(path)\n",
    "        instance = cls([], [])\n",
    "        instance.tokenizer = tokenizer\n",
    "        return instance\n",
    "\n",
    "    def validate_tokens(self):\n",
    "        vocab = self.tokenizer.get_vocab()\n",
    "        all_tokens = self.special_tokens + self.predefined_tokens\n",
    "        \n",
    "        print(\"Special and Predefined Tokens:\")\n",
    "        for token in all_tokens:\n",
    "            if token in vocab:\n",
    "                print(f\"Token: {token}, ID: {vocab[token]}\")\n",
    "            else:\n",
    "                print(f\"Warning: Token '{token}' not found in vocabulary\")\n",
    "\n",
    "        print(\"\\nFull Vocabulary:\")\n",
    "        sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "        for token, id in sorted_vocab:\n",
    "            print(f\"Token: {token}, ID: {id}\")\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "# Usage example\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "predefined_tokens = bangla_alphabets + conjunct_consonants\n",
    "\n",
    "\n",
    "# Create and train the tokenizer\n",
    "bengali_tokenizer = BengaliTokenizer(special_tokens, predefined_tokens)\n",
    "files = [\"demo_1M.txt\"]\n",
    "bengali_tokenizer.train(files)\n",
    "\n",
    "# Validate tokens\n",
    "# bengali_tokenizer.validate_tokens()\n",
    "\n",
    "# Save the tokenizer\n",
    "bengali_tokenizer.save(\"bengali_tokenizer.json\")\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = BengaliTokenizer.from_file(\"bengali_tokenizer.json\")\n",
    "\n",
    "# Test the tokenizer\n",
    "bengali_text = \"আপনার বাংলা ভাষা tokenizer এখানে\"\n",
    "encoded = loaded_tokenizer.encode(bengali_text)\n",
    "\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:\", encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25388\n"
     ]
    }
   ],
   "source": [
    "# print(len(bengali_tokenizer.tokenizer.get_vocab()))\n",
    "vocab = bengali_tokenizer.tokenizer.get_vocab()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('·', 100),\n",
       " ('»', 101),\n",
       " ('¿', 102),\n",
       " ('Ñ', 103),\n",
       " ('í', 104),\n",
       " ('ó', 105),\n",
       " ('ú', 106),\n",
       " ('œ', 107),\n",
       " ('Ÿ', 108),\n",
       " ('।', 109),\n",
       " ('॥', 110),\n",
       " ('ঁ', 111),\n",
       " ('ং', 112),\n",
       " ('ঃ', 113),\n",
       " ('অ', 114),\n",
       " ('আ', 115),\n",
       " ('ই', 116),\n",
       " ('ঈ', 117),\n",
       " ('উ', 118),\n",
       " ('ঊ', 119),\n",
       " ('ঋ', 120),\n",
       " ('এ', 121),\n",
       " ('ঐ', 122),\n",
       " ('ও', 123),\n",
       " ('ঔ', 124),\n",
       " ('ক', 125),\n",
       " ('খ', 126),\n",
       " ('গ', 127),\n",
       " ('ঘ', 128),\n",
       " ('ঙ', 129),\n",
       " ('চ', 130),\n",
       " ('ছ', 131),\n",
       " ('জ', 132),\n",
       " ('ঝ', 133),\n",
       " ('ঞ', 134),\n",
       " ('ট', 135),\n",
       " ('ঠ', 136),\n",
       " ('ড', 137),\n",
       " ('ঢ', 138),\n",
       " ('ণ', 139),\n",
       " ('ত', 140),\n",
       " ('থ', 141),\n",
       " ('দ', 142),\n",
       " ('ধ', 143),\n",
       " ('ন', 144),\n",
       " ('প', 145),\n",
       " ('ফ', 146),\n",
       " ('ব', 147),\n",
       " ('ভ', 148),\n",
       " ('ম', 149),\n",
       " ('য', 150),\n",
       " ('র', 151),\n",
       " ('ল', 152),\n",
       " ('শ', 153),\n",
       " ('ষ', 154),\n",
       " ('স', 155),\n",
       " ('হ', 156),\n",
       " ('়', 157),\n",
       " ('া', 158),\n",
       " ('ি', 159),\n",
       " ('ী', 160),\n",
       " ('ু', 161),\n",
       " ('ূ', 162),\n",
       " ('ৃ', 163),\n",
       " ('ে', 164),\n",
       " ('ৈ', 165),\n",
       " ('ো', 166),\n",
       " ('ৌ', 167),\n",
       " ('্', 168),\n",
       " ('ৎ', 169),\n",
       " ('ৗ', 170),\n",
       " ('ড়', 171),\n",
       " ('ঢ়', 172),\n",
       " ('য়', 173),\n",
       " ('০', 174),\n",
       " ('১', 175),\n",
       " ('২', 176),\n",
       " ('৩', 177),\n",
       " ('৪', 178),\n",
       " ('৫', 179),\n",
       " ('৬', 180),\n",
       " ('৭', 181),\n",
       " ('৮', 182),\n",
       " ('৯', 183),\n",
       " ('৳', 184),\n",
       " ('৷', 185),\n",
       " ('\\u200b', 186),\n",
       " ('\\u200c', 187),\n",
       " ('\\u200d', 188),\n",
       " ('–', 189),\n",
       " ('—', 190),\n",
       " ('‘', 191),\n",
       " ('’', 192),\n",
       " ('“', 193),\n",
       " ('”', 194),\n",
       " ('•', 195),\n",
       " ('…', 196),\n",
       " ('›', 197),\n",
       " ('€', 198),\n",
       " ('℃', 199)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.items(), key=lambda x:x[1])[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CustomBPETokenizer at 0x7ccde45b7160>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [685, 1275, 860, 215, 1229, 2437, 324, 109], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer JSON file\n",
    "with open(\"bengali_tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "\n",
    "# Save the JSON content to a temporary file\n",
    "temp_json_path = \"temp_tokenizer.json\"\n",
    "with open(temp_json_path, \"w\", encoding=\"utf-8\") as temp_f:\n",
    "    json.dump(tokenizer_json, temp_f)\n",
    "\n",
    "# Create a PreTrainedTokenizerFast instance from the JSON file\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=temp_json_path)\n",
    "\n",
    "# Save the tokenizer in Hugging Face format\n",
    "# tokenizer.save_pretrained(\"path/to/save/hf_tokenizer\")\n",
    "\n",
    "# Example usage\n",
    "encoded = tokenizer.encode_plus(\"আপনার লেখা পাঠ্য এখানে লিখুন।\")\n",
    "print(encoded)\n",
    "\n",
    "# decoded = tokenizer.decode(encoded)\n",
    "# print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আপনার', 'লেখা', 'পাঠ', '্য', 'এখানে', 'লিখ', 'ুন', '।']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(\"আপনার লেখা পাঠ্য এখানে লিখুন।\")\n",
    "print(encoded)\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "class CustomBPETokenizer:\n",
    "    def __init__(self, vocab_size=30000, special_tokens=None, predefined_tokens=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = special_tokens if special_tokens is not None else [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "        self.predefined_tokens = predefined_tokens if predefined_tokens is not None else []\n",
    "        \n",
    "        self.tokenizer = Tokenizer(models.BPE())\n",
    "        self.tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        self.tokenizer.decoder = decoders.ByteLevel()\n",
    "        self.tokenizer.post_processor = processors.ByteLevel()\n",
    "        \n",
    "    def train(self, files):\n",
    "        # Add predefined tokens as initial alphabet\n",
    "        initial_alphabet = list(set([token for token in self.predefined_tokens]))\n",
    "        \n",
    "        # Create BPE Trainer with initial alphabet and special tokens\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            special_tokens=self.special_tokens,\n",
    "            initial_alphabet=initial_alphabet\n",
    "        )\n",
    "        \n",
    "        # Train the tokenizer\n",
    "        self.tokenizer.train(files, trainer)\n",
    "        \n",
    "    def save(self, path):\n",
    "        self.tokenizer.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "    predefined_tokens = [\"অ\", \"আ\", \"ই\", \"ঈ\", \"উ\", \"ঊ\", \"ঋ\", \"এ\", \"ঐ\", \"ও\", \"ঔ\", \n",
    "                        \"ক\", \"খ\", \"গ\", \"ঘ\", \"ঙ\", \"চ\", \"ছ\", \"জ\", \"ঝ\", \"ঞ\", \n",
    "                        \"ট\", \"ঠ\", \"ড\", \"ঢ\", \"ণ\", \"ত\", \"থ\", \"দ\", \"ধ\", \"ন\", \n",
    "                        \"প\", \"ফ\", \"ব\", \"ভ\", \"ম\", \"য\", \"র\", \"ল\", \"শ\", \"ষ\", \"স\", \"হ\",\n",
    "                        \"ড়\", \"ঢ়\", \"য়\", \"ৎ\", \"ং\", \"ঃ\", \"ঁ\"]  # Add more initial tokens as needed\n",
    "\n",
    "    files = [\"demo_1M.txt\"]\n",
    "    tokenizer = CustomBPETokenizer(vocab_size=30000, special_tokens=special_tokens, predefined_tokens=predefined_tokens)\n",
    "    tokenizer.train(files)\n",
    "    tokenizer.save(\"./tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from Tokenizer_train.custom_tokenizer_bpe import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Checking all special and predefined tokens existance:\n",
      "All predifined and special tokens were found in the tokenizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "special_tokens = [\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<mask>\"]\n",
    "initial_tokens = bangla_alphabets + conjunct_consonants\n",
    "\n",
    "tokenizer = BengaliBPETokenizer()\n",
    "files = [\"demo_1M.txt\"]\n",
    "hf_tokenizer = tokenizer.train_tokenizer(\n",
    "    files=files,\n",
    "    vocab_size=30_000,\n",
    "    special_tokens= [\n",
    "        \n",
    "    ],\n",
    "    initial_tokens= initial_tokens,\n",
    "    min_frequency=2,\n",
    "    limit_alphabet=500,\n",
    "    show_progress=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "tokenizer.validate_predefined_tokens()\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"bengali_tokenizer_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['আপনার', 'বা', 'ং', 'লা', 'ভাষা', 'to', 'ken', 'iz', 'er', 'এখানে']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bengali_text = \"আপনার বাংলা ভাষা tokenizer এখানে\"\n",
    "hf_tokenizer.encode_plus(bengali_text).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['কা', 'কি', 'কী', 'কু', 'কূ', 'কৃ', 'কে', 'কৈ', 'কো', 'কৌ', 'কং', 'কঃ', 'কঁ', 'খা', 'খি', 'খী', 'খু', 'খূ', 'খৃ', 'খে', 'খৈ', 'খো', 'খৌ', 'খং', 'খঃ', 'খঁ', 'গা', 'গি', 'গী', 'গু', 'গূ', 'গৃ', 'গে', 'গৈ', 'গো', 'গৌ', 'গং', 'গঃ', 'গঁ', 'ঘা', 'ঘি', 'ঘী', 'ঘু', 'ঘূ', 'ঘৃ', 'ঘে', 'ঘৈ', 'ঘো', 'ঘৌ', 'ঘং', 'ঘঃ', 'ঘঁ', 'ঙা', 'ঙি', 'ঙী', 'ঙু', 'ঙূ', 'ঙৃ', 'ঙে', 'ঙৈ', 'ঙো', 'ঙৌ', 'ঙং', 'ঙঃ', 'ঙঁ', 'চা', 'চি', 'চী', 'চু', 'চূ', 'চৃ', 'চে', 'চৈ', 'চো', 'চৌ', 'চং', 'চঃ', 'চঁ', 'ছা', 'ছি', 'ছী', 'ছু', 'ছূ', 'ছৃ', 'ছে', 'ছৈ', 'ছো', 'ছৌ', 'ছং', 'ছঃ', 'ছঁ', 'জা', 'জি', 'জী', 'জু', 'জূ', 'জৃ', 'জে', 'জৈ', 'জো', 'জৌ', 'জং', 'জঃ', 'জঁ', 'ঝা', 'ঝি', 'ঝী', 'ঝু', 'ঝূ', 'ঝৃ', 'ঝে', 'ঝৈ', 'ঝো', 'ঝৌ', 'ঝং', 'ঝঃ', 'ঝঁ', 'টা', 'টি', 'টী', 'টু', 'টূ', 'টৃ', 'টে', 'টৈ', 'টো', 'টৌ', 'টং', 'টঃ', 'টঁ', 'ঠা', 'ঠি', 'ঠী', 'ঠু', 'ঠূ', 'ঠৃ', 'ঠে', 'ঠৈ', 'ঠো', 'ঠৌ', 'ঠং', 'ঠঃ', 'ঠঁ', 'ডা', 'ডি', 'ডী', 'ডু', 'ডূ', 'ডৃ', 'ডে', 'ডৈ', 'ডো', 'ডৌ', 'ডং', 'ডঃ', 'ডঁ', 'ঢা', 'ঢি', 'ঢী', 'ঢু', 'ঢূ', 'ঢৃ', 'ঢে', 'ঢৈ', 'ঢো', 'ঢৌ', 'ঢং', 'ঢঃ', 'ঢঁ', 'ণা', 'ণি', 'ণী', 'ণু', 'ণূ', 'ণৃ', 'ণে', 'ণৈ', 'ণো', 'ণৌ', 'ণং', 'ণঃ', 'ণঁ', 'তা', 'তি', 'তী', 'তু', 'তূ', 'তৃ', 'তে', 'তৈ', 'তো', 'তৌ', 'তং', 'তঃ', 'তঁ', 'থা', 'থি', 'থী', 'থু', 'থূ', 'থৃ', 'থে', 'থৈ', 'থো', 'থৌ', 'থং', 'থঃ', 'থঁ', 'দা', 'দি', 'দী', 'দু', 'দূ', 'দৃ', 'দে', 'দৈ', 'দো', 'দৌ', 'দং', 'দঃ', 'দঁ', 'ধা', 'ধি', 'ধী', 'ধু', 'ধূ', 'ধৃ', 'ধে', 'ধৈ', 'ধো', 'ধৌ', 'ধং', 'ধঃ', 'ধঁ', 'না', 'নি', 'নী', 'নু', 'নূ', 'নৃ', 'নে', 'নৈ', 'নো', 'নৌ', 'নং', 'নঃ', 'নঁ', 'পা', 'পি', 'পী', 'পু', 'পূ', 'পৃ', 'পে', 'পৈ', 'পো', 'পৌ', 'পং', 'পঃ', 'পঁ', 'ফা', 'ফি', 'ফী', 'ফু', 'ফূ', 'ফৃ', 'ফে', 'ফৈ', 'ফো', 'ফৌ', 'ফং', 'ফঃ', 'ফঁ', 'বা', 'বি', 'বী', 'বু', 'বূ', 'বৃ', 'বে', 'বৈ', 'বো', 'বৌ', 'বং', 'বঃ', 'বঁ', 'ভা', 'ভি', 'ভী', 'ভু', 'ভূ', 'ভৃ', 'ভে', 'ভৈ', 'ভো', 'ভৌ', 'ভং', 'ভঃ', 'ভঁ', 'মা', 'মি', 'মী', 'মু', 'মূ', 'মৃ', 'মে', 'মৈ', 'মো', 'মৌ', 'মং', 'মঃ', 'মঁ', 'যা', 'যি', 'যী', 'যু', 'যূ', 'যৃ', 'যে', 'যৈ', 'যো', 'যৌ', 'যং', 'যঃ', 'যঁ', 'রা', 'রি', 'রী', 'রু', 'রূ', 'রৃ', 'রে', 'রৈ', 'রো', 'রৌ', 'রং', 'রঃ', 'রঁ', 'লা', 'লি', 'লী', 'লু', 'লূ', 'লৃ', 'লে', 'লৈ', 'লো', 'লৌ', 'লং', 'লঃ', 'লঁ', 'শা', 'শি', 'শী', 'শু', 'শূ', 'শৃ', 'শে', 'শৈ', 'শো', 'শৌ', 'শং', 'শঃ', 'শঁ', 'ষা', 'ষি', 'ষী', 'ষু', 'ষূ', 'ষৃ', 'ষে', 'ষৈ', 'ষো', 'ষৌ', 'ষং', 'ষঃ', 'ষঁ', 'সা', 'সি', 'সী', 'সু', 'সূ', 'সৃ', 'সে', 'সৈ', 'সো', 'সৌ', 'সং', 'সঃ', 'সঁ', 'হা', 'হি', 'হী', 'হু', 'হূ', 'হৃ', 'হে', 'হৈ', 'হো', 'হৌ', 'হং', 'হঃ', 'হঁ', 'ড়া', 'ড়ি', 'ড়ী', 'ড়ু', 'ড়ূ', 'ড়ৃ', 'ড়ে', 'ড়ৈ', 'ড়ো', 'ড়ৌ', 'ড়ং', 'ড়ঃ', 'ড়ঁ', 'ঢ়া', 'ঢ়ি', 'ঢ়ী', 'ঢ়ু', 'ঢ়ূ', 'ঢ়ৃ', 'ঢ়ে', 'ঢ়ৈ', 'ঢ়ো', 'ঢ়ৌ', 'ঢ়ং', 'ঢ়ঃ', 'ঢ়ঁ', 'য়া', 'য়ি', 'য়ী', 'য়ু', 'য়ূ', 'য়ৃ', 'য়ে', 'য়ৈ', 'য়ো', 'য়ৌ', 'য়ং', 'য়ঃ', 'য়ঁ']\n"
     ]
    }
   ],
   "source": [
    "bengali_consonants = [\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ',\n",
    "    'স', 'হ', 'ড়', 'ঢ়', 'য়'\n",
    "]\n",
    "\n",
    "bengali_kar = [\n",
    "    'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', 'ং', 'ঃ', 'ঁ'\n",
    "]\n",
    "\n",
    "consonants_with_kar = []\n",
    "\n",
    "for consonant in bengali_consonants:\n",
    "    for kar in bengali_kar:\n",
    "        consonants_with_kar.append(consonant + kar)\n",
    "\n",
    "print(consonants_with_kar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ক্য', 'ক্র', 'খ্য', 'খ্র', 'গ্য', 'গ্র', 'ঘ্য', 'ঘ্র', 'চ্য', 'চ্র', 'ছ্য', 'ছ্র', 'জ্য', 'জ্র', 'ঝ্য', 'ঝ্র', 'ঞ্য', 'ঞ্র', 'ট্য', 'ট্র', 'ঠ্য', 'ঠ্র', 'ড্য', 'ড্র', 'ঢ্য', 'ঢ্র', 'ণ্য', 'ণ্র', 'ত্য', 'ত্র', 'থ্য', 'থ্র', 'দ্য', 'দ্র', 'ধ্য', 'ধ্র', 'ন্য', 'ন্র', 'প্য', 'প্র', 'ফ্য', 'ফ্র', 'ব্য', 'ব্র', 'ভ্য', 'ভ্র', 'ম্য', 'ম্র', 'য্য', 'য্র', 'র্য', 'র্র', 'ল্য', 'ল্র', 'শ্য', 'শ্র', 'ষ্য', 'ষ্র', 'স্য', 'স্র', 'হ্য', 'হ্র', 'ড়্য', 'ড়্র', 'ঢ়্য', 'ঢ়্র', 'য়্য', 'য়্র']\n"
     ]
    }
   ],
   "source": [
    "bengali_consonants = [\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ',\n",
    "    'স', 'হ', 'ড়', 'ঢ়', 'য়'\n",
    "]\n",
    "\n",
    "bengali_kar = [\n",
    "    'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৎ', 'ৌ', 'ং', 'ঃ', 'ঁ'\n",
    "]\n",
    "\n",
    "z_fola = '্য'\n",
    "ra_fola = '্র'\n",
    "\n",
    "consonants_with_fola_and_kar = []\n",
    "\n",
    "for consonant in bengali_consonants:\n",
    "    # for kar in bengali_kar:\n",
    "        consonants_with_fola_and_kar.append(consonant + z_fola )\n",
    "        consonants_with_fola_and_kar.append(consonant + ra_fola )\n",
    "\n",
    "print(consonants_with_fola_and_kar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ক্য', 'ক্র', 'র্ক', 'খ্য', 'খ্র', 'র্খ', 'গ্য', 'গ্র', 'র্গ', 'ঘ্য', 'ঘ্র', 'র্ঘ', 'চ্য', 'চ্র', 'র্চ', 'ছ্য', 'ছ্র', 'র্ছ', 'জ্য', 'জ্র', 'র্জ', 'ঝ্য', 'ঝ্র', 'র্ঝ', 'ট্য', 'ট্র', 'র্ট', 'ঠ্য', 'ঠ্র', 'র্ঠ', 'ড্য', 'ড্র', 'র্ড', 'ঢ্য', 'ঢ্র', 'র্ঢ', 'ণ্য', 'ণ্র', 'র্ণ', 'ত্য', 'ত্র', 'র্ত', 'থ্য', 'থ্র', 'র্থ', 'দ্য', 'দ্র', 'র্দ', 'ধ্য', 'ধ্র', 'র্ধ', 'ন্য', 'ন্র', 'র্ন', 'প্য', 'প্র', 'র্প', 'ফ্য', 'ফ্র', 'র্ফ', 'ব্য', 'ব্র', 'র্ব', 'ভ্য', 'ভ্র', 'র্ভ', 'ম্য', 'ম্র', 'র্ম', 'য্য', 'য্র', 'র্য', 'র্য', 'র্র', 'র্র', 'ল্য', 'ল্র', 'র্ল', 'শ্য', 'শ্র', 'র্শ', 'ষ্য', 'ষ্র', 'র্ষ', 'স্য', 'স্র', 'র্স', 'হ্য', 'হ্র', 'র্হ', 'ড়্য', 'ড়্র', 'র্ড়', 'ঢ়্য', 'ঢ়্র', 'র্ঢ়', 'য়্য', 'য়্র', 'র্য়']\n"
     ]
    }
   ],
   "source": [
    "bengali_consonants = [\n",
    "    'ক', 'খ', 'গ', 'ঘ', 'চ', 'ছ', 'জ', 'ঝ',\n",
    "    'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন',\n",
    "    'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ',\n",
    "    'স', 'হ', 'ড়', 'ঢ়', 'য়',\n",
    "]\n",
    "\n",
    "bengali_kar = [\n",
    "    'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ'\n",
    "]\n",
    "\n",
    "z_fola = '্য'\n",
    "ra_fola = '্র'\n",
    "rhi_kar = 'র্'\n",
    "\n",
    "consonants_with_fola_and_kar = []\n",
    "\n",
    "for consonant in bengali_consonants:\n",
    "    # for kar in bengali_kar:\n",
    "        consonants_with_fola_and_kar.append(consonant + z_fola )\n",
    "        consonants_with_fola_and_kar.append(consonant + ra_fola )\n",
    "        consonants_with_fola_and_kar.append(rhi_kar + consonant )\n",
    "\n",
    "print(consonants_with_fola_and_kar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    Regex,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(models.BPE)\n",
    "# dir(models)\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Train in module sentencepiece:\n",
      "\n",
      "Train(arg=None, logstream=None, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dir(spm)\n",
    "help(spm.SentencePieceTrainer.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"temp_input.txt\",\n",
    "    model_prefix=self.model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=model_type,\n",
    "    character_coverage=character_coverage,\n",
    "    max_sentence_length=max_sentence_length,\n",
    "    pad_id=0,\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    unk_id=3,\n",
    "    user_defined_symbols=user_defined_symbols,\n",
    "    control_symbols=control_symbols,\n",
    "    input_sentence_size=10000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    treat_whitespace_as_suffix=True,  # This ensures no space in tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model=models.BPE(\n",
    "    unk_token=\"[UNK]\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"), # replace 2+ spaces with one.\n",
    "    ]\n",
    ")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Metaspace(),\n",
    "                                                   pre_tokenizers.Digits(individual_digits=True)])\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS] $A [EOS]\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[EOS]\", 2)],\n",
    ")\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, special_tokens=special_tokens, )\n",
    "tokenizer.train(files=[\"opus100-en.txt\"], trainer=trainer)\n",
    "os.makedirs(\"./tokenizers\", exist_ok=True)\n",
    "tokenizer.save(\"./tokenizers/en-sentpiece.json\")\n",
    "encoded = tokenizer.encode(text_en)\n",
    "print(\"len:\", len(encoded.tokens), encoded.tokens)\n",
    "print(tokenizer.decode(encoded.ids))\n",
    "print(\"------------------\")\n",
    "print(tokenizer.normalizer.normalize_str(text_en))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_m_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
